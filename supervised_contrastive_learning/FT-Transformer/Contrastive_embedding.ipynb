{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0aa47b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan  5 14:03:26 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 528.24       Driver Version: 528.24       CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| 30%   42C    P3    31W / 320W |      0MiB / 16376MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caead66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DAHS\\anaconda3\\envs\\DL\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from data_provider import *\n",
    "from model import *\n",
    "from pytorch_metric_learning import losses\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2379c7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_data_dir = '/Users/DAHS/Desktop/ECP_CONT/ECP_SCL/Case Labeling/MIMIC-IV.csv.gz'\n",
    "mimic_df = pd.read_csv(mimic_data_dir, compression = 'gzip', usecols = ['subject_id','classes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b37cf26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build Dataset ....\n",
      "test set : test set = 0.7 : 0.30000000000000004\n",
      "Train set class:  classes\n",
      "0    73412\n",
      "1    73412\n",
      "2    13995\n",
      "3    13995\n",
      "Name: count, dtype: int64\n",
      "Test set class:  classes\n",
      "0    579948\n",
      "1     31520\n",
      "2      5986\n",
      "3     67460\n",
      "Name: count, dtype: int64\n",
      "--------------------\n",
      "Train class ratio: 0.8388778853486103:0.0459989059814493:0.008769066218198429:0.10635414245174193\n",
      "Test class ratio: 0.8467457228206753:0.04602037628081774:0.00873978338886342:0.09849411750964354\n",
      "--------------------\n",
      "Number of trainset patient: 9554\n",
      "Number of testset patient: 4201\n",
      "Number of trainset stay: 10126\n",
      "Number of testset stay: 4459\n",
      "--------------------\n",
      "Split seed:  2582\n",
      "train ratio: 0.7\n",
      "Threshold: 0.05\n",
      "--------------------\n",
      "총 소요 시간(초):1.5349442958831787\n",
      "시도한 trial 수:  0\n",
      "test set : test set = 0.7 : 0.30000000000000004\n",
      "Train set class:  classes\n",
      "0    73412\n",
      "1    73412\n",
      "2    13995\n",
      "3    13995\n",
      "Name: count, dtype: int64\n",
      "Test set class:  classes\n",
      "0    579948\n",
      "1     31520\n",
      "2      5986\n",
      "3     67460\n",
      "Name: count, dtype: int64\n",
      "--------------------\n",
      "Train class ratio: 0.8388778853486103:0.0459989059814493:0.008769066218198429:0.10635414245174193\n",
      "Test class ratio: 0.8467457228206753:0.04602037628081774:0.00873978338886342:0.09849411750964354\n",
      "--------------------\n",
      "Number of trainset patient: 9554\n",
      "Number of testset patient: 4201\n",
      "Number of trainset stay: 10126\n",
      "Number of testset stay: 4459\n",
      "--------------------\n",
      "Split seed:  2582\n",
      "train ratio: 0.7\n",
      "Threshold: 0.05\n",
      "--------------------\n",
      "총 소요 시간(초):1.5309979915618896\n",
      "시도한 trial 수:  0\n"
     ]
    }
   ],
   "source": [
    "## Build Dataset \n",
    "print(f'Build Dataset ....')\n",
    "\n",
    "dataset_train = TableDataset(data_path=mimic_data_dir, data_type='mimic',mode='train',seed=2582)\n",
    "\n",
    "y_train_indices = dataset_train.df_num.index\n",
    "y_train = [dataset_train.y[i] for i in y_train_indices]\n",
    "class_sample_count = np.array([len(np.where(y_train == t)[0]) for t in np.unique(y_train)])\n",
    "\n",
    "weight = 1. / class_sample_count\n",
    "    \n",
    "samples_weight = np.array([weight[t] for t in y_train])\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "sampler = WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))\n",
    "\n",
    "loader_train = DataLoader(dataset_train, batch_size=128, shuffle=False, sampler=sampler, drop_last=True)\n",
    "\n",
    "dataset_val = TableDataset(data_path=mimic_data_dir, data_type='mimic',mode='valid',seed=2582)\n",
    "loader_val = DataLoader(dataset_val, batch_size=128, shuffle=False, drop_last=True)\n",
    "\n",
    "# Tuple Containing the number of unique values within each category\n",
    "card_categories = []\n",
    "for col in dataset_train.df_cat.columns:\n",
    "    card_categories.append(dataset_train.df_cat[col].nunique())\n",
    "\n",
    "# dataset_test = TableDataset(data_path=args.mimic_data_dir, data_type='eicu',mode='test',seed=args.seed)\n",
    "# loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "# # Tuple Containing the number of unique values within each category\n",
    "# card_categories = []\n",
    "# for col in dataset_test.df_cat.columns:\n",
    "#     card_categories.append(dataset_test.df_cat[col].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7ffe285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "SEED = 2582\n",
    "seed_everything(seed = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39144cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "class CosineAnnealingWarmUpRestarts(_LRScheduler):\n",
    "    def __init__(self, optimizer, T_0, T_mult=1, eta_max=0.1, T_up=0, gamma=1., last_epoch=-1):\n",
    "        if T_0 <= 0 or not isinstance(T_0, int):\n",
    "            raise ValueError(\"Expected positive integer T_0, but got {}\".format(T_0))\n",
    "        if T_mult < 1 or not isinstance(T_mult, int):\n",
    "            raise ValueError(\"Expected integer T_mult >= 1, but got {}\".format(T_mult))\n",
    "        if T_up < 0 or not isinstance(T_up, int):\n",
    "            raise ValueError(\"Expected positive integer T_up, but got {}\".format(T_up))\n",
    "        self.T_0 = T_0\n",
    "        self.T_mult = T_mult\n",
    "        self.base_eta_max = eta_max\n",
    "        self.eta_max = eta_max\n",
    "        self.T_up = T_up\n",
    "        self.T_i = T_0\n",
    "        self.gamma = gamma\n",
    "        self.cycle = 0\n",
    "        self.T_cur = last_epoch\n",
    "        super(CosineAnnealingWarmUpRestarts, self).__init__(optimizer, last_epoch)\n",
    "    \n",
    "    def get_lr(self):\n",
    "        if self.T_cur == -1:\n",
    "            return self.base_lrs\n",
    "        elif self.T_cur < self.T_up:\n",
    "            return [(self.eta_max - base_lr)*self.T_cur / self.T_up + base_lr for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr + (self.eta_max - base_lr) * (1 + math.cos(math.pi * (self.T_cur-self.T_up) / (self.T_i - self.T_up))) / 2\n",
    "                    for base_lr in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "            self.T_cur = self.T_cur + 1\n",
    "            if self.T_cur >= self.T_i:\n",
    "                self.cycle += 1\n",
    "                self.T_cur = self.T_cur - self.T_i\n",
    "                self.T_i = (self.T_i - self.T_up) * self.T_mult + self.T_up\n",
    "        else:\n",
    "            if epoch >= self.T_0:\n",
    "                if self.T_mult == 1:\n",
    "                    self.T_cur = epoch % self.T_0\n",
    "                    self.cycle = epoch // self.T_0\n",
    "                else:\n",
    "                    n = int(math.log((epoch / self.T_0 * (self.T_mult - 1) + 1), self.T_mult))\n",
    "                    self.cycle = n\n",
    "                    self.T_cur = epoch - self.T_0 * (self.T_mult ** n - 1) / (self.T_mult - 1)\n",
    "                    self.T_i = self.T_0 * self.T_mult ** (n)\n",
    "            else:\n",
    "                self.T_i = self.T_0\n",
    "                self.T_cur = epoch\n",
    "                \n",
    "        self.eta_max = self.base_eta_max * (self.gamma**self.cycle)\n",
    "        self.last_epoch = math.floor(epoch)\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ef538f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "path_model = \"./checkpoint\"\n",
    "if not os.path.exists(path_model):\n",
    "    os.makedirs(os.path.join(path_model))\n",
    "\n",
    "def param():\n",
    "    categories = card_categories\n",
    "    num_continuous = 196 \n",
    "    dim = 32                       # Embedding Dimension of Input Data  32\n",
    "    depth = 4                      # Nums of Attention Layer Depth  6\n",
    "    heads = 8                      # Nums of Attention head\n",
    "    dim_head = 17                  # Dimension of Attention(Q,K,V)\n",
    "    attn_dropout = 0.1             # Ratio of Attention Layer dropout\n",
    "    ff_dropout = 0.6215269253354271  # Ratio of FeedForward Layer dropout\n",
    "    temp = 0.38413918034866346\n",
    "    total_epoch = 50\n",
    "    lr = 0.0001\n",
    "    num_special_tokens = 2\n",
    "    return categories, num_continuous, dim, depth, heads, dim_head, attn_dropout, ff_dropout, temp, total_epoch, lr, num_special_tokens\n",
    "\n",
    "def train(trial, search = True):\n",
    "    global emb_model, results_loss, optimizer, scheduler\n",
    "    patience = 3\n",
    "    early_stop_counter = 0\n",
    "    categories, num_continuous, dim, depth, heads, dim_head, attn_dropout, ff_dropout, temp, total_epoch, lr, num_special_tokens = param()\n",
    "    \n",
    "    # search parameters\n",
    "    if search == True:\n",
    "        # total_epoch = trial.suggest_int('total_epoch', 4,10)\n",
    "        dim      = trial.suggest_int('dim', 20,90)\n",
    "        depth      = trial.suggest_int('depth', 3,5)\n",
    "        ff_dropout = trial.suggest_uniform('FeedForward Layer dropout', 0.56, 0.65)\n",
    "        temp       = trial.suggest_uniform('temp', 0.1, 0.5)\n",
    "        \n",
    "    print('learning_rate : ', lr, \"\\nepoch : \", total_epoch, \"\\nEmbedding Dimension of Input Data : \", dim, \"\\nNums of Attention Layer Depth : \", depth, \"\\ndrop_rate : \", ff_dropout, \"\\ntemperature : \", temp)\n",
    "    \n",
    "    results_loss = {\n",
    "    'epoch_by_trn'          : [],\n",
    "    'epoch_by_val'          : []\n",
    "    }\n",
    "\n",
    "    # 모델 정의\n",
    "    emb_model = FTTransformer(categories=categories,\n",
    "    num_continuous=num_continuous,\n",
    "    dim=dim,\n",
    "    depth=depth,\n",
    "    heads=heads,\n",
    "    dim_head=dim_head,\n",
    "    num_special_tokens = 2,\n",
    "    attn_dropout=attn_dropout,\n",
    "    ff_dropout=ff_dropout).to(device)\n",
    "    \n",
    "    print(emb_model)\n",
    "    \n",
    "    contrastive_loss = losses.SupConLoss(temperature=temp)\n",
    "    # optimizer = optim.RMSprop(emb_model.parameters(), lr= lr)\n",
    "    optimizer = optim.AdamW(emb_model.parameters(), lr = 0.000001)\n",
    "    scheduler = CosineAnnealingWarmUpRestarts(optimizer, T_0=15, T_mult=1, eta_max=0.01,  T_up=10, gamma=0.6)\n",
    "    \n",
    "    ## Model Train and Eval\n",
    "    Best_valid_loss = 1e9\n",
    "    for epoch in range(1, total_epoch+1):\n",
    "        emb_model.train()\n",
    "        running_loss = 0\n",
    "\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        \n",
    "        for num_iter, batch_data in enumerate(tqdm(loader_train)):\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            X_num, X_cat, label = batch_data\n",
    "            X_num, X_cat, label = X_num.to(device), X_cat.to(device), label.to(device)\n",
    "\n",
    "            latent, _ = emb_model(X_cat,X_num,True)\n",
    "            label =  label.type(torch.LongTensor)\n",
    "            \n",
    "            # backward pass\n",
    "            loss = contrastive_loss(latent, label)\n",
    "            # loss = criterion(output, label.unsqueeze(dim = 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            scheduler.step()  \n",
    "            \n",
    "        if num_iter % 3000 == 0:\n",
    "                print(\"TRAIN: EPOCH %04d / %04d | ITER %04d / %04d | LOSS %.4f\" %\n",
    "                    (epoch, total_epoch, num_iter+1, len(loader_train), running_loss / (num_iter+1)))\n",
    "        print(f'Epoch{epoch} / {total_epoch} Train Loss : {running_loss / len(loader_train)} Current learning rate : {current_lr}')\n",
    "        # print(f'Epoch{epoch} / {total_epoch} Train Loss : {running_loss / len(loader_train)}')\n",
    "        print(f'---------Epoch{epoch} Training Finish---------')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            emb_model.eval()\n",
    "            running_loss = 0\n",
    "\n",
    "            for num_iter, batch_data in enumerate(tqdm(loader_val)):\n",
    "                X_num, X_cat, label = batch_data\n",
    "                X_num, X_cat, label = X_num.to(device), X_cat.to(device), label.to(device)\n",
    "                \n",
    "                latent,_ = emb_model(X_cat,X_num,True)\n",
    "                loss = contrastive_loss(latent, label)\n",
    "                # loss = criterion(output, label.unsqueeze(dim = 1))\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "\n",
    "            if num_iter % 3000 == 0:\n",
    "                print(\"VALID: EPOCH %04d / %04d | ITER %04d / %04d | LOSS %.4f\" %\n",
    "                    (epoch, total_epoch, num_iter+1, len(loader_val), running_loss / (num_iter+1)))\n",
    "                \n",
    "        print(f'Epoch{epoch} / {total_epoch} Valid Loss : {running_loss / len(loader_val)}')\n",
    "\n",
    "\n",
    "        if running_loss / len(loader_val) < Best_valid_loss:\n",
    "            print(f'Best Loss {Best_valid_loss:.4f} -> {running_loss / len(loader_val):.4f} Update! & Save Checkpoint')\n",
    "            Best_valid_loss = running_loss / len(loader_val)\n",
    "            early_stop_counter = 0\n",
    "            torch.save(emb_model.state_dict(),f'{path_model}/FTTransformer.pth')\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "        \n",
    "        print(f'---------Epoch{epoch} Valid Finish---------')\n",
    "        \n",
    "    return running_loss / len(loader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9765f5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_embeded_df(model_name): \n",
    "#     print()\n",
    "#     print('Start Getting the latent space vector(Train, Valid sample)')\n",
    "    \n",
    "#     mimic_df = target_data.copy()\n",
    "#     print(len(mimic_df))\n",
    "#     information = mimic_df[['subject_id', 'stay_id', 'hadm_id']]\n",
    "#     trn_x, trn_y  = load_data(mimic_df)\n",
    "    \n",
    "#     scaler        = MinMaxScaler()\n",
    "#     trn_sclaed_x  = scaler.fit_transform(trn_x)\n",
    "\n",
    "#     trn_tensor_x  = torch.FloatTensor(trn_sclaed_x)\n",
    "#     trn_tensor_y  = torch.LongTensor(trn_y.values) \n",
    "    \n",
    "    \n",
    "#     n_feat = trn_tensor_x.shape[1]\n",
    "        \n",
    "#     train_dataset = TensorDataset(trn_tensor_x, trn_tensor_y)\n",
    "#     for_latent_loader_trn  = torch.utils.data.DataLoader(dataset= train_dataset, batch_size=trn_tensor_x.shape[0], shuffle=False, drop_last=False)\n",
    "    \n",
    "    \n",
    "#     start = time.time()\n",
    "#     model_name.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for X_l, y_l  in for_latent_loader_trn: # Full batch\n",
    "                \n",
    "#                 X_l  = X_l.to(device)\n",
    "#                 latent_vector_train = model_name.forward(X_l)\n",
    "                \n",
    "#                 emb_train_x = pd.DataFrame(np.array(latent_vector_train.cpu()), index = original_index)\n",
    "#                 emb_train = pd.concat([emb_train_x, pd.DataFrame(np.array(y_l))], axis = 1)\n",
    "#                 # emb_train = pd.concat([information, emb_train], axis = 1)\n",
    "#     end = time.time()            \n",
    "#     print()\n",
    "#     print('End, Time consume(min):{}'.format((end - start)/60))  \n",
    "    \n",
    "#     return emb_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fba9128",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-05 14:04:55,871] A new study created in memory with name: no-name-5f25a6be-c3ce-4514-a1cf-2158310123ee\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate :  0.0001 \n",
      "epoch :  50 \n",
      "Embedding Dimension of Input Data :  31 \n",
      "Nums of Attention Layer Depth :  5 \n",
      "drop_rate :  0.6058754443068571 \n",
      "temperature :  0.3387178797742324\n",
      "FTTransformer(\n",
      "  (categorical_embeds): Embedding(58, 31)\n",
      "  (numerical_embedder): NumericalEmbedder()\n",
      "  (transformer): Transformer(\n",
      "    (layers): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): Attention(\n",
      "          (norm): LayerNorm((31,), eps=1e-05, elementwise_affine=True)\n",
      "          (to_qkv): Linear(in_features=31, out_features=408, bias=False)\n",
      "          (to_out): Linear(in_features=136, out_features=31, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): LayerNorm((31,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=31, out_features=248, bias=True)\n",
      "          (2): GEGLU()\n",
      "          (3): Dropout(p=0.6058754443068571, inplace=False)\n",
      "          (4): Linear(in_features=124, out_features=31, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (1): ModuleList(\n",
      "        (0): Attention(\n",
      "          (norm): LayerNorm((31,), eps=1e-05, elementwise_affine=True)\n",
      "          (to_qkv): Linear(in_features=31, out_features=408, bias=False)\n",
      "          (to_out): Linear(in_features=136, out_features=31, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): LayerNorm((31,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=31, out_features=248, bias=True)\n",
      "          (2): GEGLU()\n",
      "          (3): Dropout(p=0.6058754443068571, inplace=False)\n",
      "          (4): Linear(in_features=124, out_features=31, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (2): ModuleList(\n",
      "        (0): Attention(\n",
      "          (norm): LayerNorm((31,), eps=1e-05, elementwise_affine=True)\n",
      "          (to_qkv): Linear(in_features=31, out_features=408, bias=False)\n",
      "          (to_out): Linear(in_features=136, out_features=31, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): LayerNorm((31,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=31, out_features=248, bias=True)\n",
      "          (2): GEGLU()\n",
      "          (3): Dropout(p=0.6058754443068571, inplace=False)\n",
      "          (4): Linear(in_features=124, out_features=31, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (3): ModuleList(\n",
      "        (0): Attention(\n",
      "          (norm): LayerNorm((31,), eps=1e-05, elementwise_affine=True)\n",
      "          (to_qkv): Linear(in_features=31, out_features=408, bias=False)\n",
      "          (to_out): Linear(in_features=136, out_features=31, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): LayerNorm((31,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=31, out_features=248, bias=True)\n",
      "          (2): GEGLU()\n",
      "          (3): Dropout(p=0.6058754443068571, inplace=False)\n",
      "          (4): Linear(in_features=124, out_features=31, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (4): ModuleList(\n",
      "        (0): Attention(\n",
      "          (norm): LayerNorm((31,), eps=1e-05, elementwise_affine=True)\n",
      "          (to_qkv): Linear(in_features=31, out_features=408, bias=False)\n",
      "          (to_out): Linear(in_features=136, out_features=31, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): LayerNorm((31,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=31, out_features=248, bias=True)\n",
      "          (2): GEGLU()\n",
      "          (3): Dropout(p=0.6058754443068571, inplace=False)\n",
      "          (4): Linear(in_features=124, out_features=31, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1365/1365 [01:44<00:00, 13.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1 / 50 Train Loss : 4.8443012890798265 Current learning rate : 0.0\n",
      "---------Epoch1 Training Finish---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5350/5350 [02:35<00:00, 34.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1 / 50 Valid Loss : 2.5669558916359305\n",
      "Best Loss 1000000000.0000 -> 2.5670 Update! & Save Checkpoint\n",
      "---------Epoch1 Valid Finish---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1365/1365 [01:42<00:00, 13.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch2 / 50 Train Loss : 4.844267505603832 Current learning rate : 0.0\n",
      "---------Epoch2 Training Finish---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5350/5350 [02:36<00:00, 34.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch2 / 50 Valid Loss : 2.5669558916359305\n",
      "---------Epoch2 Valid Finish---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1365/1365 [01:41<00:00, 13.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch3 / 50 Train Loss : 4.844285888112945 Current learning rate : 0.0\n",
      "---------Epoch3 Training Finish---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5350/5350 [02:34<00:00, 34.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch3 / 50 Valid Loss : 2.5669558916359305\n",
      "---------Epoch3 Valid Finish---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1365/1365 [01:41<00:00, 13.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch4 / 50 Train Loss : 4.844278640537471 Current learning rate : 0.0\n",
      "---------Epoch4 Training Finish---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5350/5350 [02:34<00:00, 34.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch4 / 50 Valid Loss : 2.5669558916359305\n",
      "---------Epoch4 Valid Finish---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1365/1365 [01:42<00:00, 13.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch5 / 50 Train Loss : 4.844279348631918 Current learning rate : 0.0\n",
      "---------Epoch5 Training Finish---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5350/5350 [02:36<00:00, 34.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch5 / 50 Valid Loss : 2.5669558916359305\n",
      "---------Epoch5 Valid Finish---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1365/1365 [01:40<00:00, 13.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch6 / 50 Train Loss : 4.8442779981173 Current learning rate : 0.0\n",
      "---------Epoch6 Training Finish---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5350/5350 [02:34<00:00, 34.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch6 / 50 Valid Loss : 2.5669558916359305\n",
      "---------Epoch6 Valid Finish---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1365/1365 [01:41<00:00, 13.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch7 / 50 Train Loss : 4.844274843306769 Current learning rate : 0.0\n",
      "---------Epoch7 Training Finish---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5350/5350 [02:34<00:00, 34.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch7 / 50 Valid Loss : 2.5669558916359305\n",
      "---------Epoch7 Valid Finish---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1365/1365 [01:42<00:00, 13.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch8 / 50 Train Loss : 4.844281776833447 Current learning rate : 0.0\n",
      "---------Epoch8 Training Finish---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5350/5350 [02:37<00:00, 34.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch8 / 50 Valid Loss : 2.5669558916359305\n",
      "---------Epoch8 Valid Finish---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1365/1365 [01:43<00:00, 13.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch9 / 50 Train Loss : 4.844274085956616 Current learning rate : 0.0\n",
      "---------Epoch9 Training Finish---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5350/5350 [02:34<00:00, 34.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch9 / 50 Valid Loss : 2.5669558916359305\n",
      "---------Epoch9 Valid Finish---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1365/1365 [01:43<00:00, 13.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch10 / 50 Train Loss : 4.844282842643095 Current learning rate : 0.0\n",
      "---------Epoch10 Training Finish---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5350/5350 [02:38<00:00, 33.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch10 / 50 Valid Loss : 2.5669558916359305\n",
      "---------Epoch10 Valid Finish---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 216/1365 [00:16<01:27, 13.12it/s]\n",
      "[W 2024-01-05 14:48:15,425] Trial 0 failed with parameters: {'dim': 31, 'depth': 5, 'FeedForward Layer dropout': 0.6058754443068571, 'temp': 0.3387178797742324} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\DAHS\\anaconda3\\envs\\DL\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\DAHS\\AppData\\Local\\Temp\\ipykernel_15640\\2966313273.py\", line 85, in train\n",
      "    loss.backward()\n",
      "  File \"c:\\Users\\DAHS\\anaconda3\\envs\\DL\\lib\\site-packages\\torch\\_tensor.py\", line 396, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"c:\\Users\\DAHS\\anaconda3\\envs\\DL\\lib\\site-packages\\torch\\autograd\\__init__.py\", line 173, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "[W 2024-01-05 14:48:15,427] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Set parameters\u001b[39;00m\n\u001b[0;32m     13\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(sampler\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mTPESampler(), direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[0;32m     16\u001b[0m pruned_trials \u001b[38;5;241m=\u001b[39m [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m study\u001b[38;5;241m.\u001b[39mtrials \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m optuna\u001b[38;5;241m.\u001b[39mtrial\u001b[38;5;241m.\u001b[39mTrialState\u001b[38;5;241m.\u001b[39mPRUNED]\n\u001b[0;32m     17\u001b[0m complete_trials \u001b[38;5;241m=\u001b[39m [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m study\u001b[38;5;241m.\u001b[39mtrials \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m optuna\u001b[38;5;241m.\u001b[39mtrial\u001b[38;5;241m.\u001b[39mTrialState\u001b[38;5;241m.\u001b[39mCOMPLETE]\n",
      "File \u001b[1;32mc:\\Users\\DAHS\\anaconda3\\envs\\DL\\lib\\site-packages\\optuna\\study\\study.py:443\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    341\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    350\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \n\u001b[0;32m    352\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 443\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DAHS\\anaconda3\\envs\\DL\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\DAHS\\anaconda3\\envs\\DL\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\DAHS\\anaconda3\\envs\\DL\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\DAHS\\anaconda3\\envs\\DL\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[7], line 85\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(trial, search)\u001b[0m\n\u001b[0;32m     83\u001b[0m loss \u001b[38;5;241m=\u001b[39m contrastive_loss(latent, label)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# loss = criterion(output, label.unsqueeze(dim = 1))\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     88\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\DAHS\\anaconda3\\envs\\DL\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DAHS\\anaconda3\\envs\\DL\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import optuna\n",
    "\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= '0'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']= '1'\n",
    "n_gpu             = 1\n",
    "device            = torch.device('cuda:0')\n",
    "\n",
    "# Set parameters\n",
    "\n",
    "study = optuna.create_study(sampler=optuna.samplers.TPESampler(), direction=\"minimize\")\n",
    "study.optimize(train, n_trials = 30) \n",
    "\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "67886bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden': [98, 65, 40]}\n",
      "learning_rate :  0.0006 \n",
      "epoch :  120 \n",
      "drop_rate :  0.223573505016481 \n",
      "temperature :  0.3137977865167101\n",
      "Contrastive_Embedding(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=214, out_features=98, bias=True)\n",
      "    (1): BatchNorm1d(98, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Dropout(p=0.223573505016481, inplace=False)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=98, out_features=65, bias=True)\n",
      "    (5): BatchNorm1d(65, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Dropout(p=0.223573505016481, inplace=False)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=65, out_features=40, bias=True)\n",
      "    (9): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): Dropout(p=0.223573505016481, inplace=False)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      ")\n",
      "epoch 1  time: 78.0996sec trn_contrastive:  4.9233\n",
      "epoch 2  time: 77.5303sec trn_contrastive:  4.9232\n",
      "epoch 3  time: 80.7687sec trn_contrastive:  4.9232\n",
      "epoch 4  time: 80.3857sec trn_contrastive:  4.9232\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\DAHS\\MIMIC-IV-Data-Pipeline\\MIMIC_pipeline\\supervised_contrastive_learning\\Contrastive_embedding_NCirculatoryState.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DAHS/MIMIC-IV-Data-Pipeline/MIMIC_pipeline/supervised_contrastive_learning/Contrastive_embedding_NCirculatoryState.ipynb#X23sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m n_gpu             \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DAHS/MIMIC-IV-Data-Pipeline/MIMIC_pipeline/supervised_contrastive_learning/Contrastive_embedding_NCirculatoryState.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m device            \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/DAHS/MIMIC-IV-Data-Pipeline/MIMIC_pipeline/supervised_contrastive_learning/Contrastive_embedding_NCirculatoryState.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m model \u001b[39m=\u001b[39m train(trial\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, search \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[1;32mc:\\Users\\DAHS\\MIMIC-IV-Data-Pipeline\\MIMIC_pipeline\\supervised_contrastive_learning\\Contrastive_embedding_NCirculatoryState.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/DAHS/MIMIC-IV-Data-Pipeline/MIMIC_pipeline/supervised_contrastive_learning/Contrastive_embedding_NCirculatoryState.ipynb#X23sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m \u001b[39m# current_lr = optimizer.param_groups[0][\"lr\"]\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/DAHS/MIMIC-IV-Data-Pipeline/MIMIC_pipeline/supervised_contrastive_learning/Contrastive_embedding_NCirculatoryState.ipynb#X23sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/DAHS/MIMIC-IV-Data-Pipeline/MIMIC_pipeline/supervised_contrastive_learning/Contrastive_embedding_NCirculatoryState.ipynb#X23sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m \u001b[39mfor\u001b[39;00m j, (X, y)  \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/DAHS/MIMIC-IV-Data-Pipeline/MIMIC_pipeline/supervised_contrastive_learning/Contrastive_embedding_NCirculatoryState.ipynb#X23sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m     X  \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/DAHS/MIMIC-IV-Data-Pipeline/MIMIC_pipeline/supervised_contrastive_learning/Contrastive_embedding_NCirculatoryState.ipynb#X23sZmlsZQ%3D%3D?line=121'>122</a>\u001b[0m     y  \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\DAHS\\anaconda3\\envs\\DL\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\DAHS\\anaconda3\\envs\\DL\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\DAHS\\anaconda3\\envs\\DL\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 52\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\DAHS\\anaconda3\\envs\\DL\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:175\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    172\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 175\u001b[0m     \u001b[39mreturn\u001b[39;00m [default_collate(samples) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\DAHS\\anaconda3\\envs\\DL\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:175\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    172\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 175\u001b[0m     \u001b[39mreturn\u001b[39;00m [default_collate(samples) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\DAHS\\anaconda3\\envs\\DL\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:141\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    139\u001b[0m         storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mstorage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    140\u001b[0m         out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[1;32m--> 141\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n\u001b[0;32m    142\u001b[0m \u001b[39melif\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__module__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mstr_\u001b[39m\u001b[39m'\u001b[39m \\\n\u001b[0;32m    143\u001b[0m         \u001b[39mand\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mstring_\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mif\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mndarray\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmemmap\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    145\u001b[0m         \u001b[39m# array of string classes and object\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import optuna\n",
    "\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= '0'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']= '1'\n",
    "n_gpu             = 1\n",
    "device            = torch.device('cuda:0')\n",
    "\n",
    "model = train(trial=1, search = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea05d267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAioAAAHACAYAAACMB0PKAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABfeUlEQVR4nO3dd3hUZeL28e+kTQpJCAFCAmlIXTpSlLosKKKioquuogKLnSYIktB7gijqooviIugqoKIodhApAqKIoigdkhAgdEivM8/7hy/5baQGkpxJcn+ua66LOXPmzD2kzJ3zPOccmzHGICIiIuKC3KwOICIiInIhKioiIiLislRURERExGWpqIiIiIjLUlERERERl6WiIiIiIi5LRUVERERcloqKiIiIuCwVFREREXFZKioiIiLisipMUVm3bh29e/cmLCwMm83GRx99VKqvFxcXR9u2bfH396dmzZrccccd7Nq1q8g6OTk5DBo0iODgYKpUqcJdd93F0aNHi6yzatUqOnTogL+/P7Vq1WL06NEUFBSUanYREZHyosIUlczMTFq0aMErr7xSJq+3du1aBg0axKZNm1i5ciX5+fnceOONZGZmFq4zfPhwPvnkE95//33Wrl3L4cOHufPOOwsf/+WXX7j55pu56aab+Pnnn3n33XdZvnw5MTExZfIeREREXJ6pgACzbNmyIstycnLM008/bcLCwoyvr69p166dWb16dYm95rFjxwxg1q5da4wx5syZM8bT09O8//77hevs2LHDAOa7774zxhgTGxtr2rRpU2Q7y5cvN97e3iYtLa3EsomIiJRXFWaPyqUMHjyY7777jiVLlvDrr79y9913c9NNN7Fnz54S2X5qaioA1apVA2DLli3k5+fTo0ePwnUaNWpEREQE3333HQC5ubl4e3sX2Y6Pjw85OTls2bKlRHKJiIiUZ5WiqBw4cIAFCxbw/vvv07lzZ6655hpGjhxJp06dWLBgwVVv3+l08tRTT9GxY0eaNm0KwJEjR/Dy8qJq1apF1g0JCeHIkSMA9OzZk40bN7J48WIcDgeHDh1iypQpAKSkpFx1LhERkfKuUhSVbdu24XA4aNCgAVWqVCm8rV27ln379gGwc+dObDbbRW8XmjsyaNAgfvvtN5YsWVKsXDfeeCOzZs3i8ccfx26306BBA26++WYA3NwqxZdGRETkojysDlAWMjIycHd3Z8uWLbi7uxd5rEqVKgDUrVuXHTt2XHQ7wcHB5ywbPHgwn376KevWraNOnTqFy2vVqkVeXh5nzpwpslfl6NGj1KpVq/D+iBEjGD58OCkpKQQFBZGYmEhsbCx169a9krcqIiJSoVSKotKqVSscDgfHjh2jc+fO513Hy8uLRo0aXfY2jTEMGTKEZcuWsWbNGqKjo4s8fu211+Lp6cmqVau46667ANi1axcHDhzg+uuvL7KuzWYjLCwMgMWLFxMeHk7r1q2L8xZFREQqpApTVDIyMti7d2/h/YSEBLZu3Uq1atVo0KABffv25aGHHuL555+nVatWHD9+nFWrVtG8eXNuueWWYr/eoEGDWLRoER9//DH+/v6F804CAwPx8fEhMDCQgQMHMmLECKpVq0ZAQABDhgzh+uuv57rrrivczqxZs7jppptwc3Pjww8/JD4+nvfee++cPT8iIiKVktWHHZWU1atXG+CcW79+/YwxxuTl5ZkJEyaYqKgo4+npaUJDQ02fPn3Mr7/+ekWvd77XAsyCBQsK18nOzjZPPvmkCQoKMr6+vqZPnz4mJSWlyHa6detmAgMDjbe3t2nfvr35/PPPr/S/QEREpMKxGWOMRR1JRERE5KJ0aImIiIi4LBUVERERcVnlejKt0+nk8OHD+Pv7Y7PZrI4jIiIil8EYQ3p6OmFhYZc8b1i5LiqHDx8mPDzc6hgiIiJyBZKTk4ucg+x8ynVR8ff3B/54owEBARanERERkcuRlpZGeHh44ef4xZTronJ2uCcgIEBFRUREpJy5nGkbmkwrIiIiLktFRURERFyWioqIiIi4rHI9R+VyORwO8vPzrY4hIuLyPD09da0xcSkVuqgYYzhy5AhnzpyxOoqISLlRtWpVatWqpfNTiUuo0EXlbEmpWbMmvr6++qETEbkIYwxZWVkcO3YMgNDQUIsTiVTgouJwOApLSnBwsNVxRETKBR8fHwCOHTtGzZo1NQwklquwk2nPzknx9fW1OImISPly9vem5vaJK6iwReUsDfeIiBSPfm+KK6nwRUVERETKLxWVSiIqKooXX3zxstdfs2YNNputwhwxZbPZ+Oijj6yO4VIWLlxI1apVC+9PmjSJli1bWpZHROR8VFRcjM1mu+ht0qRJV7TdzZs38+ijj172+h06dCAlJYXAwMArer3iMMYwb9482rdvT5UqVahatSpt2rThxRdfJCsrq0ReIyUlhV69epXIti4lOzubatWqUb16dXJzc8vkNUvCyJEjWbVqVam+xpdffonNZuPIkSNFloeGhhIVFVVkWWJiIjabrdQzXY7+/fsX/gx6enoSHR3NM888Q05OTols/+x73bp1a4lsr6L9oSGVm4qKi0lJSSm8vfjiiwQEBBRZNnLkyMJ1jTEUFBRc1nZr1KhRrInFXl5eZXYehQcffJCnnnqK22+/ndWrV7N161bGjx/Pxx9/zIoVK0rkNWrVqoXdbi+RbV3KBx98QJMmTWjUqFG52otTpUqVUj9CrlOnTnh4eLBmzZrCZTt27CA7O5vTp0+TmJhYuHz16tXY7XY6dux4Ra9V0hNBb7rpJlJSUti/fz8vvPACr732GhMnTizR1ygJmgArJWlT2iZynCVTyK+UioqLqVWrVuEtMDAQm81WeH/nzp34+/vzxRdfcO2112K321m/fj379u3j9ttvJyQkhCpVqtC2bVu+/vrrItv989CPzWbjP//5D3369MHX15f69euzfPnywsf//BfZ2WGCr776isaNG1OlSpXCX9xnFRQUMHToUKpWrUpwcDCjR4+mX79+3HHHHRd8v++99x7vvPMOixcvZsyYMbRt25aoqChuv/12vvnmG7p16waA0+lkypQp1KlTB7vdTsuWLfnyyy8Lt5OXl8fgwYMJDQ3F29ubyMhI4uLiirzfs6Xh7F+vH374Id26dcPX15cWLVrw3XffFcm2fv16OnfujI+PD+Hh4QwdOpTMzMxLfg3nz5/PAw88wAMPPMD8+fPPefxy/+9XrVpFmzZt8PX1pUOHDuzatavIdj7++GNat26Nt7c3devWZfLkyUWK6+zZs2nWrBl+fn6Eh4fz5JNPkpGRccHcfx766d+/P3fccQfPPfccoaGhBAcHM2jQoCIfhCkpKdxyyy34+PgQHR3NokWLLjrMePb783+Lypo1a+jUqRMdO3Y8Z/l1112Ht7c3X375JZ06dSr83rr11lvZt29f4bpnv6bvvvsuXbt2xdvbm3feeafwPcyYMYOQkBCqVq3KlClTKCgoYNSoUVSrVo06deqwYMGCC/6/nGW326lVqxbh4eHccccd9OjRg5UrVxY+7nQ6iYuLIzo6Gh8fH1q0aMHSpUsLHz99+jR9+/alRo0a+Pj4UL9+/cLXjY6OBqBVq1bYbDb++te/An/sCb3hhhuoXr06gYGBdO3alZ9++qlILpvNxty5c7ntttvw8/PjkUceKfy5CQoKwmaz0b9/fwCWLl1Ks2bN8PHxITg4mB49elzW97RUPvnOfGYfnM2gvYOYfXC2tWFMOZaammoAk5qaes5j2dnZZvv27SY7O7twmdPpNFkFWWV+czqdV/T+FixYYAIDAwvvr1692gCmefPmZsWKFWbv3r3m5MmTZuvWrebVV18127ZtM7t37zbjxo0z3t7eJikpqfC5kZGR5oUXXii8D5g6deqYRYsWmT179pihQ4eaKlWqmJMnTxZ5rdOnTxdm8fT0ND169DCbN282W7ZsMY0bNzb3339/4TanTZtmqlWrZj788EOzY8cO8/jjj5uAgABz++23X/A93nbbbaZhw4aX/L+YPXu2CQgIMIsXLzY7d+40zzzzjPH09DS7d+82xhgza9YsEx4ebtatW2cSExPNt99+axYtWlTk/S5btswYY0xCQoIBTKNGjcynn35qdu3aZf7+97+byMhIk5+fb4wxZu/evcbPz8+88MILZvfu3WbDhg2mVatWpn///hfNuXfvXmO3282pU6fMyZMnjbe3t0lMTCyyzuX+37dv396sWbPG/P7776Zz586mQ4cOhdtYt26dCQgIMAsXLjT79u0zK1asMFFRUWbSpEmF67zwwgvmm2++MQkJCWbVqlWmYcOG5oknnih8/M/fXxMnTjQtWrQovN+vXz8TEBBgHn/8cbNjxw7zySefGF9fXzNv3rzCdXr06GFatmxpNm3aZLZs2WK6du1qfHx8inyv/dmYMWNMgwYNCu/ffffdZtasWSY+Pt489NBDhcsjIiIK38/SpUvNBx98YPbs2WN+/vln07t3b9OsWTPjcDiMMf/3NY2KijIffPCB2b9/vzl8+LDp16+f8ff3N4MGDTI7d+408+fPN4Dp2bOnmT59utm9e7eZOnWq8fT0NMnJyRfM3K9fvyLfx9u2bTO1atUy7du3L1w2bdo006hRI/Pll1+affv2mQULFhi73W7WrFljjDFm0KBBpmXLlmbz5s0mISHBrFy50ixfvtwYY8wPP/xgAPP111+blJSUwu+FVatWmf/+979mx44dZvv27WbgwIEmJCTEpKWlFb4uYGrWrGneeOMNs2/fPpOYmGg++OADA5hdu3aZlJQUc+bMGXP48GHj4eFhZs+ebRISEsyvv/5qXnnlFZOenn7O+z3f70+pPA7mHDQP7njQtN7S2rTe0to8l/yccTgdJfoaF/v8/rNKVVSyCrIK/+PL8pZVkHVF7+9CReWjjz665HObNGli5syZU3j/fEVl3LhxhfczMjIMYL744osir/W/RQUwe/fuLXzOK6+8YkJCQgrvh4SEmFmzZhXeLygoMBERERctKo0bNza33XbbJd9PWFiYmT59epFlbdu2NU8++aQxxpghQ4aYv/3tbxcshecrKv/5z38KH//9998NYHbs2GGMMWbgwIHm0UcfLbKNb7/91ri5uV30l/eYMWPMHXfcUXj/9ttvNxMnTjwny+X833/99deF63z22WcGKHzt7t27mxkzZhTZ7n//+18TGhp6wWzvv/++CQ4OLrx/OUUlMjLSFBQUFC67++67zb333muMMWbHjh0GMJs3by58fM+ePQa4aFFZuXKlAczhw4eNMcbUrFnT/PDDD2bjxo0mMjLSGGPMvn37DGDWrl173m0cP37cAGbbtm3GmP/7mr744otF1jv7Hs4WGmOMadiwoencuXPh/YKCAuPn52cWL158wcz9+vUz7u7uxs/Pz9jtdgMYNzc3s3TpUmOMMTk5OcbX19ds3LixyPMGDhxo7rvvPmOMMb179zYDBgw47/bP5v/5558vmMEYYxwOh/H39zeffPJJ4TLAPPXUU0XW+/PPrzHGbNmyxQDnFOfzUVGpvFadWmW6bO1iWm9pbf669a9mzek1pfI6xSkqGvoph9q0aVPkfkZGBiNHjqRx48ZUrVqVKlWqsGPHDg4cOHDR7TRv3rzw335+fgQEBBSeOvt8fH19ueaaawrvh4aGFq6fmprK0aNHadeuXeHj7u7uXHvttRfNYIy56OMAaWlpHD58+Jy5Ch07dmTHjh3AH8MUW7dupWHDhgwdOvSy5rb87/s/e6rws+/nl19+YeHChVSpUqXw1rNnT5xOJwkJCefdnsPh4M033+SBBx4oXPbAAw+wcOFCnE7nBV/7Qv/3l8o3ZcqUIvkeeeQRUlJSCicgf/3113Tv3p3atWvj7+/Pgw8+yMmTJ4s1QblJkyZFzkz6v1/zXbt24eHhQevWrQsfr1evHkFBQRfdZocOHfDy8mLNmjVs376d7OxsWrduTZs2bTh+/DgJCQmsWbMGHx8frrvuOgD27NnDfffdR926dQkICCicePvn7/E//2ycfQ9ubv/3qy4kJIRmzZoV3nd3dyc4OPii3/sA3bp1Y+vWrXz//ff069ePAQMGcNdddwGwd+9esrKyuOGGG4p8Td56663CIaonnniCJUuW0LJlS5555hk2btx40dcDOHr0KI888gj169cnMDCQgIAAMjIyLut9/1mLFi3o3r07zZo14+677+b111/n9OnTl3yeVA65zlyeTX6WUQmjyHBk0MyvGYsaLaJr1a5WR6u4p9A/H283b9a3WG/J65YkPz+/IvdHjhzJypUree6556hXrx4+Pj78/e9/Jy8v76Lb8fT0LHLfZrOd84F6qfUvp2hcTIMGDdi5c+dVbQOgdevWJCQk8MUXX/D1119zzz330KNHjyJzBP7sf9/P2UnDZ99/RkYGjz32GEOHDj3neREREefd3ldffcWhQ4e49957iyx3OBysWrWKG2644byvffb1//x/f6l8kydP5s477zwnh7e3N4mJidx666088cQTTJ8+nWrVqrF+/XoGDhxIXl7eZU+sLu73yOXw9fWlXbt2rF69mlOnTtGpUyfc3d1xd3enQ4cOrF69mtWrV9OxY0e8vLwA6N27N5GRkbz++uuEhYXhdDpp2rTpOd/jf/7ZuNB7uJL35efnR7169QB44403aNGiBfPnz2fgwIGFc38+++wzateuXeR5Zydx9+rVi6SkJD7//HNWrlxJ9+7dGTRoEM8999wFX7Nfv36cPHmSl156icjISOx2O9dff/1lve8/c3d3Z+XKlWzcuJEVK1YwZ84cxo4dy/fff184R0Yqp+ScZEYnjGZX9h/z4PqF9OOJsCfwtHle4pllo1IVFZvNho+7j9UxStyGDRvo378/ffr0Af74EPvfoyfKQmBgICEhIWzevJkuXboAf3xA//TTTxc9N8f999/PP/7xDz7++GNuv/32Io8ZY0hLSyMwMJCwsDA2bNhA167/1+43bNhQZA9OQEAA9957L/feey9///vfuemmmzh16hTVqlUr9vtp3bo127dvL/xguhzz58/nH//4B2PHji2yfPr06cyfP79IUblarVu3ZteuXRfMt2XLFpxOJ88//3zh3oT33nuvxF4foGHDhhQUFPDzzz8X7jnbu3fvZf2V3q1bN5YsWcLp06cLJ44CdOnShTVr1rB27Voef/xxAE6ePMmuXbt4/fXX6dy5M/DHRGcrubm5MWbMGEaMGMH999/PX/7yF+x2OwcOHCjyPfpnNWrUoF+/fvTr14/OnTszatQonnvuucJC5nA4iqy/YcMG/v3vf3PzzTcDkJyczIkTJy6Z70Lbs9lsdOzYkY4dOzJhwgQiIyNZtmwZI0aMKNb7l4pjxakVTDswjUxnJoHugUyJmkKnwE5WxyrC8qJy6NAhRo8ezRdffEFWVhb16tVjwYIFl7UrU/5Qv359PvzwQ3r37o3NZmP8+PFX/VfvlRgyZAhxcXHUq1ePRo0aMWfOHE6fPn3RQ5zvueceli1bxn333ce4ceO48cYbqVGjBtu2beOFF15gyJAh3HHHHYwaNYqJEydyzTXX0LJlSxYsWMDWrVt55513gD+OcAkNDaVVq1a4ubnx/vvvU6tWrSInNCuO0aNHc9111zF48GAefvhh/Pz82L59OytXruTll18+Z/3jx4/zySefsHz5cpo2bVrksYceeog+ffpccWk6nwkTJnDrrbcSERHB3//+d9zc3Pjll1/47bffmDZtGvXq1SM/P585c+bQu3dvNmzYwKuvvloir31Wo0aN6NGjB48++ihz587F09OTp59+Gh8fn0se1t6tWzemTp3KkSNHihxy37VrV2bNmkV6enqRI1eCg4OZN28eoaGhHDhwgJiYmBJ9L1fi7rvvZtSoUbzyyiuMHDmSkSNHMnz4cJxOJ506dSI1NZUNGzYQEBBAv379mDBhAtdeey1NmjQhNzeXTz/9lMaNGwNQs2ZNfHx8+PLLL6lTpw7e3t4EBgZSv359/vvf/9KmTRvS0tIYNWpU4UUDLyYyMhKbzcann37KzTffjI+PD7///jurVq3ixhtvpGbNmnz//fccP368MINULjnOHGYfnM0HJz4AoKVfS2ZEzyDEK8TiZOeydI7K6dOn6dixI56ennzxxRds376d559//pJj3FLU7NmzCQoKokOHDvTu3ZuePXsWmTdQVkaPHs19993HQw89xPXXX184r8Pb+8JDXzabjUWLFjF79mw++ugjunbtSvPmzZk0aRK33347PXv2BGDo0KGMGDGCp59+mmbNmvHll1+yfPly6tevD4C/vz/PPvssbdq0oW3btiQmJvL5558XmZtQHM2bN2ft2rXs3r2bzp0706pVKyZMmEBYWNh513/rrbfw8/Oje/fu5zzWvXt3fHx8ePvtt68oy/n07NmTTz/9lBUrVtC2bVuuu+46XnjhBSIjI4E/5iPMnj2bmTNn0rRpU955550ih2uXlLfeeouQkBC6dOlCnz59eOSRR/D397/o1xzg+uuvx263Y4wpMo+pffv25OfnFx7GDH/svViyZAlbtmyhadOmDB8+nFmzZpX4eykuDw8PBg8ezLPPPktmZiZTp05l/PjxxMXF0bhxY2666SY+++yzwmEVLy8vYmNjad68OV26dMHd3Z0lS5YUbutf//oXr732GmFhYYV7F+fPn8/p06dp3bo1Dz74IEOHDqVmzZqXzFa7dm0mT55MTEwMISEhDB48mICAANatW8fNN99MgwYNGDduHM8//3yZnQhRXEdiTiL9d/XngxMfYMPGwFoDea3Bay5ZUgBs5monGVyFmJgYNmzYwLfffntFzz87LJCamkpAQECRx3JyckhISCA6OvqSvzSldDidTho3bsw999zD1KlTrY4jZeDgwYOEh4cXTuSV8km/Pyuuz09+zozkGWQ7swnyCGJa1DSuC7iuzHNc7PP7zywd+lm+fDk9e/bk7rvvZu3atdSuXZsnn3ySRx55xMpYcoWSkpJYsWIFXbt2JTc3l5dffpmEhATuv/9+q6NJKfnmm2/IyMigWbNmpKSk8MwzzxAVFVU4T0lEXEO2M5tZybP4+OTHALSp0oZp0dOo4VnD4mSXZmlR2b9/P3PnzmXEiBGMGTOGzZs3M3ToULy8vOjXr9856+fm5ha5dkpaWlpZxpVLcHNzY+HChYwcORJjDE2bNuXrr7/WGHgFlp+fz5gxY9i/fz/+/v506NCBd95555yjakTEOvuz9zM6YTT7c/Zjw8YjtR7h4dCHcbe5X/rJLsDSoR8vLy/atGlT5HwCQ4cOZfPmzeeczhz+OMX35MmTz1muoR8RkZKj358VgzGGT059QvyBeHJNLsEewUyLnkY7/3aXfnIpK87Qj6WTaUNDQ/nLX/5SZFnjxo0veKKy2NhYUlNTC2/JycllEVNERKRcyXJkMTFpIpOTJpNrcmnv357FjRe7REkpLkuHfjp27HjOhdZ2795deOTCn9nt9mJfAdfCHUYiIuWSfm+Wb3uy9hCTEENibiJuuPFE2BP0D+mPm618noze0qIyfPhwOnTowIwZM7jnnnv44YcfmDdvHvPmzbvqbZ8dI8/Kyrqs8w6IiMgfzl5mQXONyhdjDMtOLuO55OfINbnU9KzJjOgZtKrSyupoV8XSotK2bVuWLVtGbGwsU6ZMITo6mhdffJG+ffte9bbd3d2pWrVq4fU7fH19L3kSKhGRyswYQ1ZWFseOHaNq1apFrvMkri3DkcGMAzP46vRXAHQM6MjkqMkEeZT/85JZOpn2al1qMo4xhiNHjnDmzJmyDyciUk5VrVqVWrVq6Y+7cmJn1k5iEmJIzk3GHXcG1x7MAzUfcOmhnnJzHpXSZrPZCA0NpWbNmuTn51sdR0TE5Xl6empPSjlhjOH9E+8z++Bs8k0+IZ4hxEfH07xK80s/uRyp0EXlrLNXZhUREakI0gvSmXpgKqvOrAKga2BXJkZOJNAj0OJkJa9SFBUREZGK4vfM34lNiOVQ3iE8bB4Mqz2M+2rcV2GH6lRUREREygFjDIuPL+alQy9RYAoI8wojPjqeJn5NrI5WqlRUREREXFxqQSqTkyazNnUtAH+r+jcmREzA38Pf4mSlT0VFRETEhW3L3EZMQgxH8o7gafNkRJ0R3F397go71PNnKioiIiIuyGmcvH3sbV4+9DIOHNSx1yE+Op7GvpXrQq8qKiIiIi7mdMFpJiVOYn3aegBuDLqRsRFjqeJexeJkZU9FRURExIX8nPEzYxLGcCz/GF42L0aFj6JPcJ9KM9TzZyoqIiIiLsBpnCw8upBXD7+KAweR9khmRs+kvm99q6NZSkVFRETEYqfyTzE+cTyb0jcBcHO1m4kNj8XX3dfiZNZTUREREbHQj+k/MjZxLCfyT2C32YmJiKF3td6Vdqjnz1RURERELOAwDuYfmc/rKa/jxEld77rER8dzjc81VkdzKSoqIiIiZexE/gnGJY5jc/pmAG4Lvo1n6jyDj7uPxclcj4qKiIhIGfo+7XvGJY7jVMEpfNx8iA2P5ZbgW6yO5bJUVERERMpAgSlgXso83jjyBgZDPe96xNeNJ9o72upoLk1FRUREpJQdyzvG2MSx/JTxEwB3Vb+LEXVG4O3mbXEy16eiIiIiUoo2pm5kfNJ4zhScwc/Nj7ERY+lZrafVscoNFRUREZFSkG/ymXt4Lm8efROAhj4NiY+OJ8I7wuJk5YuKioiISAlLyUthbMJYfsn8BYB7atzDU7Wfwu5mtzhZ+aOiIiIiUoLWnVnHxKSJpDnSqOJehQkRE+ge1N3qWOWWioqIiEgJyHfmM+fwHN459g4ATXybMCN6BnXsdSxOVr6pqIiIiFylQ7mHiE2I5fes3wG4v+b9DA0biqebp8XJyj8VFRERkavwzelvmHxgMhmODALcA5gUOYmuVbtaHavCUFERERG5AnnOPF489CLvHn8XgGZ+zYiLiiPUHmpxsopFRUVERKSYknOSiUmIYWf2TgD6hfTjibAn8LRpqKekqaiIiIgUw4rTK5iWNI1MZyaB7oFMiZpCp8BOVseqsFRURERELkOOM4fZB2fzwYkPAGjp15IZ0TMI8QqxOFnFpqIiIiJyCYk5icQkxLAnew82bAwIGcBjYY/hYdPHaGnT/7CIiMhFfH7qc2YcmEG2M5sgjyCmRk3l+oDrrY5VaaioiIiInEe2M5tZybP4+OTHAFxb5VqmR0+nhmcNi5NVLioqIiIif7I/ez8xCTHsy9mHDRuP1HqEh0Mfxt3mbnW0SkdFRURE5H8sP7mcmckzyXHmEOwRzLToabTzb2d1rEpLRUVERATIcmQRnxzPZ6c+A6C9f3umRk0l2DPY4mSVm4qKiIhUenuy9xCzP4bE3ETccOPx0McZUGsAbjY3q6NVeioqIiJSaRljWHZyGc8lP0euyaWGZw1mRM2gtX9rq6PJ/6eiIiIilVKGI4MZB2bw1emvAOgY0JHJkZMJ8gyyOJn8LxUVERGpdHZm7SQmIYbk3GTccWdQ2CAeDHlQQz0uSEVFREQqDWMM7594n9kHZ5Nv8gnxDCEuOo4WVVpYHU0uQEVFREQqhXRHOlOTprLqzCoAugR2YVLkJAI9Ai1OJhejoiIiIhXe9sztxCTEcCjvEB42D4aGDeX+mvdjs9msjiaXoKIiIiIVljGGxccX89KhlygwBYR5hREXHUdTv6ZWR5PLpKIiIiIVUmpBKpOTJrM2dS0Af6v6NyZETMDfw9/iZFIcKioiIlLhbMvcRkxCDEfyjuBp82R47eHcU+MeDfWUQyoqIiJSYTiNk7ePvc3Lh17GgYM69jrER8fT2Lex1dHkCqmoiIhIhXC64DSTEiexPm09ADdUvYFxkeOo4l7F4mRyNVRURESk3Ps542fGJozlaP5RvGxejKwzkjur36mhngpARUVERMotp3Gy8OhCXj38Kg4cRNojiY+Op4FvA6ujSQlRURERkXLpVP4pxieOZ1P6JgB6BfUiNiIWP3c/i5NJSVJRERGRcufH9B8ZmziWE/knsNvsjA4fzW3Bt2mopwKy9OpLkyZNwmazFbk1atTIykgiIuLCHMbBvJR5PLHnCU7knyDaO5q3Gr3F7dVvV0mpoCzfo9KkSRO+/vrrwvseHpZHEhERF3Qi/wTjEsexOX0zALcF38YzdZ7Bx93H4mRSmixvBR4eHtSqVcvqGCIi4sK+T/uecYnjOFVwCm83b8aEj+GW4FusjiVlwPKismfPHsLCwvD29ub6668nLi6OiIiI866bm5tLbm5u4f20tLSyiikiIhYoMAXMS5nHG0fewGCo512P+LrxRHtHWx1Nyoilc1Tat2/PwoUL+fLLL5k7dy4JCQl07tyZ9PT0864fFxdHYGBg4S08PLyME4uISFk5lneMJ/Y8wfwj8zEY+gT34c1Gb6qkVDI2Y4yxOsRZZ86cITIyktmzZzNw4MBzHj/fHpXw8HBSU1MJCAgoy6giIlKKNqZuZHzSeM4UnMHXzZexEWO5qdpNVseSEpKWlkZgYOBlfX5bPvTzv6pWrUqDBg3Yu3fveR+32+3Y7fYyTiUiImUl3+Qz9/Bc3jz6JgANfRoSHx1PhPf5pwRIxWfp0M+fZWRksG/fPkJDQ62OIiIiZexI3hEe2/1YYUm5u/rdLGi4QCWlkrN0j8rIkSPp3bs3kZGRHD58mIkTJ+Lu7s59991nZSwRESlj686sY1LSJFIdqfi5+TEhcgI9gnpYHUtcgKVF5eDBg9x3332cPHmSGjVq0KlTJzZt2kSNGjWsjCUiImUk35nPnMNzeOfYOwA09m1MXHQc4XYdLCF/sLSoLFmyxMqXFxERCx3KPURsQiy/Z/0OwH017mNo7aF4uXlZnExciUtNphURkcrhmzPfMDlpMhmODPzd/ZkYOZFuVbtZHUtckIqKiIiUmTxnHi8eepF3j78LQFPfpsRHxxNq10EUcn4qKiIiUiaSc5OJTYhlR9YOAB6s+SCDag/C0+ZpcTJxZSoqIiJS6lacXsG0pGlkOjMJdA9kctRkOgd2tjqWlAMqKiIiUmpynbk8f/B5PjjxAQAt/FoQFx1HiFeIxcmkvFBRERGRUpGUk0RMQgy7s3cDMCBkAI+HPY6HTR89cvn03SIiIiXu81OfM+PADLKd2QR5BDElagodAjpYHUvKIRUVEREpMdnObGYlz+Ljkx8DcG2Va5keNZ0aXjqRp1wZFRURESkR+7P3E5MQw76cfdiw8XCth3kk9BHcbe5WR5NyTEVFRESu2vKTy5mZPJMcZw7BHsFMi5pGu4B2VseSCkBFRURErliWI4v45Hg+O/UZAO392zM1airBnsEWJ5OKQkVFRESuyJ7sPcTsjyExNxE33Hgs9DEG1BqgoR4pUSoqIiJSLMYYPjr5EbOSZ5FrcqnhWYPpUdO51v9aq6NJBaSiIiIily3Tkcn0A9P56vRXAHQI6MCUyCkEeQZZnEwqKhUVERG5LLuydhGTEMOB3AO4486TYU/yUMhDuNncrI4mFZiKioiIXJQxhqUnljL74GzyTB4hniHMiJ5ByyotrY4mlYCKioiIXFC6I51pSdP4+szXAHQO6MykqElU9ahqbTCpNFRURETkvLZnbicmIYZDeYdwx52htYfSt2ZfbDab1dGkElFRERGRIowxLDm+hBcPvUiBKSDUK5S46Dia+TWzOppUQioqIiJSKK0gjclJk1mTugaAboHdmBA5gQCPAGuDSaWloiIiIgBsy9xGbEIsKXkpeNo8ear2U9xb414N9YililVUduzYwZIlS/j2229JSkoiKyuLGjVq0KpVK3r27Mldd92F3W4vrawiIlIKnMbJO8feYc6hOThwUNurNjPrzqSxb2Oro4lgM8aYS630008/8cwzz7B+/Xo6duxIu3btCAsLw8fHh1OnTvHbb7/x7bffkpaWxjPPPMNTTz1VJoUlLS2NwMBAUlNTCQjQbkkRkeI6U3CGiYkTWZ+2HoAbqt7A2Mix+Lv7W5xMKrLifH5f1h6Vu+66i1GjRrF06VKqVq16wfW+++47XnrpJZ5//nnGjBlTrNAiIlK2tmZsZUzCGI7mH8XL5sXTdZ7mrup3aahHXMpl7VHJz8/H09Pzsjda3PWvlPaoiIgUn9M4efPom8w9PBcHDiLsEcRHx9PQt6HV0aSSKPE9KsUtHWVRUkREpPhO5Z9iQtIEvkv7DoCbgm5iTMQY/Nz9LE4mcn5XdNTPqlWrWLVqFceOHcPpdBZ57I033iiRYCIiUrK2pG9hTOIYTuSfwG6z80z4M9wefLuGesSlFbuoTJ48mSlTptCmTRtCQ0P1DS4i4uIcxsEbR95gXso8nDiJ9o4mPjqeej71rI4mcknFLiqvvvoqCxcu5MEHHyyNPCIiUoJO5J9gfOJ4fkj/AYDe1XozOnw0Pu4+FicTuTzFLip5eXl06NChNLKIiEgJ+iHtB8YljuNkwUm83byJDY/l1uBbrY4lUixuxX3Cww8/zKJFi0oji4iIlACHcTD38Fye3PskJwtOco33Nbzd8G2VFCmXir1HJScnh3nz5vH111/TvHnzc47wmT17domFExGR4jmed5yxiWPZkrEFgD7BfRgZPhJvN2+Lk4lcmWIXlV9//ZWWLVsC8NtvvxV5TBNrRUSsszFtI+MTx3Om4Ay+br6MiRhDr2q9rI4lclWKXVRWr15dGjlEROQKFZgC5h6ey8KjCwFo6NOQuOg4Ir0jrQ0mUgKu+OrJe/fuZd++fXTp0gUfHx+MMdqjIiJSxo7kHWFMwhh+yfwFgLur383wOsOxu+kCsVIxFLuonDx5knvuuYfVq1djs9nYs2cPdevWZeDAgQQFBfH888+XRk4REfmTdanrmJQ4iVRHKn5ufoyPHM8NQTdYHUukRBX7qJ/hw4fj6enJgQMH8PX1LVx+77338uWXX5ZoOBEROVe+M58XDr7A8H3DSXWk0ti3Me80fkclRSqkYu9RWbFiBV999RV16tQpsrx+/fokJSWVWDARETnX4dzDxCbE8lvWHwcz3FfjPobWHoqXm5fFyURKR7GLSmZmZpE9KWedOnUKu11joiIipWX1mdVMTppMuiMdf3d/JkZOpFvVblbHEilVxR766dy5M2+99VbhfZvNhtPp5Nlnn6VbN/3AiIiUtDxnHrOSZzFy/0jSHek09W3KokaLVFKkUij2HpVnn32W7t278+OPP5KXl8czzzzD77//zqlTp9iwYUNpZBQRqbSSc5OJTYhlR9YOAB6s+SCDag/C0+Z5iWeKVAzFLipNmzZl9+7dzJkzB39/fzIyMrjzzjsZNGgQoaGhpZFRRKRSWnl6JVOTppLpzCTQPZDJUZPpHNjZ6lgiZcpmjDFWh7hSaWlpBAYGkpqaSkBAgNVxRERKRK4zl9kHZ7P0xFIAWvi1YEb0DGp51bI4mUjJKM7nd7HnqAB8++23PPDAA3To0IFDhw4B8N///pf169dfyeZEROT/O5BzgAG7BhSWlAEhA5jXYJ5KilRaxS4qH3zwAT179sTHx4effvqJ3NxcAFJTU5kxY0aJBxQRqSy+PPUlfXf2ZVf2LoI8gphTbw6Daw/Gw3bFJxEXKfeKXVSmTZvGq6++yuuvv17kyskdO3bkp59+KtFwIiKVQY4zh6lJUxmbOJYsZxbXVrmWxY0W0yGgg9XRRCxX7Jq+a9cuunTpcs7ywMBAzpw5UxKZREQqjYTsBEYnjGZfzj5s2Hi41sM8HPqw9qKI/H/F/kmoVasWe/fuJSoqqsjy9evXU7du3ZLKJSJS4X168lPikuPIceYQ7BHMtKhptAtoZ3UsEZdS7KLyyCOPMGzYMN544w1sNhuHDx/mu+++Y+TIkYwfP740MoqIVCjZjmxmJs/kk1OfANDOvx1To6ZS3bO6xclEXE+xi0pMTAxOp5Pu3buTlZVFly5dsNvtjBw5kiFDhpRGRhGRCmNv9l5iEmJIyEnADTceC32MAbUG4G5ztzqaiEsq1mRah8PBt99+y6BBgzh16hS//fYbmzZt4vjx40ydOvWqgsTHx2Oz2XjqqaeuajsiIq7IGMNHJz7ioZ0PkZCTQA3PGrxa/1UeDn1YJUXkIoq1R8Xd3Z0bb7yRHTt2ULVqVf7yl7+USIjNmzfz2muv0bx58xLZnoiIK8l0ZBJ3II4vTn8BQIeADkyJnEKQZ5DFyURcX7EPT27atCn79+8vsQAZGRn07duX119/naAg/dCKSMWyO2s3D+58kC9Of4E77gwJG8JL17ykkiJyma7oPCojR47k008/JSUlhbS0tCK34ho0aBC33HILPXr0uOS6ubm5V/16IiJlwRjD0uNL6berH0m5SYR4hjCvwTz61+qPm+2KTgouUikVezLtzTffDMBtt92GzWYrXG6MwWaz4XA4LntbS5Ys4aeffmLz5s2XtX5cXByTJ08uXmARkTKW7khnetJ0Vp5ZCUDngM5MippEVY+q1gYTKYeKXVRWr15dIi+cnJzMsGHDWLlyJd7e3pf1nNjYWEaMGFF4Py0tjfDw8BLJIyJSEnZk7SAmIYaDuQdxx52htYfSt2bfIn/Yicjls+zqyR999BF9+vTB3f3/Zrs7HA5sNhtubm7k5uYWeex8dPVkEXEVxhjePf4uLx56kXyTT6hXKHHRcTTza2Z1NBGXU5zP72LvUfn111/Pu9xms+Ht7U1ERAR2u/2S2+nevTvbtm0rsmzAgAE0atSI0aNHX7KkiIi4irSCNKYkTWF16h97nP8a+FcmRk4kwEN/QIlcrWIXlZYtW150F6anpyf33nsvr7322kWHdPz9/WnatGmRZX5+fgQHB5+zXETEVf2W+RuxCbEczjuMp82Tp2o/xb017tVQj0gJKfbU82XLllG/fn3mzZvH1q1b2bp1K/PmzaNhw4YsWrSI+fPn88033zBu3LjSyCsi4hKMMbx99G3+ueufHM47TG2v2ixouIB/1PyHSopICSr2HJV27doxdepUevbsWWT5V199xfjx4/nhhx/46KOPePrpp9m3b1+Jhv0zzVERESukFqQyKWkS61LXAdCjag/GRY7D393f4mQi5UOpzlHZtm0bkZGR5yyPjIwsnHPSsmVLUlJSirtpERGX90vGL8QmxHI0/yheNi+ervM0d1W/S3tRREpJsYd+GjVqRHx8PHl5eYXL8vPziY+Pp1GjRgAcOnSIkJCQkkspImIxp3Gy8MhCHtn9CEfzjxJhj2Bhw4X8vcbfVVJESlGx96i88sor3HbbbdSpU6fw2jzbtm3D4XDw6aefArB//36efPLJkk0qImKR0/mnmZg0kQ1pGwC4KegmxkSMwc/dz+JkIhXfFZ1HJT09nXfeeYfdu3cD0LBhQ+6//378/ct2fFZzVESktP2U/hNjEsdwPP84dpudZ8Kf4fbg27UXReQqlOocFfjj0OLHH3/8isKJiJQHDuNgwZEFvJbyGk6cRHtHEx8dTz2felZHE6lULmuOyqZNmy57g1lZWfz+++9XHEhExGon808yZO8Q5qbMxYmTW6vdyn8b/lclRcQCl1VUHnzwQXr27Mn7779PZmbmedfZvn07Y8aM4ZprrmHLli0lGlJEpKz8kP4D9+24j+/Tv8fbzZtJkZOYHDUZH3cfq6OJVEqXNfSzfft25s6dy7hx47j//vtp0KABYWFheHt7c/r0aXbu3ElGRgZ9+vRhxYoVNGuma1uISPniMA5eT3md/xz5DwbDNd7XMDN6JtE+0VZHE6nUij2Z9scff2T9+vUkJSWRnZ1N9erVadWqFd26daNatWqllfO8NJlWRErC8bzjjE0cy5aMP/YG3xF8ByPDR+Ljpr0oIqWhVCfTtmnThjZt2lxxOBERV/Jd2neMTxzP6YLT+Lr5MiZiDL2q9bI6loj8f1d01E9BQQFr1qxh3759hYclHz58mICAAKpUqVLSGUVESlyBKeC1w6/xxtE3AGjg04D46Hgivc8987aIWKfYRSUpKYmbbrqJAwcOkJubyw033IC/vz8zZ84kNzeXV199tTRyioiUmKN5RxmTMIatmVsBuLv63QyvMxy7m93aYCJyjmKfQn/YsGG0adOG06dP4+Pzf+O3ffr0YdWqVSUaTkSkpK1PXc99O+5ja+ZW/Nz8iIuOIyYiRiVFxEUVe4/Kt99+y8aNG/Hy8iqyPCoqikOHDpVYMBGRkpRv8vn3oX/z1rG3AGjs25i46DjC7eEWJxORiyl2UXE6nTgcjnOWHzx4sMxPoS8icjlSclOITYxlW+YfV3j/R41/MKz2MLzcvC7xTBGxWrGHfm688UZefPHFwvs2m42MjAwmTpzIzTffXJLZRESu2poza7h/5/1sy9yGv7s/z9V9jlHho1RSRMqJYp9H5eDBg/Ts2RNjDHv27KFNmzbs2bOH6tWrs27dOmrWrFlaWc+h86iIyIXkO/N56dBLLD6+GICmvk2Ji44jzB5mcTIRKc7n9xVdPbmgoIB3332XX375hYyMDFq3bk3fvn2LTK4tCyoqInI+B3MPEpsQy/as7QA8UPMBBocNxtPN0+JkIgKlXFTWrVtHhw4d8PAoOr2loKCAjRs30qVLl+InvkIqKiLyZ1+f/popSVPIdGYS6B7IpKhJdAksu99LInJppXpm2m7dupGSknLOEE9qairdunU770RbEZHSluvM5YWDL/D+ifcBaOHXghnRM6jlVcviZCJyNYpdVIwx2Gy2c5afPHkSPz+/EgklIlIcB3IOEJMQw67sXQD0D+nP42GP42nTUI9IeXfZReXOO+8E/jjKp3///tjt/3dyJIfDwa+//kqHDh1KPqGIyEV8deorph2YRpYzi6oeVZkaOZUOgfpdJFJRXHZRCQwMBP7Yo+Lv719k4qyXlxfXXXcdjzzySMknFBE5jxxnDs8ffJ4PT3wIQOsqrZkeNZ2aXmV35KGIlL7LLioLFiwA/jgD7ciRIzXMIyKWSchJIGZ/DHtz9mLDxsBaA3kk9BE8bFd0nVURcWFXdHiyq9BRPyKVz2cnPyMuOY5sZzbBHsFMjZpK+4D2VscSkWIo1aN+AJYuXcp7773HgQMHyMvLK/LYTz/9dCWbFBG5qGxHNjOTZ/LJqU8AaOvflmlR06juWd3iZCJSmop9Cv1//etfDBgwgJCQEH7++WfatWtHcHAw+/fvp1evXqWRUUQquX3Z+3ho10N8cuoT3HDj8dDHeaXeKyopIpVAsYvKv//9b+bNm8ecOXPw8vLimWeeYeXKlQwdOpTU1NTSyCgilZQxho9PfMyDOx9kf85+qntWZ279uTwS+gjuNner44lIGSh2UTlw4EDhYcg+Pj6kp6cD8OCDD7J48eKSTScilVaWI4vxieOZcmAKuSaX6/yvY3GjxbTxb2N1NBEpQ8UuKrVq1eLUqVMAREREsGnTJgASEhIox/NyRcSF7M7azQM7H+CL01/gjjuDwwYzp94cqnlWszqaiJSxYk+m/dvf/sby5ctp1aoVAwYMYPjw4SxdupQff/yx8KRwIiJXwhjDhyc+5LmDz5Fn8gjxDGF69HRaVWlldTQRsUixD092Op04nc7CixIuWbKEjRs3Ur9+fR577DG8vLxKJej56PBkkYojw5HB9APTWXF6BQCdAjoxKWoSQR5BFicTkZJWaldPLigoYMaMGfzzn/+kTp06Vx30aqmoiFQMO7J2EJMQw8Hcg38M9dQezAM1H8DNVuzRaREpB4rz+V2s3wIeHh48++yzFBQUXFVAERH4Y6jn3WPvMmDXAA7mHiTUK5T5DefzUMhDKikiAlzBHJXu3buzdu1aoqKiSiGOiFQW6QXpTDkwhW/OfANA18CuTIqcRICH9o6KyP8pdlHp1asXMTExbNu2jWuvvfaca/7cdtttJRZORCqm3zJ/IzYhlsN5h/GweTCs9jDuq3EfNpvN6mgi4mKKPZnWze3Cu2NtNhsOh+OqQ10uzVERKV+MMSw6toh/Hf4XBaaA2l61iYuOo4lfE6ujiUgZKtVr/TidzisOJiKVV2pBKpOSJrEudR0A3at2Z3zkePzd/S1OJiKuTNdEF5FS90vGL8QmxHI0/yieNk9G1BnB3dXv1lCPiFySioqIlBqncfLfo//llcOv4MBBuD2c+Oh4Gvk2sjqaiJQTKioiUipO559mYtJENqRtAKBnUE/GRozFz93vEs8UEfk/KioiUuJ+Sv+JMYljOJ5/HLvNzsjwkfQJ7qOhHhEpNhUVESkxTuNkwZEFvJryKk6cRNojmRk9k/q+9a2OJiLlVLGLSlpa2nmX22w27HZ7mV7rR0Rcx8n8k4xPHM/36d8DcEu1W4gJj8HX3dfiZCJSnhW7qFStWvWiu2/r1KlD//79mThx4kXPuSIiFccP6T8wLmEcJwtOYrfZiYmI4bZgnfxRRK5esYvKwoULGTt2LP3796ddu3YA/PDDD7z55puMGzeO48eP89xzz2G32xkzZkyJBxYR1+EwDv6T8h9eP/I6BsM13tcQHx1PXZ+6VkcTkQqi2EXlzTff5Pnnn+eee+4pXNa7d2+aNWvGa6+9xqpVq4iIiGD69OkqKiIV2PH844xNGMuWjC0A3B58O6PCR+Hj5mNxMhGpSIo9NrNx40ZatWp1zvJWrVrx3XffAdCpUycOHDhw9elExCV9l/Yd9+24jy0ZW/Bx82Fq5FQmRE5QSRGRElfsohIeHs78+fPPWT5//nzCw8MBOHnyJEFBQVefTkRcSoEp4JVDrzBk7xBOF5ymvk993m70NjcH32x1NBGpoIo99PPcc89x991388UXX9C2bVsAfvzxR3bu3MnSpUsB2Lx5M/fee+8ltzV37lzmzp1LYmIiAE2aNGHChAn06tWruLFEpJQdzTvKmIQxbM3cCsBd1e9iRJ0ReLt5WxtMRCq0Yl89GSAhIYHXXnuN3bt3A9CwYUMee+wxoqKiirWdTz75BHd3d+rXr48xhjfffJNZs2bx888/06TJpa+mqqsni5SN9anrmZA4gVRHKn5ufoyLGMeN1W60OpaIlFPF+fy+oqJSmqpVq8asWbMYOHDgJddVUREpXfkmn38f+jdvHXsLgEY+jYiPjifcO9ziZCJSnhXn8/uKzkx75swZfvjhB44dO4bT6Szy2EMPPXQlm8ThcPD++++TmZnJ9ddff0XbEJGSk5KbQmxiLNsytwFwb417ear2U3i56aSOIlJ2il1UPvnkE/r27UtGRgYBAQFFTv5ms9mKXVS2bdvG9ddfT05ODlWqVGHZsmX85S9/Oe+6ubm55ObmFt6/0FlyReTqrDmzhslJk0lzpFHFvQoTIibQPai71bFEpBIq9tBPgwYNuPnmm5kxYwa+vld/auy8vDwOHDhAamoqS5cu5T//+Q9r1649b1mZNGkSkydPPme5hn5ESka+M59/Hf4Xi44tAqCJbxPiouOoba9tcTIRqUhKdY6Kn58f27Zto27d0jnzZI8ePbjmmmt47bXXznnsfHtUwsPDVVRESsDB3IPEJsSyPWs7AH1r9mVI2BA83TwtTiYiFU2pzlHp2bMnP/74Y6kVFafTWaSM/C+73Y7dbi+V1xWpzFadXsXkpMlkOjMJcA9gUuQkulbtanUsEZHiF5VbbrmFUaNGsX37dpo1a4anZ9G/tm677fIvRBYbG0uvXr2IiIggPT2dRYsWsWbNGr766qvixhKRK5DrzOXFQy/y3vH3AGju15wZ0TMI9Qq1OJmIyB+KPfRzsSsi22w2HA7HZW9r4MCBrFq1ipSUFAIDA2nevDmjR4/mhhtuuKzn6/BkkSt3IOcAMQkx7MreBUC/kH48EfYEnjYN9YhI6SrVoZ8/H458Nc53Kn4RKX1fnfqK6Qemk+nMJNA9kClRU+gU2MnqWCIi57ii86iISPmU48zh+YPP8+GJDwFoVaUV06OmE+IVYnEyEZHzu6yi8q9//YtHH30Ub29v/vWvf1103aFDh5ZIMBEpWQk5CcTsj2Fvzl5s2PhnrX/yaOijeNj094qIuK7LmqMSHR3Njz/+SHBwMNHR0RfemM3G/v37SzTgxWiOisjl+ezkZ8Qlx5HtzKaaRzWmRk3luoDrrI4lIpVUic9RSUhIOO+/RcS1ZTuyefbgsyw/uRyANlXaMC16GjU8a1icTETk8mifr0gFtS97HzEJMezP2Y8NG4+GPsrAWgNxt7lbHU1E5LIVu6g4HA4WLlzIqlWrzntRwm+++abEwolI8RljWH5yOTOTZ5Jrcgn2CGZ69HTa+re1OpqISLEVu6gMGzaMhQsXcsstt9C0adMiFyUUEWtlObKIS47j81OfA9Devz3ToqZRzbOaxclERK5MsYvKkiVLeO+997j55ptLI4+IXKE9WXsYnTCapNwk3HDjibAn6B/SHzfbhU/SKCLi6opdVLy8vKhXr15pZBGRK2CM4cMTH/LcwefIM3nU9KzJjOgZtKrSyupoIiJXrdh/aj399NO89NJLFPPM+yJSCjIcGYxJHMOM5BnkmTw6BnRkUeNFKikiUmEUe4/K+vXrWb16NV988QVNmjQ556KEH374YYmFE5EL25G1g9iEWJJzk3HHncG1B/NAzQc01CMiFUqxi0rVqlXp06dPaWQRkctgjOG94+/xwqEXyDf51PKqRVxUHM2rNLc6mohIiStWUSkoKKBbt27ceOON1KpVq7QyicgFpBekM+XAFL4588dpALoGdmVi5EQCPQItTiYiUjqKVVQ8PDx4/PHH2bFjR2nlEZEL+C3zN2ITYjmcdxgPmwfDag/jvhr36RQBIlKhFXvop127dvz8889ERkaWRh4R+RNjDIuOLeJfh/9FgSmgtldt4qLjaOLXxOpoIiKlrthF5cknn+Tpp5/m4MGDXHvttfj5+RV5vHlzjZOLlJTUglQmJ01mbepaALpX7c74iPH4e/hbnExEpGxc1tWT/5eb27lHFNhsNowx2Gw2HA5HiYW7FF09WSqyXzN+JSYhhqP5R/G0eTKizgjurn63hnpEpNwr8asn/y9dPVmkdDmNk/8e/S+vHH4FBw7C7eHER8fTyLeR1dFERMpcsYuK5qaIlJ7T+aeZmDSRDWkbAOgZ1JMxEWOo4l7F4mQiItYodlE5a/v27Rw4cIC8vLwiy2+77barDiVSGf2c8TOxCbEczz+O3WZnZPhI+gT30VCPiFRqxS4q+/fvp0+fPmzbtq1wbgpQ+Mu0LOeoiFQETuNkwZEFvJryKk6cRNojmRk9k/q+9a2OJiJiuWKfa3vYsGFER0dz7NgxfH19+f3331m3bh1t2rRhzZo1pRBRpOI6mX+SwXsH8++Uf+PEyS3VbuHtRm+rpIiI/H/F3qPy3Xff8c0331C9enXc3Nxwc3OjU6dOxMXFMXToUH7++efSyClS4fyQ/gPjEsZxsuAkdpudmIgYelfrraEeEZH/Ueyi4nA48Pf/4xwO1atX5/DhwzRs2JDIyEh27dpV4gFFKhqHcfCflP/w+pHXMRjqetclPjqea3yusTqaiIjLKXZRadq0Kb/88gvR0dG0b9+eZ599Fi8vL+bNm0fdunVLI6NIhXE8/zjjEsbxY8aPANwefDujwkfh4+ZjcTIREddU7KIybtw4MjMzAZgyZQq33nornTt3Jjg4mHfffbfEA4pUFJvSNjE+cTynCk7h4+bDmPAx3Bx8s9WxRERcWrHPTHs+p06dIigoqMzH1nVmWikPCkwBrx1+jQVHF2Aw1POuR3zdeKK9o62OJiJiiVI9M+1Ze/fuZd++fXTp0oVq1apRAn1HpMI5mneUsYlj+Tnjj0nmd1W/ixF1RuDt5m1xMhGR8qHYReXkyZPcc889rF69GpvNxp49e6hbty4DBw4kKCiI559/vjRyipQ761PXMyFxAqmOVPzc/BgbMZae1XpaHUtEpFwp9nlUhg8fjqenJwcOHMDX17dw+b333suXX35ZouFEyqN8k89LB19i2L5hpDpSaejTkHcavaOSIiJyBYq9R2XFihV89dVX1KlTp8jy+vXrk5SUVGLBRMqjlLwUxiSM4dfMXwG4t8a9DKs9DLub3eJkIiLlU7GLSmZmZpE9KWedOnUKu12/jKXyWntmLZOSJpHmSKOKexUmREyge1B3q2OJiJRrxR766dy5M2+99VbhfZvNhtPp5Nlnn6Vbt24lGk6kPMh35vP8wecZsX8EaY40mvg2YVGjRSopIiIloNh7VJ599lm6d+/Ojz/+SF5eHs888wy///47p06dYsOGDaWRUcRlHco9RGxCLL9n/Q5A35p9GRI2BE83T4uTiYhUDFd0Ztrdu3fz8ssv4+/vT0ZGBnfeeSeDBg0iNDS0NDKKuKRvTn/D5AOTyXBkEOAewKTISXSt2tXqWCIiFcoVnUclMDCQsWPHFll28OBBHn30UebNm1ciwURcVa4zl5cOvcS7x/84E3Nzv+bMiJpBqF1FXUSkpBV7jsqFnDx5kvnz55fU5kRcUnJOMgN2DSgsKf1C+jGvwTyVFBGRUnLFZ6YVqWxWnFrBtAPTyHRmEugeyJSoKXQK7GR1LBGRCk1FReQScpw5zD44mw9OfABAS7+WzIieQYhXiMXJREQqPhUVkYtIzEkkJiGGPdl7sGHjn7X+yaOhj+Jh04+OiEhZuOzftnfeeedFHz9z5szVZhFxKZ+f/JwZyTPIdmZTzaMaU6Omcl3AdVbHEhGpVC67qAQGBl7y8YceeuiqA4lYLduZzazkWXx88mMA2lRpw7ToadTwrGFxMhGRyueyi8qCBQtKM4eIS9iXvY+YhBj25+zHho1HQx9lYK2BuNvcrY4mIlIpaaBdBDDG8MmpT4g/EE+uySXYI5jp0dNp69/W6mgiIpWaiopUelmOLOKT4/ns1GcAtPdvz9SoqQR7BlucTEREVFSkUtuTtYfRCaNJyk3CDTeeCHuC/iH9cbOV2LkQRUTkKqioSKVkjGHZyWU8l/wcuSaXmp41mRE9g1ZVWlkdTURE/oeKilQ6GY4MZhyYwVenvwKgY0BHJkdNJsgjyOJkIiLyZyoqUqnszNpJTEIMybnJuOPO4NqDeaDmAxrqERFxUSoqUikYY3j/xPvMPjibfJNPLa9axEXF0bxKc6ujiYjIRVj6Z2RcXBxt27bF39+fmjVrcscdd7Br1y4rI0kFlF6QzuiE0cxMnkm+yadrYFcWNVqkkiIiUg5YWlTWrl3LoEGD2LRpEytXriQ/P58bb7yRzMxMK2NJBfJ75u/03dmXVWdW4WHz4Ok6T/N83ecJ9Lj4mZZFRMQ12IwxxuoQZx0/fpyaNWuydu1aunTpcsn109LSCAwMJDU1lYCAgDJIKOWFMYbFxxfz0qGXKDAFhHmFER8dTxO/JlZHExGp9Irz+e1Sc1RSU1MBqFat2nkfz83NJTc3t/B+WlpameSS8iW1IJXJSZNZm7oWgL9V/RsTIibg7+FvcTIRESkulznUwel08tRTT9GxY0eaNm163nXi4uIIDAwsvIWHh5dxSnF12zK3cf/O+1mbuhZPmyejw0fzbPSzKikiIuWUywz9PPHEE3zxxResX7+eOnXqnHed8+1RCQ8P19CP4DRO3j72Ni8fehkHDurY6xAfHU9j38ZWRxMRkT8pd0M/gwcP5tNPP2XdunUXLCkAdrsdu91ehsmkPDhdcJpJiZNYn7YegBuDbmRsxFiquFexOJmIiFwtS4uKMYYhQ4awbNky1qxZQ3R0tJVxpBz6OeNnxiSM4Vj+MbxsXowKH0Wf4D7YbDaro4mISAmwtKgMGjSIRYsW8fHHH+Pv78+RI0cACAwMxMfHx8po4uKcxsnCowt59fCrOHAQaY9kZvRM6vvWtzqaiIiUIEvnqFzor94FCxbQv3//Sz5fhydXTqfyTzE+cTyb0jcBcHO1m4kNj8XX3dfiZCIicjnKzRwVF5nHK+XIj+k/MjZxLCfyT2C32YmJiKF3td4a6hERqaBcYjKtyKU4jIP5R+bzesrrOHFS17su8dHxXONzjdXRRESkFKmoiMs7kX+CcYnj2Jy+GYDbg29nVPgofNw0j0lEpKJTURGX9n3a94xLHMepglP4uPkQGx7LLcG3WB1LRETKiIqKuKQCU8C8lHm8ceQNDIZ63vWIrxtPtLcOYRcRqUxUVMTlHMs7xtjEsfyU8RMAd1W/ixF1RuDt5m1xMhERKWsqKuJSNqZuZHzSeM4UnMHPzY+xEWPpWa2n1bFERMQiKiriEvJNPnMPz+XNo28C0NCnIfHR8UR4R1icTERErKSiIpZLyUthbMJYfsn8BYB7atzDU7Wfwu6m6zqJiFR2KipiqXVn1jExaSJpjjSquFdhQsQEugd1tzqWiIi4CBUVsUS+M585h+fwzrF3AGji24QZ0TOoY7/w1bNFRKTyUVGRMnco9xCxCbH8nvU7APfXvJ+hYUPxdPO0OJmIiLgaFRUpU9+c/obJByaT4cjA392fSZGT+GvVv1odS0REXJSKipSJPGceLx56kXePvwtAM79mxEXFEWoPtTiZiIi4MhUVKXXJOcnEJMSwM3snAA/VfIgnaz+Jp01DPSIicnEqKlKqVpxewbSkaWQ6Mwl0D2Ry1GQ6B3a2OpaIiJQTKipSKnKcOcw+OJsPTnwAQEu/lsyInkGIV4jFyUREpDxRUZESl5iTSExCDHuy92DDxoCQATwW9hgeNn27iYhI8eiTQ0rU56c+Z8aBGWQ7swnyCGJq1FSuD7je6lgiIlJOqahIich2ZjMreRYfn/wYgGurXMv06OnU8KxhcTIRESnPVFTkqu3P3k9MQgz7cvZhw8YjtR7h4dCHcbe5Wx1NRETKORUVuSrLTy5nZvJMcpw5BHsEMy16Gu3821kdS0REKggVFbkiWY4s4pPj+ezUZwC092/P1KipBHsGW5xMREQqEhUVKbY92XuI2R9DYm4ibrjxeOjjDKg1ADebm9XRRESkglFRkctmjGHZyWU8l/wcuSaXGp41mBE1g9b+ra2OJiIiFZSKilyWDEcGMw7M4KvTXwHQIaADUyKnEOQZZHEyERGpyFRU5JJ2Zu0kJiGG5Nxk3HFnUNggHgx5UEM9IiJS6lRU5IKMMbx/4n1mH5xNvsknxDOEuOg4WlRpYXU0ERGpJFRU5LzSHelMTZrKqjOrAOgS2IWJkROp6lHV2mAiIlKpqKjIObZnbicmIYZDeYdwx51htYdxf837sdlsVkcTEZFKRkVFChljWHx8MS8deokCU0CYVxhx0XE09WtqdTQREamkVFQEgNSCVCYnTWZt6loAugV2Y2LkRPw9/C1OJiIilZmKirAtcxsxCTEcyTuCp82T4bWHc0+NezTUIyIillNRqcScxsnbx97m5UMv48BBHXsd4qPjaezb2OpoIiIigIpKpXW64DSTEiexPm09ADdUvYFxkeOo4l7F4mQiIiL/R0WlEvo542fGJozlaP5RvGxejKwzkjur36mhHhERcTkqKpWI0zhZeHQhrx5+FQcOIu2RxEfH08C3gdXRREREzktFpZI4lX+K8Ynj2ZS+CYBeQb2IjYjFz93P4mQiIiIXpqJSCfyY/iNjE8dyIv8Edpud0eGjuS34Ng31iIiIy1NRqcAcxsH8I/N5PeV1nDiJ9o4mPjqeej71rI4mIiJyWVRUKqgT+ScYlziOzembAehdrTejw0fj4+5jcTIREZHLp6JSAX2f9j3jEsdxquAU3m7exIbHcmvwrVbHEhERKTYVlQqkwBQwL2Uebxx5A4PhGu9rmFl3JtHe0VZHExERuSIqKhXEsbxjjE0cy08ZPwHQJ7gPI8NH4u3mbXEyERGRK6eiUgFsTN3I+KTxnCk4g6+bL2MjxnJTtZusjiUiInLVVFTKsXyTz9zDc3nz6JsANPRpSHx0PBHeERYnExERKRkqKuXUkbwjjEkYwy+ZvwBwd/W7GV5nOHY3u8XJRERESo6KSjm07sw6JiVNItWRip+bHxMiJ9AjqIfVsUREREqciko5ku/MZ87hObxz7B0A/uL7F+Ki46hjr2NxMhERkdKholJOHM49TExCDL9n/Q7AfTXuY2jtoXi5eVmcTEREpPS4Wfni69ato3fv3oSFhWGz2fjoo4+sjOOyvjnzDffvvJ/fs37H392f5+o+x8jwkSopIiJS4VlaVDIzM2nRogWvvPKKlTFcVp4zj2eTn2XU/lGkO9Jp5teMxY0W061qN6ujiYiIlAlLh3569epFr169rIzgspJzk4lNiGVH1g4AHqz5IINqD8LT5mlxMhERkbKjOSouaOXplUxNmkqmM5NA90AmR02mc2Bnq2OJiIiUuXJVVHJzc8nNzS28n5aWZmGakpfrzGX2wdksPbEUgBZ+LYiLjiPEK8TiZCIiItawdI5KccXFxREYGFh4Cw8PtzpSiUnKSaL/rv6FJWVAyADmNZinkiIiIpVauSoqsbGxpKamFt6Sk5OtjlQivjj1BQ/sfIDd2bsJ8ghiTr05DK49GA9budrhJSIiUuLK1Seh3W7Hbq84p4jPdmbzXPJzfHTyIwCurXIt06OmU8OrhrXBREREXISlRSUjI4O9e/cW3k9ISGDr1q1Uq1aNiIiKfWG9hOwERieMZl/OPmzYeLjWwzwS+gjuNnero4mIiLgMS4vKjz/+SLdu/3dOkBEjRgDQr18/Fi5caFGq0vfJyU+IT44nx5lDsEcw06Kn0c6/ndWxREREXI6lReWvf/0rxhgrI5SpbEc28cnxfHrqUwDa+7dnatRUgj2DLU4mIiLimsrVHJXybG/2XkbvH01ibiJuuPFY6GMMqDVAQz0iIiIXoaJSyowxfHTyI2YlzyLX5FLDswbTo6Zzrf+1VkcTERFxeSoqpSjTkcmMAzP48vSXAHQI6MCUyCkEeQZZnExERKR8UFEpJbuydhGTEMOB3AO4486TYU/yUMhDuNnK1alrRERELKWiUsKMMSw9sZTZB2eTZ/II8QwhLjqOFlVaWB1NRESk3FFRKUHpjnSmJU3j6zNfA9AlsAsTIydS1aOqtcFERETKKRWVErI9czsxCTEcyjuEO+4MrT2UvjX7YrPZrI4mIiJSbqmoXCVjDEuOL+HFQy9SYAoI9QolLjqOZn7NrI4mIiJS7qmoXIW0gjSmJE1hdepqALoFdmNC5AQCPAIsTiYiIlIxqKhcoW2Z24hNiCUlLwVPmyfDaw/nnhr3aKhHRESkBKmoFJMxhrePvc2cQ3Nw4KCOvQ7x0fE09m1sdTQREZEKR0WlGM4UnGFS4iS+TfsWgBuq3sDYyLH4u/tbnExERKRiUlG5TFsztjImYQxH84/iZfPi6TpPc1f1uzTUIyIiUopUVC7BaZy8efRN5h6eiwMHkfZI4qPjaeDbwOpoIiIiFZ6KykWczj/NhKQJbEzbCECvoF7ERsTi5+5ncTIREZHKQUXlAn5K/4kxiWM4nn8cu83O6PDR3BZ8m4Z6REREypCKynksPb6UmckzceIk2jua+Oh46vnUszqWiIhIpaOich5N/JrgZnPjlqBbGB0+Gh93H6sjiYiIVEoqKufR2Lcx7zZ+lyjvKKujiIiIVGpuVgdwVSopIiIi1lNREREREZeloiIiIiIuS0VFREREXJaKioiIiLgsFRURERFxWSoqIiIi4rJUVERERMRlqaiIiIiIy1JREREREZeloiIiIiIuS0VFREREXJaKioiIiLgsFRURERFxWR5WB7gaxhgA0tLSLE4iIiIil+vs5/bZz/GLKddFJT09HYDw8HCLk4iIiEhxpaenExgYeNF1bOZy6oyLcjqdHD58GH9/f2w2W4luOy0tjfDwcJKTkwkICCjRbUvx6evhWvT1cC36ergefU0uzhhDeno6YWFhuLldfBZKud6j4ubmRp06dUr1NQICAvRN5kL09XAt+nq4Fn09XI++Jhd2qT0pZ2kyrYiIiLgsFRURERFxWSoqF2C325k4cSJ2u93qKIK+Hq5GXw/Xoq+H69HXpOSU68m0IiIiUrFpj4qIiIi4LBUVERERcVkqKiIiIuKyVFTO45VXXiEqKgpvb2/at2/PDz/8YHWkSisuLo62bdvi7+9PzZo1ueOOO9i1a5fVsQSIj4/HZrPx1FNPWR2lUjt06BAPPPAAwcHB+Pj40KxZM3788UerY1VKDoeD8ePHEx0djY+PD9dccw1Tp069rNPEy4WpqPzJu+++y4gRI5g4cSI//fQTLVq0oGfPnhw7dszqaJXS2rVrGTRoEJs2bWLlypXk5+dz4403kpmZaXW0Sm3z5s289tprNG/e3Oooldrp06fp2LEjnp6efPHFF2zfvp3nn3+eoKAgq6NVSjNnzmTu3Lm8/PLL7Nixg5kzZ/Lss88yZ84cq6OVazrq50/at29P27Ztefnll4E/TtMfHh7OkCFDiImJsTidHD9+nJo1a7J27Vq6dOlidZxKKSMjg9atW/Pvf/+badOm0bJlS1588UWrY1VKMTExbNiwgW+//dbqKALceuuthISEMH/+/MJld911Fz4+Prz99tsWJivftEflf+Tl5bFlyxZ69OhRuMzNzY0ePXrw3XffWZhMzkpNTQWgWrVqFiepvAYNGsQtt9xS5OdErLF8+XLatGnD3XffTc2aNWnVqhWvv/661bEqrQ4dOrBq1Sp2794NwC+//ML69evp1auXxcnKt3J9rZ+SduLECRwOByEhIUWWh4SEsHPnTotSyVlOp5OnnnqKjh070rRpU6vjVEpLlizhp59+YvPmzVZHEWD//v3MnTuXESNGMGbMGDZv3szQoUPx8vKiX79+VserdGJiYkhLS6NRo0a4u7vjcDiYPn06ffv2tTpauaaiIuXGoEGD+O2331i/fr3VUSql5ORkhg0bxsqVK/H29rY6jvBHeW/Tpg0zZswAoFWrVvz222+8+uqrKioWeO+993jnnXdYtGgRTZo0YevWrTz11FOEhYXp63EVVFT+R/Xq1XF3d+fo0aNFlh89epRatWpZlEoABg8ezKeffsq6detK/YrZcn5btmzh2LFjtG7dunCZw+Fg3bp1vPzyy+Tm5uLu7m5hwsonNDSUv/zlL0WWNW7cmA8++MCiRJXbqFGjiImJ4R//+AcAzZo1Iykpibi4OBWVq6A5Kv/Dy8uLa6+9llWrVhUuczqdrFq1iuuvv97CZJWXMYbBgwezbNkyvvnmG6Kjo62OVGl1796dbdu2sXXr1sJbmzZt6Nu3L1u3blVJsUDHjh3POVx/9+7dREZGWpSocsvKysLNrejHqru7O06n06JEFYP2qPzJiBEj6NevH23atKFdu3a8+OKLZGZmMmDAAKujVUqDBg1i0aJFfPzxx/j7+3PkyBEAAgMD8fHxsThd5eLv73/O3CA/Pz+Cg4M1Z8giw4cPp0OHDsyYMYN77rmHH374gXnz5jFv3jyro1VKvXv3Zvr06URERNCkSRN+/vlnZs+ezT//+U+ro5VvRs4xZ84cExERYby8vEy7du3Mpk2brI5UaQHnvS1YsMDqaGKM6dq1qxk2bJjVMSq1Tz75xDRt2tTY7XbTqFEjM2/ePKsjVVppaWlm2LBhJiIiwnh7e5u6deuasWPHmtzcXKujlWs6j4qIiIi4LM1REREREZeloiIiIiIuS0VFREREXJaKioiIiLgsFRURERFxWSoqIiIi4rJUVERERMRlqaiIiIiIy1JREZEKxWaz8dFHH1kdQ0RKiIqKiJSY/v37Y7PZzrnddNNNVkcTkXJKFyUUkRJ10003sWDBgiLL7Ha7RWlEpLzTHhURKVF2u51atWoVuQUFBQF/DMvMnTuXXr164ePjQ926dVm6dGmR52/bto2//e1v+Pj4EBwczKOPPkpGRkaRdd544w2aNGmC3W4nNDSUwYMHF3n8xIkT9OnTB19fX+rXr8/y5ctL902LSKlRURGRMjV+/HjuuusufvnlF/r27cs//vEPduzYAUBmZiY9e/YkKCiIzZs38/777/P1118XKSJz585l0KBBPProo2zbto3ly5dTr169Iq8xefJk7rnnHn799Vduvvlm+vbty6lTp8r0fYpICbH68s0iUnH069fPuLu7Gz8/vyK36dOnG2OMAczjjz9e5Dnt27c3TzzxhDHGmHnz5pmgoCCTkZFR+Phnn31m3NzczJEjR4wxxoSFhZmxY8deMANgxo0bV3g/IyPDAOaLL74osfcpImVHc1REpER169aNuXPnFllWrVq1wn9ff/31RR67/vrr2bp1KwA7duygRYsW+Pn5FT7esWNHnE4nu3btwmazcfjwYbp3737RDM2bNy/8t5+fHwEBARw7duxK35KIWEhFRURKlJ+f3zlDMSXFx8fnstbz9PQsct9ms+F0OksjkoiUMs1REZEytWnTpnPuN27cGIDGjRvzyy+/kJmZWfj4hg0bcHNzo2HDhvj7+xMVFcWqVavKNLOIWEd7VESkROXm5nLkyJEiyzw8PKhevToA77//Pm3atKFTp0688847/PDDD8yfPx+Avn37MnHiRPr168ekSZM4fvw4Q4YM4cEHHyQkJASASZMm8fjjj1OzZk169epFeno6GzZsYMiQIWX7RkWkTKioiEiJ+vLLLwkNDS2yrGHDhuzcuRP444icJUuW8OSTTxIaGsrixYv5y1/+AoCvry9fffUVw4YNo23btvj6+nLXXXcxe/bswm3169ePnJwcXnjhBUaOHEn16tX5+9//XnZvUETKlM0YY6wOISKVg81mY9myZdxxxx1WRxGRckJzVERERMRlqaiIiIiIy9IcFREpMxppFpHi0h4VERERcVkqKiIiIuKyVFRERETEZamoiIiIiMtSURERERGXpaIiIiIiLktFRURERFyWioqIiIi4LBUVERERcVn/DzLB1tUWMshwAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "lrs = []\n",
    "for i in range(10):\n",
    "    optimizer.step()\n",
    "    lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "plt.plot(range(10), lrs, color = 'limegreen',  label = 'Training Cosine Annealing Warm Restarts')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning rate(green)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c49f961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(120),results_loss['epoch_by_trn'], color = 'dodgerblue', label = 'Contrastive Loss')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b168aa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"model_state_dict\": model.state_dict(),\n",
    "            },\n",
    "           f\"Contrastive_Embedding_Net(1117AE).pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "85dd10be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contrastive_Embedding(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=222, out_features=256, bias=True)\n",
      "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Dropout(p=0.56, inplace=False)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Dropout(p=0.56, inplace=False)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (9): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): Dropout(p=0.56, inplace=False)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Dropout(p=0.56, inplace=False)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Dropout(p=0.56, inplace=False)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=256, out_features=222, bias=True)\n",
      "    (9): BatchNorm1d(222, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbd1d29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start Getting the latent space vector(Train, Valid sample)\n",
      "818286\n",
      "\n",
      "End, Time consume(min):0.09841051896413168\n"
     ]
    }
   ],
   "source": [
    "emb_df = make_embeded_df(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80e742e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>stay_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15332791</td>\n",
       "      <td>30003598</td>\n",
       "      <td>20683754</td>\n",
       "      <td>-0.001014</td>\n",
       "      <td>-0.009162</td>\n",
       "      <td>-0.009464</td>\n",
       "      <td>-0.001002</td>\n",
       "      <td>-0.000988</td>\n",
       "      <td>-0.009106</td>\n",
       "      <td>-0.009416</td>\n",
       "      <td>-0.009061</td>\n",
       "      <td>-0.009510</td>\n",
       "      <td>-0.000999</td>\n",
       "      <td>-0.001019</td>\n",
       "      <td>-0.009394</td>\n",
       "      <td>-0.009065</td>\n",
       "      <td>-0.000993</td>\n",
       "      <td>-0.009465</td>\n",
       "      <td>-0.009292</td>\n",
       "      <td>-0.009414</td>\n",
       "      <td>-0.009063</td>\n",
       "      <td>-0.001027</td>\n",
       "      <td>-0.009478</td>\n",
       "      <td>-0.009288</td>\n",
       "      <td>-0.009373</td>\n",
       "      <td>-0.001029</td>\n",
       "      <td>-0.009483</td>\n",
       "      <td>-0.008518</td>\n",
       "      <td>-0.009234</td>\n",
       "      <td>-0.009363</td>\n",
       "      <td>-0.009054</td>\n",
       "      <td>-0.009238</td>\n",
       "      <td>-0.001008</td>\n",
       "      <td>-0.009250</td>\n",
       "      <td>-0.009453</td>\n",
       "      <td>-0.009230</td>\n",
       "      <td>-0.009493</td>\n",
       "      <td>-0.009371</td>\n",
       "      <td>-0.009301</td>\n",
       "      <td>-0.009320</td>\n",
       "      <td>-0.001029</td>\n",
       "      <td>-0.009140</td>\n",
       "      <td>-0.001023</td>\n",
       "      <td>-0.001028</td>\n",
       "      <td>-0.009337</td>\n",
       "      <td>-0.009065</td>\n",
       "      <td>-0.009471</td>\n",
       "      <td>-0.009558</td>\n",
       "      <td>-0.009213</td>\n",
       "      <td>-0.009430</td>\n",
       "      <td>-0.009309</td>\n",
       "      <td>-0.009456</td>\n",
       "      <td>-0.009356</td>\n",
       "      <td>-0.009362</td>\n",
       "      <td>-0.001025</td>\n",
       "      <td>-0.001021</td>\n",
       "      <td>-0.001035</td>\n",
       "      <td>-0.009370</td>\n",
       "      <td>-0.009389</td>\n",
       "      <td>-1.033548e-03</td>\n",
       "      <td>-0.009285</td>\n",
       "      <td>-0.009166</td>\n",
       "      <td>-0.009313</td>\n",
       "      <td>-0.009333</td>\n",
       "      <td>-0.009467</td>\n",
       "      <td>-0.008837</td>\n",
       "      <td>-0.009275</td>\n",
       "      <td>-0.008142</td>\n",
       "      <td>-0.009303</td>\n",
       "      <td>-0.009602</td>\n",
       "      <td>-0.009177</td>\n",
       "      <td>-0.009367</td>\n",
       "      <td>-0.007936</td>\n",
       "      <td>-0.009302</td>\n",
       "      <td>-0.009367</td>\n",
       "      <td>-0.009460</td>\n",
       "      <td>-0.001046</td>\n",
       "      <td>-0.001006</td>\n",
       "      <td>-0.001023</td>\n",
       "      <td>-0.009440</td>\n",
       "      <td>-0.009383</td>\n",
       "      <td>-0.009410</td>\n",
       "      <td>-0.001024</td>\n",
       "      <td>-0.009276</td>\n",
       "      <td>-0.009462</td>\n",
       "      <td>-0.009312</td>\n",
       "      <td>-0.009202</td>\n",
       "      <td>-0.009274</td>\n",
       "      <td>-0.009259</td>\n",
       "      <td>-0.009174</td>\n",
       "      <td>-0.009142</td>\n",
       "      <td>-0.009488</td>\n",
       "      <td>-0.009448</td>\n",
       "      <td>-0.009330</td>\n",
       "      <td>-0.001023</td>\n",
       "      <td>-0.008025</td>\n",
       "      <td>-0.009363</td>\n",
       "      <td>-0.000988</td>\n",
       "      <td>-0.001019</td>\n",
       "      <td>-0.009590</td>\n",
       "      <td>-0.009362</td>\n",
       "      <td>-0.009161</td>\n",
       "      <td>-0.009398</td>\n",
       "      <td>-0.008683</td>\n",
       "      <td>-0.001001</td>\n",
       "      <td>-0.009491</td>\n",
       "      <td>-0.009557</td>\n",
       "      <td>-0.009275</td>\n",
       "      <td>-0.009317</td>\n",
       "      <td>-0.009408</td>\n",
       "      <td>-0.001010</td>\n",
       "      <td>-0.001008</td>\n",
       "      <td>-0.009300</td>\n",
       "      <td>-0.009451</td>\n",
       "      <td>-0.009306</td>\n",
       "      <td>-0.009206</td>\n",
       "      <td>-0.001004</td>\n",
       "      <td>-0.009422</td>\n",
       "      <td>-0.001021</td>\n",
       "      <td>-0.001012</td>\n",
       "      <td>-0.009244</td>\n",
       "      <td>-0.001021</td>\n",
       "      <td>-0.001018</td>\n",
       "      <td>-0.009380</td>\n",
       "      <td>-0.001021</td>\n",
       "      <td>-0.009279</td>\n",
       "      <td>-0.009660</td>\n",
       "      <td>-0.001005</td>\n",
       "      <td>-0.009348</td>\n",
       "      <td>-0.001009</td>\n",
       "      <td>-0.009439</td>\n",
       "      <td>-0.001048</td>\n",
       "      <td>-0.009281</td>\n",
       "      <td>-0.001029</td>\n",
       "      <td>-0.009491</td>\n",
       "      <td>-0.009398</td>\n",
       "      <td>-0.009379</td>\n",
       "      <td>-0.009317</td>\n",
       "      <td>-0.001008</td>\n",
       "      <td>-0.009379</td>\n",
       "      <td>-0.001011</td>\n",
       "      <td>-0.000994</td>\n",
       "      <td>-0.008322</td>\n",
       "      <td>-0.009528</td>\n",
       "      <td>-0.009209</td>\n",
       "      <td>-0.009175</td>\n",
       "      <td>-0.001023</td>\n",
       "      <td>-0.009396</td>\n",
       "      <td>-0.009037</td>\n",
       "      <td>-0.001002</td>\n",
       "      <td>-0.009413</td>\n",
       "      <td>-0.001009</td>\n",
       "      <td>-0.009416</td>\n",
       "      <td>-0.009291</td>\n",
       "      <td>-0.009421</td>\n",
       "      <td>-0.009411</td>\n",
       "      <td>-0.009273</td>\n",
       "      <td>-0.009410</td>\n",
       "      <td>-0.009351</td>\n",
       "      <td>-0.009103</td>\n",
       "      <td>-0.001015</td>\n",
       "      <td>-0.009423</td>\n",
       "      <td>-0.001018</td>\n",
       "      <td>-0.009166</td>\n",
       "      <td>-0.009409</td>\n",
       "      <td>-0.009423</td>\n",
       "      <td>-0.009473</td>\n",
       "      <td>-0.001024</td>\n",
       "      <td>-0.009293</td>\n",
       "      <td>-0.009384</td>\n",
       "      <td>-0.001020</td>\n",
       "      <td>-0.009617</td>\n",
       "      <td>-0.009393</td>\n",
       "      <td>-0.009280</td>\n",
       "      <td>-0.009501</td>\n",
       "      <td>-0.001018</td>\n",
       "      <td>-0.001024</td>\n",
       "      <td>-0.009511</td>\n",
       "      <td>-0.009249</td>\n",
       "      <td>-0.009229</td>\n",
       "      <td>-0.009345</td>\n",
       "      <td>-0.009345</td>\n",
       "      <td>-0.001010</td>\n",
       "      <td>-0.001001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15332791</td>\n",
       "      <td>30003598</td>\n",
       "      <td>20683754</td>\n",
       "      <td>-0.001003</td>\n",
       "      <td>-0.007288</td>\n",
       "      <td>-0.007451</td>\n",
       "      <td>-0.000990</td>\n",
       "      <td>-0.000978</td>\n",
       "      <td>-0.007154</td>\n",
       "      <td>-0.007484</td>\n",
       "      <td>-0.007209</td>\n",
       "      <td>-0.007576</td>\n",
       "      <td>-0.000989</td>\n",
       "      <td>-0.001007</td>\n",
       "      <td>-0.007447</td>\n",
       "      <td>-0.007117</td>\n",
       "      <td>-0.000982</td>\n",
       "      <td>-0.007538</td>\n",
       "      <td>-0.007402</td>\n",
       "      <td>-0.007486</td>\n",
       "      <td>-0.007225</td>\n",
       "      <td>-0.001018</td>\n",
       "      <td>-0.007545</td>\n",
       "      <td>-0.007335</td>\n",
       "      <td>-0.007459</td>\n",
       "      <td>-0.001019</td>\n",
       "      <td>-0.007549</td>\n",
       "      <td>-0.006784</td>\n",
       "      <td>-0.007350</td>\n",
       "      <td>-0.007437</td>\n",
       "      <td>-0.007102</td>\n",
       "      <td>-0.007348</td>\n",
       "      <td>-0.000998</td>\n",
       "      <td>-0.007353</td>\n",
       "      <td>-0.007526</td>\n",
       "      <td>-0.007352</td>\n",
       "      <td>-0.007554</td>\n",
       "      <td>-0.007345</td>\n",
       "      <td>-0.007303</td>\n",
       "      <td>-0.007397</td>\n",
       "      <td>-0.001018</td>\n",
       "      <td>-0.007278</td>\n",
       "      <td>-0.001013</td>\n",
       "      <td>-0.001017</td>\n",
       "      <td>-0.007417</td>\n",
       "      <td>-0.007215</td>\n",
       "      <td>-0.007545</td>\n",
       "      <td>-0.007584</td>\n",
       "      <td>-0.007325</td>\n",
       "      <td>-0.007496</td>\n",
       "      <td>-0.007334</td>\n",
       "      <td>-0.007505</td>\n",
       "      <td>-0.007454</td>\n",
       "      <td>-0.007461</td>\n",
       "      <td>-0.001013</td>\n",
       "      <td>-0.001012</td>\n",
       "      <td>-0.001024</td>\n",
       "      <td>-0.007456</td>\n",
       "      <td>-0.007464</td>\n",
       "      <td>-1.024011e-03</td>\n",
       "      <td>-0.007380</td>\n",
       "      <td>-0.007281</td>\n",
       "      <td>-0.007422</td>\n",
       "      <td>-0.007356</td>\n",
       "      <td>-0.007525</td>\n",
       "      <td>-0.006911</td>\n",
       "      <td>-0.007310</td>\n",
       "      <td>-0.006353</td>\n",
       "      <td>-0.007400</td>\n",
       "      <td>-0.007627</td>\n",
       "      <td>-0.007290</td>\n",
       "      <td>-0.007451</td>\n",
       "      <td>-0.006227</td>\n",
       "      <td>-0.007362</td>\n",
       "      <td>-0.007451</td>\n",
       "      <td>-0.007529</td>\n",
       "      <td>-0.001035</td>\n",
       "      <td>-0.000995</td>\n",
       "      <td>-0.001014</td>\n",
       "      <td>-0.007501</td>\n",
       "      <td>-0.007395</td>\n",
       "      <td>-0.007488</td>\n",
       "      <td>-0.001014</td>\n",
       "      <td>-0.007338</td>\n",
       "      <td>-0.007538</td>\n",
       "      <td>-0.007412</td>\n",
       "      <td>-0.007328</td>\n",
       "      <td>-0.007379</td>\n",
       "      <td>-0.007363</td>\n",
       "      <td>-0.007286</td>\n",
       "      <td>-0.007254</td>\n",
       "      <td>-0.007497</td>\n",
       "      <td>-0.007525</td>\n",
       "      <td>-0.007428</td>\n",
       "      <td>-0.001013</td>\n",
       "      <td>-0.006308</td>\n",
       "      <td>-0.007450</td>\n",
       "      <td>-0.000977</td>\n",
       "      <td>-0.001007</td>\n",
       "      <td>-0.007611</td>\n",
       "      <td>-0.007436</td>\n",
       "      <td>-0.007283</td>\n",
       "      <td>-0.007480</td>\n",
       "      <td>-0.006836</td>\n",
       "      <td>-0.000990</td>\n",
       "      <td>-0.007506</td>\n",
       "      <td>-0.007531</td>\n",
       "      <td>-0.007374</td>\n",
       "      <td>-0.007347</td>\n",
       "      <td>-0.007466</td>\n",
       "      <td>-0.001000</td>\n",
       "      <td>-0.000998</td>\n",
       "      <td>-0.007390</td>\n",
       "      <td>-0.007505</td>\n",
       "      <td>-0.007408</td>\n",
       "      <td>-0.007319</td>\n",
       "      <td>-0.000994</td>\n",
       "      <td>-0.007464</td>\n",
       "      <td>-0.001010</td>\n",
       "      <td>-0.001001</td>\n",
       "      <td>-0.007288</td>\n",
       "      <td>-0.001009</td>\n",
       "      <td>-0.001007</td>\n",
       "      <td>-0.007459</td>\n",
       "      <td>-0.001010</td>\n",
       "      <td>-0.007384</td>\n",
       "      <td>-0.007669</td>\n",
       "      <td>-0.000995</td>\n",
       "      <td>-0.007356</td>\n",
       "      <td>-0.000998</td>\n",
       "      <td>-0.007506</td>\n",
       "      <td>-0.001037</td>\n",
       "      <td>-0.007388</td>\n",
       "      <td>-0.001018</td>\n",
       "      <td>-0.007477</td>\n",
       "      <td>-0.007395</td>\n",
       "      <td>-0.007461</td>\n",
       "      <td>-0.007384</td>\n",
       "      <td>-0.000997</td>\n",
       "      <td>-0.007403</td>\n",
       "      <td>-0.001001</td>\n",
       "      <td>-0.000982</td>\n",
       "      <td>-0.006583</td>\n",
       "      <td>-0.007574</td>\n",
       "      <td>-0.007276</td>\n",
       "      <td>-0.007202</td>\n",
       "      <td>-0.001013</td>\n",
       "      <td>-0.007479</td>\n",
       "      <td>-0.007162</td>\n",
       "      <td>-0.000990</td>\n",
       "      <td>-0.007485</td>\n",
       "      <td>-0.000998</td>\n",
       "      <td>-0.007470</td>\n",
       "      <td>-0.007389</td>\n",
       "      <td>-0.007494</td>\n",
       "      <td>-0.007490</td>\n",
       "      <td>-0.007375</td>\n",
       "      <td>-0.007482</td>\n",
       "      <td>-0.007433</td>\n",
       "      <td>-0.007242</td>\n",
       "      <td>-0.001005</td>\n",
       "      <td>-0.007500</td>\n",
       "      <td>-0.001007</td>\n",
       "      <td>-0.007307</td>\n",
       "      <td>-0.007475</td>\n",
       "      <td>-0.007504</td>\n",
       "      <td>-0.007513</td>\n",
       "      <td>-0.001012</td>\n",
       "      <td>-0.007372</td>\n",
       "      <td>-0.007466</td>\n",
       "      <td>-0.001009</td>\n",
       "      <td>-0.007597</td>\n",
       "      <td>-0.007446</td>\n",
       "      <td>-0.007389</td>\n",
       "      <td>-0.007544</td>\n",
       "      <td>-0.001007</td>\n",
       "      <td>-0.001012</td>\n",
       "      <td>-0.007510</td>\n",
       "      <td>-0.007357</td>\n",
       "      <td>-0.007338</td>\n",
       "      <td>-0.007445</td>\n",
       "      <td>-0.007325</td>\n",
       "      <td>-0.000999</td>\n",
       "      <td>-0.000990</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15332791</td>\n",
       "      <td>30003598</td>\n",
       "      <td>20683754</td>\n",
       "      <td>-0.000995</td>\n",
       "      <td>-0.002681</td>\n",
       "      <td>-0.002496</td>\n",
       "      <td>-0.000977</td>\n",
       "      <td>-0.000968</td>\n",
       "      <td>-0.002344</td>\n",
       "      <td>-0.002719</td>\n",
       "      <td>-0.002637</td>\n",
       "      <td>-0.002803</td>\n",
       "      <td>-0.000981</td>\n",
       "      <td>-0.000997</td>\n",
       "      <td>-0.002664</td>\n",
       "      <td>-0.002325</td>\n",
       "      <td>-0.000974</td>\n",
       "      <td>-0.002783</td>\n",
       "      <td>-0.002726</td>\n",
       "      <td>-0.002720</td>\n",
       "      <td>-0.002675</td>\n",
       "      <td>-0.001011</td>\n",
       "      <td>-0.002779</td>\n",
       "      <td>-0.002537</td>\n",
       "      <td>-0.002722</td>\n",
       "      <td>-0.001009</td>\n",
       "      <td>-0.002778</td>\n",
       "      <td>-0.002492</td>\n",
       "      <td>-0.002711</td>\n",
       "      <td>-0.002688</td>\n",
       "      <td>-0.002309</td>\n",
       "      <td>-0.002686</td>\n",
       "      <td>-0.000991</td>\n",
       "      <td>-0.002671</td>\n",
       "      <td>-0.002769</td>\n",
       "      <td>-0.002730</td>\n",
       "      <td>-0.002770</td>\n",
       "      <td>-0.002354</td>\n",
       "      <td>-0.002407</td>\n",
       "      <td>-0.002653</td>\n",
       "      <td>-0.001009</td>\n",
       "      <td>-0.002703</td>\n",
       "      <td>-0.001005</td>\n",
       "      <td>-0.001008</td>\n",
       "      <td>-0.002692</td>\n",
       "      <td>-0.002650</td>\n",
       "      <td>-0.002789</td>\n",
       "      <td>-0.002709</td>\n",
       "      <td>-0.002645</td>\n",
       "      <td>-0.002725</td>\n",
       "      <td>-0.002463</td>\n",
       "      <td>-0.002695</td>\n",
       "      <td>-0.002748</td>\n",
       "      <td>-0.002758</td>\n",
       "      <td>-0.001002</td>\n",
       "      <td>-0.001003</td>\n",
       "      <td>-0.001015</td>\n",
       "      <td>-0.002722</td>\n",
       "      <td>-0.002719</td>\n",
       "      <td>-1.015734e-03</td>\n",
       "      <td>-0.002676</td>\n",
       "      <td>-0.002643</td>\n",
       "      <td>-0.002747</td>\n",
       "      <td>-0.002496</td>\n",
       "      <td>-0.002752</td>\n",
       "      <td>-0.002202</td>\n",
       "      <td>-0.002454</td>\n",
       "      <td>-0.001988</td>\n",
       "      <td>-0.002714</td>\n",
       "      <td>-0.002742</td>\n",
       "      <td>-0.002631</td>\n",
       "      <td>-0.002725</td>\n",
       "      <td>-0.001989</td>\n",
       "      <td>-0.002600</td>\n",
       "      <td>-0.002719</td>\n",
       "      <td>-0.002767</td>\n",
       "      <td>-0.001024</td>\n",
       "      <td>-0.000982</td>\n",
       "      <td>-0.001003</td>\n",
       "      <td>-0.002718</td>\n",
       "      <td>-0.002509</td>\n",
       "      <td>-0.002746</td>\n",
       "      <td>-0.001004</td>\n",
       "      <td>-0.002572</td>\n",
       "      <td>-0.002800</td>\n",
       "      <td>-0.002726</td>\n",
       "      <td>-0.002702</td>\n",
       "      <td>-0.002711</td>\n",
       "      <td>-0.002676</td>\n",
       "      <td>-0.002625</td>\n",
       "      <td>-0.002596</td>\n",
       "      <td>-0.002608</td>\n",
       "      <td>-0.002771</td>\n",
       "      <td>-0.002737</td>\n",
       "      <td>-0.001004</td>\n",
       "      <td>-0.002016</td>\n",
       "      <td>-0.002729</td>\n",
       "      <td>-0.000964</td>\n",
       "      <td>-0.000994</td>\n",
       "      <td>-0.002716</td>\n",
       "      <td>-0.002663</td>\n",
       "      <td>-0.002637</td>\n",
       "      <td>-0.002748</td>\n",
       "      <td>-0.002240</td>\n",
       "      <td>-0.000983</td>\n",
       "      <td>-0.002604</td>\n",
       "      <td>-0.002538</td>\n",
       "      <td>-0.002688</td>\n",
       "      <td>-0.002493</td>\n",
       "      <td>-0.002668</td>\n",
       "      <td>-0.000993</td>\n",
       "      <td>-0.000990</td>\n",
       "      <td>-0.002679</td>\n",
       "      <td>-0.002706</td>\n",
       "      <td>-0.002728</td>\n",
       "      <td>-0.002663</td>\n",
       "      <td>-0.000984</td>\n",
       "      <td>-0.002625</td>\n",
       "      <td>-0.001000</td>\n",
       "      <td>-0.000990</td>\n",
       "      <td>-0.002456</td>\n",
       "      <td>-0.001000</td>\n",
       "      <td>-0.000996</td>\n",
       "      <td>-0.002717</td>\n",
       "      <td>-0.001000</td>\n",
       "      <td>-0.002700</td>\n",
       "      <td>-0.002761</td>\n",
       "      <td>-0.000989</td>\n",
       "      <td>-0.002479</td>\n",
       "      <td>-0.000991</td>\n",
       "      <td>-0.002720</td>\n",
       "      <td>-0.001027</td>\n",
       "      <td>-0.002727</td>\n",
       "      <td>-0.001007</td>\n",
       "      <td>-0.002499</td>\n",
       "      <td>-0.002462</td>\n",
       "      <td>-0.002728</td>\n",
       "      <td>-0.002611</td>\n",
       "      <td>-0.000987</td>\n",
       "      <td>-0.002536</td>\n",
       "      <td>-0.000989</td>\n",
       "      <td>-0.000969</td>\n",
       "      <td>-0.002340</td>\n",
       "      <td>-0.002760</td>\n",
       "      <td>-0.002500</td>\n",
       "      <td>-0.002351</td>\n",
       "      <td>-0.001003</td>\n",
       "      <td>-0.002747</td>\n",
       "      <td>-0.002497</td>\n",
       "      <td>-0.000976</td>\n",
       "      <td>-0.002725</td>\n",
       "      <td>-0.000987</td>\n",
       "      <td>-0.002667</td>\n",
       "      <td>-0.002694</td>\n",
       "      <td>-0.002744</td>\n",
       "      <td>-0.002748</td>\n",
       "      <td>-0.002688</td>\n",
       "      <td>-0.002728</td>\n",
       "      <td>-0.002708</td>\n",
       "      <td>-0.002657</td>\n",
       "      <td>-0.000995</td>\n",
       "      <td>-0.002762</td>\n",
       "      <td>-0.000998</td>\n",
       "      <td>-0.002721</td>\n",
       "      <td>-0.002702</td>\n",
       "      <td>-0.002770</td>\n",
       "      <td>-0.002672</td>\n",
       "      <td>-0.001004</td>\n",
       "      <td>-0.002643</td>\n",
       "      <td>-0.002740</td>\n",
       "      <td>-0.000997</td>\n",
       "      <td>-0.002604</td>\n",
       "      <td>-0.002634</td>\n",
       "      <td>-0.002720</td>\n",
       "      <td>-0.002718</td>\n",
       "      <td>-0.000997</td>\n",
       "      <td>-0.001000</td>\n",
       "      <td>-0.002581</td>\n",
       "      <td>-0.002677</td>\n",
       "      <td>-0.002676</td>\n",
       "      <td>-0.002753</td>\n",
       "      <td>-0.002326</td>\n",
       "      <td>-0.000990</td>\n",
       "      <td>-0.000978</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15332791</td>\n",
       "      <td>30003598</td>\n",
       "      <td>20683754</td>\n",
       "      <td>-0.001012</td>\n",
       "      <td>-0.008751</td>\n",
       "      <td>-0.009039</td>\n",
       "      <td>-0.001000</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>-0.008684</td>\n",
       "      <td>-0.009004</td>\n",
       "      <td>-0.008671</td>\n",
       "      <td>-0.009101</td>\n",
       "      <td>-0.000998</td>\n",
       "      <td>-0.001017</td>\n",
       "      <td>-0.008980</td>\n",
       "      <td>-0.008639</td>\n",
       "      <td>-0.000991</td>\n",
       "      <td>-0.009054</td>\n",
       "      <td>-0.008890</td>\n",
       "      <td>-0.009001</td>\n",
       "      <td>-0.008683</td>\n",
       "      <td>-0.001025</td>\n",
       "      <td>-0.009061</td>\n",
       "      <td>-0.008864</td>\n",
       "      <td>-0.008975</td>\n",
       "      <td>-0.001028</td>\n",
       "      <td>-0.009078</td>\n",
       "      <td>-0.008179</td>\n",
       "      <td>-0.008833</td>\n",
       "      <td>-0.008957</td>\n",
       "      <td>-0.008666</td>\n",
       "      <td>-0.008840</td>\n",
       "      <td>-0.001006</td>\n",
       "      <td>-0.008845</td>\n",
       "      <td>-0.009037</td>\n",
       "      <td>-0.008822</td>\n",
       "      <td>-0.009082</td>\n",
       "      <td>-0.008937</td>\n",
       "      <td>-0.008857</td>\n",
       "      <td>-0.008915</td>\n",
       "      <td>-0.001028</td>\n",
       "      <td>-0.008741</td>\n",
       "      <td>-0.001021</td>\n",
       "      <td>-0.001026</td>\n",
       "      <td>-0.008923</td>\n",
       "      <td>-0.008674</td>\n",
       "      <td>-0.009059</td>\n",
       "      <td>-0.009144</td>\n",
       "      <td>-0.008814</td>\n",
       "      <td>-0.009022</td>\n",
       "      <td>-0.008893</td>\n",
       "      <td>-0.009034</td>\n",
       "      <td>-0.008951</td>\n",
       "      <td>-0.008960</td>\n",
       "      <td>-0.001022</td>\n",
       "      <td>-0.001020</td>\n",
       "      <td>-0.001033</td>\n",
       "      <td>-0.008966</td>\n",
       "      <td>-0.008977</td>\n",
       "      <td>-1.032175e-03</td>\n",
       "      <td>-0.008871</td>\n",
       "      <td>-0.008751</td>\n",
       "      <td>-0.008913</td>\n",
       "      <td>-0.008921</td>\n",
       "      <td>-0.009047</td>\n",
       "      <td>-0.008431</td>\n",
       "      <td>-0.008862</td>\n",
       "      <td>-0.007720</td>\n",
       "      <td>-0.008888</td>\n",
       "      <td>-0.009185</td>\n",
       "      <td>-0.008781</td>\n",
       "      <td>-0.008965</td>\n",
       "      <td>-0.007570</td>\n",
       "      <td>-0.008865</td>\n",
       "      <td>-0.008960</td>\n",
       "      <td>-0.009050</td>\n",
       "      <td>-0.001044</td>\n",
       "      <td>-0.001004</td>\n",
       "      <td>-0.001022</td>\n",
       "      <td>-0.009029</td>\n",
       "      <td>-0.008952</td>\n",
       "      <td>-0.009006</td>\n",
       "      <td>-0.001022</td>\n",
       "      <td>-0.008868</td>\n",
       "      <td>-0.009046</td>\n",
       "      <td>-0.008911</td>\n",
       "      <td>-0.008808</td>\n",
       "      <td>-0.008871</td>\n",
       "      <td>-0.008860</td>\n",
       "      <td>-0.008781</td>\n",
       "      <td>-0.008738</td>\n",
       "      <td>-0.009059</td>\n",
       "      <td>-0.009043</td>\n",
       "      <td>-0.008926</td>\n",
       "      <td>-0.001021</td>\n",
       "      <td>-0.007683</td>\n",
       "      <td>-0.008953</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>-0.001016</td>\n",
       "      <td>-0.009170</td>\n",
       "      <td>-0.008954</td>\n",
       "      <td>-0.008762</td>\n",
       "      <td>-0.008996</td>\n",
       "      <td>-0.008284</td>\n",
       "      <td>-0.000999</td>\n",
       "      <td>-0.009076</td>\n",
       "      <td>-0.009123</td>\n",
       "      <td>-0.008860</td>\n",
       "      <td>-0.008894</td>\n",
       "      <td>-0.008993</td>\n",
       "      <td>-0.001009</td>\n",
       "      <td>-0.001006</td>\n",
       "      <td>-0.008895</td>\n",
       "      <td>-0.009038</td>\n",
       "      <td>-0.008906</td>\n",
       "      <td>-0.008816</td>\n",
       "      <td>-0.001002</td>\n",
       "      <td>-0.009001</td>\n",
       "      <td>-0.001019</td>\n",
       "      <td>-0.001010</td>\n",
       "      <td>-0.008823</td>\n",
       "      <td>-0.001019</td>\n",
       "      <td>-0.001016</td>\n",
       "      <td>-0.008971</td>\n",
       "      <td>-0.001019</td>\n",
       "      <td>-0.008875</td>\n",
       "      <td>-0.009236</td>\n",
       "      <td>-0.001004</td>\n",
       "      <td>-0.008910</td>\n",
       "      <td>-0.001007</td>\n",
       "      <td>-0.009024</td>\n",
       "      <td>-0.001046</td>\n",
       "      <td>-0.008880</td>\n",
       "      <td>-0.001027</td>\n",
       "      <td>-0.009074</td>\n",
       "      <td>-0.008964</td>\n",
       "      <td>-0.008973</td>\n",
       "      <td>-0.008895</td>\n",
       "      <td>-0.001006</td>\n",
       "      <td>-0.008952</td>\n",
       "      <td>-0.001010</td>\n",
       "      <td>-0.000991</td>\n",
       "      <td>-0.007931</td>\n",
       "      <td>-0.009114</td>\n",
       "      <td>-0.008804</td>\n",
       "      <td>-0.008762</td>\n",
       "      <td>-0.001021</td>\n",
       "      <td>-0.008996</td>\n",
       "      <td>-0.008627</td>\n",
       "      <td>-0.001000</td>\n",
       "      <td>-0.009006</td>\n",
       "      <td>-0.001007</td>\n",
       "      <td>-0.009013</td>\n",
       "      <td>-0.008889</td>\n",
       "      <td>-0.009011</td>\n",
       "      <td>-0.009000</td>\n",
       "      <td>-0.008868</td>\n",
       "      <td>-0.008993</td>\n",
       "      <td>-0.008942</td>\n",
       "      <td>-0.008700</td>\n",
       "      <td>-0.001013</td>\n",
       "      <td>-0.009020</td>\n",
       "      <td>-0.001015</td>\n",
       "      <td>-0.008773</td>\n",
       "      <td>-0.008995</td>\n",
       "      <td>-0.009018</td>\n",
       "      <td>-0.009061</td>\n",
       "      <td>-0.001022</td>\n",
       "      <td>-0.008874</td>\n",
       "      <td>-0.008972</td>\n",
       "      <td>-0.001018</td>\n",
       "      <td>-0.009190</td>\n",
       "      <td>-0.008986</td>\n",
       "      <td>-0.008881</td>\n",
       "      <td>-0.009087</td>\n",
       "      <td>-0.001016</td>\n",
       "      <td>-0.001022</td>\n",
       "      <td>-0.009090</td>\n",
       "      <td>-0.008847</td>\n",
       "      <td>-0.008831</td>\n",
       "      <td>-0.008939</td>\n",
       "      <td>-0.008943</td>\n",
       "      <td>-0.001008</td>\n",
       "      <td>-0.000999</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15332791</td>\n",
       "      <td>30003598</td>\n",
       "      <td>20683754</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>0.648427</td>\n",
       "      <td>0.643528</td>\n",
       "      <td>0.008855</td>\n",
       "      <td>0.011781</td>\n",
       "      <td>0.620801</td>\n",
       "      <td>0.656128</td>\n",
       "      <td>0.631469</td>\n",
       "      <td>0.665730</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>-0.000176</td>\n",
       "      <td>0.653565</td>\n",
       "      <td>0.608773</td>\n",
       "      <td>-0.000123</td>\n",
       "      <td>0.663517</td>\n",
       "      <td>0.645223</td>\n",
       "      <td>0.650598</td>\n",
       "      <td>0.648566</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>0.659954</td>\n",
       "      <td>0.647487</td>\n",
       "      <td>0.651744</td>\n",
       "      <td>-0.000232</td>\n",
       "      <td>0.660096</td>\n",
       "      <td>0.620524</td>\n",
       "      <td>0.651149</td>\n",
       "      <td>0.655505</td>\n",
       "      <td>0.610383</td>\n",
       "      <td>0.653320</td>\n",
       "      <td>-0.000199</td>\n",
       "      <td>0.650320</td>\n",
       "      <td>0.663511</td>\n",
       "      <td>0.651852</td>\n",
       "      <td>0.662088</td>\n",
       "      <td>0.616023</td>\n",
       "      <td>0.624541</td>\n",
       "      <td>0.639967</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>0.649639</td>\n",
       "      <td>0.010598</td>\n",
       "      <td>0.017045</td>\n",
       "      <td>0.648658</td>\n",
       "      <td>0.631175</td>\n",
       "      <td>0.667505</td>\n",
       "      <td>0.666819</td>\n",
       "      <td>0.640381</td>\n",
       "      <td>0.657863</td>\n",
       "      <td>0.641844</td>\n",
       "      <td>0.657854</td>\n",
       "      <td>0.653839</td>\n",
       "      <td>0.656496</td>\n",
       "      <td>0.018186</td>\n",
       "      <td>0.023106</td>\n",
       "      <td>0.013818</td>\n",
       "      <td>0.656313</td>\n",
       "      <td>0.657319</td>\n",
       "      <td>-5.540359e-08</td>\n",
       "      <td>0.646976</td>\n",
       "      <td>0.640504</td>\n",
       "      <td>0.658676</td>\n",
       "      <td>0.634443</td>\n",
       "      <td>0.652564</td>\n",
       "      <td>0.614382</td>\n",
       "      <td>0.633134</td>\n",
       "      <td>0.542302</td>\n",
       "      <td>0.648736</td>\n",
       "      <td>0.660888</td>\n",
       "      <td>0.636784</td>\n",
       "      <td>0.655600</td>\n",
       "      <td>0.560615</td>\n",
       "      <td>0.643516</td>\n",
       "      <td>0.655644</td>\n",
       "      <td>0.658351</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.002762</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.653391</td>\n",
       "      <td>0.646850</td>\n",
       "      <td>0.657715</td>\n",
       "      <td>0.007887</td>\n",
       "      <td>0.646733</td>\n",
       "      <td>0.659195</td>\n",
       "      <td>0.652912</td>\n",
       "      <td>0.643206</td>\n",
       "      <td>0.647057</td>\n",
       "      <td>0.648090</td>\n",
       "      <td>0.642169</td>\n",
       "      <td>0.641640</td>\n",
       "      <td>0.653995</td>\n",
       "      <td>0.661781</td>\n",
       "      <td>0.655282</td>\n",
       "      <td>0.014960</td>\n",
       "      <td>0.570485</td>\n",
       "      <td>0.652350</td>\n",
       "      <td>0.010563</td>\n",
       "      <td>0.009437</td>\n",
       "      <td>0.660488</td>\n",
       "      <td>0.653095</td>\n",
       "      <td>0.634200</td>\n",
       "      <td>0.653922</td>\n",
       "      <td>0.607131</td>\n",
       "      <td>0.021454</td>\n",
       "      <td>0.654925</td>\n",
       "      <td>0.656218</td>\n",
       "      <td>0.649529</td>\n",
       "      <td>0.639990</td>\n",
       "      <td>0.649495</td>\n",
       "      <td>-0.000094</td>\n",
       "      <td>0.006709</td>\n",
       "      <td>0.653425</td>\n",
       "      <td>0.655688</td>\n",
       "      <td>0.652335</td>\n",
       "      <td>0.644923</td>\n",
       "      <td>0.008469</td>\n",
       "      <td>0.648315</td>\n",
       "      <td>0.010931</td>\n",
       "      <td>0.008140</td>\n",
       "      <td>0.632811</td>\n",
       "      <td>0.004315</td>\n",
       "      <td>0.021207</td>\n",
       "      <td>0.652762</td>\n",
       "      <td>0.012574</td>\n",
       "      <td>0.648345</td>\n",
       "      <td>0.665775</td>\n",
       "      <td>-0.000112</td>\n",
       "      <td>0.631719</td>\n",
       "      <td>0.016916</td>\n",
       "      <td>0.658777</td>\n",
       "      <td>0.009262</td>\n",
       "      <td>0.648784</td>\n",
       "      <td>-0.000121</td>\n",
       "      <td>0.653595</td>\n",
       "      <td>0.641731</td>\n",
       "      <td>0.656637</td>\n",
       "      <td>0.645517</td>\n",
       "      <td>0.025143</td>\n",
       "      <td>0.639684</td>\n",
       "      <td>-0.000106</td>\n",
       "      <td>0.024876</td>\n",
       "      <td>0.595344</td>\n",
       "      <td>0.661314</td>\n",
       "      <td>0.635416</td>\n",
       "      <td>0.622680</td>\n",
       "      <td>0.002382</td>\n",
       "      <td>0.658893</td>\n",
       "      <td>0.635736</td>\n",
       "      <td>0.006522</td>\n",
       "      <td>0.658221</td>\n",
       "      <td>0.011569</td>\n",
       "      <td>0.655396</td>\n",
       "      <td>0.650684</td>\n",
       "      <td>0.653964</td>\n",
       "      <td>0.662640</td>\n",
       "      <td>0.643047</td>\n",
       "      <td>0.660101</td>\n",
       "      <td>0.655441</td>\n",
       "      <td>0.636419</td>\n",
       "      <td>-0.000145</td>\n",
       "      <td>0.659663</td>\n",
       "      <td>0.022437</td>\n",
       "      <td>0.649595</td>\n",
       "      <td>0.653430</td>\n",
       "      <td>0.658319</td>\n",
       "      <td>0.651154</td>\n",
       "      <td>0.019136</td>\n",
       "      <td>0.644634</td>\n",
       "      <td>0.656112</td>\n",
       "      <td>0.020919</td>\n",
       "      <td>0.655382</td>\n",
       "      <td>0.648386</td>\n",
       "      <td>0.650683</td>\n",
       "      <td>0.655027</td>\n",
       "      <td>0.027261</td>\n",
       "      <td>0.000915</td>\n",
       "      <td>0.654234</td>\n",
       "      <td>0.648112</td>\n",
       "      <td>0.646842</td>\n",
       "      <td>0.653333</td>\n",
       "      <td>0.607793</td>\n",
       "      <td>0.011688</td>\n",
       "      <td>0.001387</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818281</th>\n",
       "      <td>13651601</td>\n",
       "      <td>39999230</td>\n",
       "      <td>22584645</td>\n",
       "      <td>-0.001007</td>\n",
       "      <td>-0.007982</td>\n",
       "      <td>-0.008218</td>\n",
       "      <td>-0.000995</td>\n",
       "      <td>-0.000982</td>\n",
       "      <td>-0.007906</td>\n",
       "      <td>-0.008217</td>\n",
       "      <td>-0.007908</td>\n",
       "      <td>-0.008304</td>\n",
       "      <td>-0.000993</td>\n",
       "      <td>-0.001012</td>\n",
       "      <td>-0.008191</td>\n",
       "      <td>-0.007861</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>-0.008262</td>\n",
       "      <td>-0.008112</td>\n",
       "      <td>-0.008205</td>\n",
       "      <td>-0.007927</td>\n",
       "      <td>-0.001021</td>\n",
       "      <td>-0.008269</td>\n",
       "      <td>-0.008070</td>\n",
       "      <td>-0.008185</td>\n",
       "      <td>-0.001023</td>\n",
       "      <td>-0.008279</td>\n",
       "      <td>-0.007462</td>\n",
       "      <td>-0.008063</td>\n",
       "      <td>-0.008165</td>\n",
       "      <td>-0.007864</td>\n",
       "      <td>-0.008067</td>\n",
       "      <td>-0.001002</td>\n",
       "      <td>-0.008076</td>\n",
       "      <td>-0.008248</td>\n",
       "      <td>-0.008058</td>\n",
       "      <td>-0.008284</td>\n",
       "      <td>-0.008128</td>\n",
       "      <td>-0.008042</td>\n",
       "      <td>-0.008120</td>\n",
       "      <td>-0.001023</td>\n",
       "      <td>-0.007987</td>\n",
       "      <td>-0.001017</td>\n",
       "      <td>-0.001022</td>\n",
       "      <td>-0.008142</td>\n",
       "      <td>-0.007905</td>\n",
       "      <td>-0.008270</td>\n",
       "      <td>-0.008333</td>\n",
       "      <td>-0.008034</td>\n",
       "      <td>-0.008227</td>\n",
       "      <td>-0.008086</td>\n",
       "      <td>-0.008244</td>\n",
       "      <td>-0.008168</td>\n",
       "      <td>-0.008172</td>\n",
       "      <td>-0.001017</td>\n",
       "      <td>-0.001015</td>\n",
       "      <td>-0.001028</td>\n",
       "      <td>-0.008183</td>\n",
       "      <td>-0.008191</td>\n",
       "      <td>-1.027544e-03</td>\n",
       "      <td>-0.008096</td>\n",
       "      <td>-0.007981</td>\n",
       "      <td>-0.008130</td>\n",
       "      <td>-0.008107</td>\n",
       "      <td>-0.008256</td>\n",
       "      <td>-0.007652</td>\n",
       "      <td>-0.008049</td>\n",
       "      <td>-0.007008</td>\n",
       "      <td>-0.008109</td>\n",
       "      <td>-0.008374</td>\n",
       "      <td>-0.007998</td>\n",
       "      <td>-0.008177</td>\n",
       "      <td>-0.006865</td>\n",
       "      <td>-0.008080</td>\n",
       "      <td>-0.008173</td>\n",
       "      <td>-0.008252</td>\n",
       "      <td>-0.001039</td>\n",
       "      <td>-0.000999</td>\n",
       "      <td>-0.001017</td>\n",
       "      <td>-0.008237</td>\n",
       "      <td>-0.008153</td>\n",
       "      <td>-0.008217</td>\n",
       "      <td>-0.001017</td>\n",
       "      <td>-0.008091</td>\n",
       "      <td>-0.008260</td>\n",
       "      <td>-0.008133</td>\n",
       "      <td>-0.008032</td>\n",
       "      <td>-0.008104</td>\n",
       "      <td>-0.008078</td>\n",
       "      <td>-0.008006</td>\n",
       "      <td>-0.007955</td>\n",
       "      <td>-0.008257</td>\n",
       "      <td>-0.008243</td>\n",
       "      <td>-0.008151</td>\n",
       "      <td>-0.001017</td>\n",
       "      <td>-0.006947</td>\n",
       "      <td>-0.008172</td>\n",
       "      <td>-0.000981</td>\n",
       "      <td>-0.001011</td>\n",
       "      <td>-0.008366</td>\n",
       "      <td>-0.008172</td>\n",
       "      <td>-0.007999</td>\n",
       "      <td>-0.008208</td>\n",
       "      <td>-0.007508</td>\n",
       "      <td>-0.000994</td>\n",
       "      <td>-0.008249</td>\n",
       "      <td>-0.008284</td>\n",
       "      <td>-0.008084</td>\n",
       "      <td>-0.008098</td>\n",
       "      <td>-0.008195</td>\n",
       "      <td>-0.001004</td>\n",
       "      <td>-0.001002</td>\n",
       "      <td>-0.008111</td>\n",
       "      <td>-0.008248</td>\n",
       "      <td>-0.008128</td>\n",
       "      <td>-0.008041</td>\n",
       "      <td>-0.000998</td>\n",
       "      <td>-0.008201</td>\n",
       "      <td>-0.001013</td>\n",
       "      <td>-0.001005</td>\n",
       "      <td>-0.008035</td>\n",
       "      <td>-0.001014</td>\n",
       "      <td>-0.001010</td>\n",
       "      <td>-0.008188</td>\n",
       "      <td>-0.001013</td>\n",
       "      <td>-0.008101</td>\n",
       "      <td>-0.008413</td>\n",
       "      <td>-0.000999</td>\n",
       "      <td>-0.008102</td>\n",
       "      <td>-0.001002</td>\n",
       "      <td>-0.008231</td>\n",
       "      <td>-0.001041</td>\n",
       "      <td>-0.008105</td>\n",
       "      <td>-0.001022</td>\n",
       "      <td>-0.008257</td>\n",
       "      <td>-0.008170</td>\n",
       "      <td>-0.008189</td>\n",
       "      <td>-0.008113</td>\n",
       "      <td>-0.001002</td>\n",
       "      <td>-0.008138</td>\n",
       "      <td>-0.001005</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>-0.007258</td>\n",
       "      <td>-0.008307</td>\n",
       "      <td>-0.008001</td>\n",
       "      <td>-0.007943</td>\n",
       "      <td>-0.001016</td>\n",
       "      <td>-0.008203</td>\n",
       "      <td>-0.007853</td>\n",
       "      <td>-0.000994</td>\n",
       "      <td>-0.008218</td>\n",
       "      <td>-0.001002</td>\n",
       "      <td>-0.008216</td>\n",
       "      <td>-0.008102</td>\n",
       "      <td>-0.008215</td>\n",
       "      <td>-0.008218</td>\n",
       "      <td>-0.008080</td>\n",
       "      <td>-0.008210</td>\n",
       "      <td>-0.008159</td>\n",
       "      <td>-0.007945</td>\n",
       "      <td>-0.001009</td>\n",
       "      <td>-0.008230</td>\n",
       "      <td>-0.001011</td>\n",
       "      <td>-0.008015</td>\n",
       "      <td>-0.008193</td>\n",
       "      <td>-0.008233</td>\n",
       "      <td>-0.008254</td>\n",
       "      <td>-0.001016</td>\n",
       "      <td>-0.008095</td>\n",
       "      <td>-0.008189</td>\n",
       "      <td>-0.001013</td>\n",
       "      <td>-0.008356</td>\n",
       "      <td>-0.008185</td>\n",
       "      <td>-0.008105</td>\n",
       "      <td>-0.008275</td>\n",
       "      <td>-0.001011</td>\n",
       "      <td>-0.001016</td>\n",
       "      <td>-0.008271</td>\n",
       "      <td>-0.008072</td>\n",
       "      <td>-0.008049</td>\n",
       "      <td>-0.008160</td>\n",
       "      <td>-0.008088</td>\n",
       "      <td>-0.001004</td>\n",
       "      <td>-0.000994</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818282</th>\n",
       "      <td>13651601</td>\n",
       "      <td>39999230</td>\n",
       "      <td>22584645</td>\n",
       "      <td>-0.004451</td>\n",
       "      <td>0.628115</td>\n",
       "      <td>0.640276</td>\n",
       "      <td>-0.004469</td>\n",
       "      <td>-0.004445</td>\n",
       "      <td>0.613780</td>\n",
       "      <td>0.638984</td>\n",
       "      <td>0.610086</td>\n",
       "      <td>0.644864</td>\n",
       "      <td>-0.004456</td>\n",
       "      <td>-0.004248</td>\n",
       "      <td>0.635045</td>\n",
       "      <td>0.604346</td>\n",
       "      <td>-0.004580</td>\n",
       "      <td>0.643793</td>\n",
       "      <td>0.628418</td>\n",
       "      <td>0.637311</td>\n",
       "      <td>0.627658</td>\n",
       "      <td>-0.004393</td>\n",
       "      <td>0.642199</td>\n",
       "      <td>0.635978</td>\n",
       "      <td>0.634470</td>\n",
       "      <td>-0.004414</td>\n",
       "      <td>0.640854</td>\n",
       "      <td>0.605101</td>\n",
       "      <td>0.632492</td>\n",
       "      <td>0.638145</td>\n",
       "      <td>0.600562</td>\n",
       "      <td>0.630259</td>\n",
       "      <td>-0.004429</td>\n",
       "      <td>0.633191</td>\n",
       "      <td>0.643026</td>\n",
       "      <td>0.632824</td>\n",
       "      <td>0.646970</td>\n",
       "      <td>0.603738</td>\n",
       "      <td>0.622274</td>\n",
       "      <td>0.629036</td>\n",
       "      <td>-0.004297</td>\n",
       "      <td>0.624684</td>\n",
       "      <td>-0.004405</td>\n",
       "      <td>-0.004386</td>\n",
       "      <td>0.632126</td>\n",
       "      <td>0.613561</td>\n",
       "      <td>0.646033</td>\n",
       "      <td>0.647766</td>\n",
       "      <td>0.620910</td>\n",
       "      <td>0.638679</td>\n",
       "      <td>0.634528</td>\n",
       "      <td>0.638768</td>\n",
       "      <td>0.634471</td>\n",
       "      <td>0.636383</td>\n",
       "      <td>-0.004418</td>\n",
       "      <td>-0.004299</td>\n",
       "      <td>-0.004354</td>\n",
       "      <td>0.636894</td>\n",
       "      <td>0.635798</td>\n",
       "      <td>-4.366832e-03</td>\n",
       "      <td>0.630947</td>\n",
       "      <td>0.623783</td>\n",
       "      <td>0.633022</td>\n",
       "      <td>0.626138</td>\n",
       "      <td>0.640988</td>\n",
       "      <td>0.630153</td>\n",
       "      <td>0.630044</td>\n",
       "      <td>0.588631</td>\n",
       "      <td>0.632741</td>\n",
       "      <td>0.645851</td>\n",
       "      <td>0.620243</td>\n",
       "      <td>0.639835</td>\n",
       "      <td>0.629482</td>\n",
       "      <td>0.634154</td>\n",
       "      <td>0.638605</td>\n",
       "      <td>0.638037</td>\n",
       "      <td>-0.004445</td>\n",
       "      <td>-0.004457</td>\n",
       "      <td>-0.004352</td>\n",
       "      <td>0.642375</td>\n",
       "      <td>0.639228</td>\n",
       "      <td>0.630375</td>\n",
       "      <td>-0.004286</td>\n",
       "      <td>0.634462</td>\n",
       "      <td>0.640336</td>\n",
       "      <td>0.631461</td>\n",
       "      <td>0.628850</td>\n",
       "      <td>0.633872</td>\n",
       "      <td>0.634180</td>\n",
       "      <td>0.627775</td>\n",
       "      <td>0.628867</td>\n",
       "      <td>0.646236</td>\n",
       "      <td>0.642502</td>\n",
       "      <td>0.638432</td>\n",
       "      <td>-0.004673</td>\n",
       "      <td>0.615510</td>\n",
       "      <td>0.637732</td>\n",
       "      <td>-0.004425</td>\n",
       "      <td>-0.004459</td>\n",
       "      <td>0.647280</td>\n",
       "      <td>0.635196</td>\n",
       "      <td>0.619968</td>\n",
       "      <td>0.635305</td>\n",
       "      <td>0.609155</td>\n",
       "      <td>-0.004635</td>\n",
       "      <td>0.641852</td>\n",
       "      <td>0.653162</td>\n",
       "      <td>0.631987</td>\n",
       "      <td>0.632678</td>\n",
       "      <td>0.643281</td>\n",
       "      <td>-0.004247</td>\n",
       "      <td>-0.004434</td>\n",
       "      <td>0.637444</td>\n",
       "      <td>0.636687</td>\n",
       "      <td>0.632359</td>\n",
       "      <td>0.623401</td>\n",
       "      <td>-0.004305</td>\n",
       "      <td>0.641467</td>\n",
       "      <td>-0.004361</td>\n",
       "      <td>-0.004600</td>\n",
       "      <td>0.627626</td>\n",
       "      <td>-0.004601</td>\n",
       "      <td>-0.004169</td>\n",
       "      <td>0.633103</td>\n",
       "      <td>-0.004218</td>\n",
       "      <td>0.630970</td>\n",
       "      <td>0.648643</td>\n",
       "      <td>-0.004501</td>\n",
       "      <td>0.624420</td>\n",
       "      <td>-0.004588</td>\n",
       "      <td>0.641712</td>\n",
       "      <td>-0.004508</td>\n",
       "      <td>0.631868</td>\n",
       "      <td>-0.004497</td>\n",
       "      <td>0.648118</td>\n",
       "      <td>0.632395</td>\n",
       "      <td>0.637571</td>\n",
       "      <td>0.635983</td>\n",
       "      <td>-0.004636</td>\n",
       "      <td>0.636064</td>\n",
       "      <td>-0.004412</td>\n",
       "      <td>-0.004309</td>\n",
       "      <td>0.613374</td>\n",
       "      <td>0.642822</td>\n",
       "      <td>0.630843</td>\n",
       "      <td>0.615015</td>\n",
       "      <td>-0.004310</td>\n",
       "      <td>0.640584</td>\n",
       "      <td>0.633190</td>\n",
       "      <td>-0.004439</td>\n",
       "      <td>0.642474</td>\n",
       "      <td>-0.004415</td>\n",
       "      <td>0.638843</td>\n",
       "      <td>0.634262</td>\n",
       "      <td>0.635276</td>\n",
       "      <td>0.638636</td>\n",
       "      <td>0.631835</td>\n",
       "      <td>0.644080</td>\n",
       "      <td>0.638551</td>\n",
       "      <td>0.619666</td>\n",
       "      <td>-0.004269</td>\n",
       "      <td>0.641941</td>\n",
       "      <td>-0.004624</td>\n",
       "      <td>0.629596</td>\n",
       "      <td>0.637684</td>\n",
       "      <td>0.642160</td>\n",
       "      <td>0.640476</td>\n",
       "      <td>-0.004441</td>\n",
       "      <td>0.630963</td>\n",
       "      <td>0.635360</td>\n",
       "      <td>-0.004538</td>\n",
       "      <td>0.653489</td>\n",
       "      <td>0.637526</td>\n",
       "      <td>0.634881</td>\n",
       "      <td>0.641884</td>\n",
       "      <td>-0.004483</td>\n",
       "      <td>-0.004465</td>\n",
       "      <td>0.646213</td>\n",
       "      <td>0.632926</td>\n",
       "      <td>0.629428</td>\n",
       "      <td>0.638639</td>\n",
       "      <td>0.589754</td>\n",
       "      <td>-0.004335</td>\n",
       "      <td>-0.004362</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818283</th>\n",
       "      <td>13651601</td>\n",
       "      <td>39999230</td>\n",
       "      <td>22584645</td>\n",
       "      <td>-0.002580</td>\n",
       "      <td>0.390234</td>\n",
       "      <td>0.407091</td>\n",
       "      <td>-0.002581</td>\n",
       "      <td>-0.002497</td>\n",
       "      <td>0.393172</td>\n",
       "      <td>0.395674</td>\n",
       "      <td>0.372623</td>\n",
       "      <td>0.397213</td>\n",
       "      <td>-0.002574</td>\n",
       "      <td>-0.002446</td>\n",
       "      <td>0.397202</td>\n",
       "      <td>0.384369</td>\n",
       "      <td>-0.002708</td>\n",
       "      <td>0.394732</td>\n",
       "      <td>0.385647</td>\n",
       "      <td>0.395470</td>\n",
       "      <td>0.384530</td>\n",
       "      <td>-0.002598</td>\n",
       "      <td>0.394285</td>\n",
       "      <td>0.405601</td>\n",
       "      <td>0.393887</td>\n",
       "      <td>-0.002574</td>\n",
       "      <td>0.397027</td>\n",
       "      <td>0.372450</td>\n",
       "      <td>0.393519</td>\n",
       "      <td>0.397055</td>\n",
       "      <td>0.385307</td>\n",
       "      <td>0.392029</td>\n",
       "      <td>-0.002556</td>\n",
       "      <td>0.390711</td>\n",
       "      <td>0.395694</td>\n",
       "      <td>0.390901</td>\n",
       "      <td>0.397042</td>\n",
       "      <td>0.389058</td>\n",
       "      <td>0.401308</td>\n",
       "      <td>0.393894</td>\n",
       "      <td>-0.002476</td>\n",
       "      <td>0.386602</td>\n",
       "      <td>-0.002540</td>\n",
       "      <td>-0.002515</td>\n",
       "      <td>0.388986</td>\n",
       "      <td>0.375801</td>\n",
       "      <td>0.396980</td>\n",
       "      <td>0.402945</td>\n",
       "      <td>0.388552</td>\n",
       "      <td>0.396542</td>\n",
       "      <td>0.406457</td>\n",
       "      <td>0.400064</td>\n",
       "      <td>0.389095</td>\n",
       "      <td>0.390280</td>\n",
       "      <td>-0.002574</td>\n",
       "      <td>-0.002404</td>\n",
       "      <td>-0.002467</td>\n",
       "      <td>0.391506</td>\n",
       "      <td>0.392246</td>\n",
       "      <td>-2.502233e-03</td>\n",
       "      <td>0.392797</td>\n",
       "      <td>0.384954</td>\n",
       "      <td>0.391134</td>\n",
       "      <td>0.397446</td>\n",
       "      <td>0.398440</td>\n",
       "      <td>0.405233</td>\n",
       "      <td>0.406519</td>\n",
       "      <td>0.379392</td>\n",
       "      <td>0.391406</td>\n",
       "      <td>0.400474</td>\n",
       "      <td>0.387892</td>\n",
       "      <td>0.398253</td>\n",
       "      <td>0.397103</td>\n",
       "      <td>0.396083</td>\n",
       "      <td>0.393390</td>\n",
       "      <td>0.395145</td>\n",
       "      <td>-0.002589</td>\n",
       "      <td>-0.002552</td>\n",
       "      <td>-0.002502</td>\n",
       "      <td>0.395366</td>\n",
       "      <td>0.407419</td>\n",
       "      <td>0.392243</td>\n",
       "      <td>-0.002445</td>\n",
       "      <td>0.400946</td>\n",
       "      <td>0.391099</td>\n",
       "      <td>0.388780</td>\n",
       "      <td>0.383642</td>\n",
       "      <td>0.386484</td>\n",
       "      <td>0.396348</td>\n",
       "      <td>0.392160</td>\n",
       "      <td>0.397522</td>\n",
       "      <td>0.409125</td>\n",
       "      <td>0.395895</td>\n",
       "      <td>0.391692</td>\n",
       "      <td>-0.002781</td>\n",
       "      <td>0.387466</td>\n",
       "      <td>0.395038</td>\n",
       "      <td>-0.002588</td>\n",
       "      <td>-0.002540</td>\n",
       "      <td>0.404068</td>\n",
       "      <td>0.397030</td>\n",
       "      <td>0.384124</td>\n",
       "      <td>0.391730</td>\n",
       "      <td>0.387492</td>\n",
       "      <td>-0.002723</td>\n",
       "      <td>0.408402</td>\n",
       "      <td>0.419339</td>\n",
       "      <td>0.391598</td>\n",
       "      <td>0.402969</td>\n",
       "      <td>0.404803</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>-0.002518</td>\n",
       "      <td>0.397161</td>\n",
       "      <td>0.393365</td>\n",
       "      <td>0.392074</td>\n",
       "      <td>0.391494</td>\n",
       "      <td>-0.002426</td>\n",
       "      <td>0.404081</td>\n",
       "      <td>-0.002412</td>\n",
       "      <td>-0.002726</td>\n",
       "      <td>0.400338</td>\n",
       "      <td>-0.002705</td>\n",
       "      <td>-0.002282</td>\n",
       "      <td>0.394175</td>\n",
       "      <td>-0.002330</td>\n",
       "      <td>0.392265</td>\n",
       "      <td>0.405943</td>\n",
       "      <td>-0.002630</td>\n",
       "      <td>0.395761</td>\n",
       "      <td>-0.002709</td>\n",
       "      <td>0.399803</td>\n",
       "      <td>-0.002660</td>\n",
       "      <td>0.386705</td>\n",
       "      <td>-0.002600</td>\n",
       "      <td>0.415050</td>\n",
       "      <td>0.402829</td>\n",
       "      <td>0.393255</td>\n",
       "      <td>0.401552</td>\n",
       "      <td>-0.002709</td>\n",
       "      <td>0.403778</td>\n",
       "      <td>-0.002567</td>\n",
       "      <td>-0.002437</td>\n",
       "      <td>0.375301</td>\n",
       "      <td>0.399507</td>\n",
       "      <td>0.400996</td>\n",
       "      <td>0.392912</td>\n",
       "      <td>-0.002459</td>\n",
       "      <td>0.393743</td>\n",
       "      <td>0.396658</td>\n",
       "      <td>-0.002547</td>\n",
       "      <td>0.393421</td>\n",
       "      <td>-0.002526</td>\n",
       "      <td>0.401790</td>\n",
       "      <td>0.393314</td>\n",
       "      <td>0.394298</td>\n",
       "      <td>0.395555</td>\n",
       "      <td>0.390857</td>\n",
       "      <td>0.399255</td>\n",
       "      <td>0.393268</td>\n",
       "      <td>0.380970</td>\n",
       "      <td>-0.002402</td>\n",
       "      <td>0.395566</td>\n",
       "      <td>-0.002733</td>\n",
       "      <td>0.385286</td>\n",
       "      <td>0.399189</td>\n",
       "      <td>0.395053</td>\n",
       "      <td>0.401833</td>\n",
       "      <td>-0.002580</td>\n",
       "      <td>0.389914</td>\n",
       "      <td>0.391775</td>\n",
       "      <td>-0.002642</td>\n",
       "      <td>0.417125</td>\n",
       "      <td>0.403281</td>\n",
       "      <td>0.387903</td>\n",
       "      <td>0.401257</td>\n",
       "      <td>-0.002640</td>\n",
       "      <td>-0.002559</td>\n",
       "      <td>0.408135</td>\n",
       "      <td>0.389162</td>\n",
       "      <td>0.390465</td>\n",
       "      <td>0.390602</td>\n",
       "      <td>0.382147</td>\n",
       "      <td>-0.002467</td>\n",
       "      <td>-0.002478</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818284</th>\n",
       "      <td>13651601</td>\n",
       "      <td>39999230</td>\n",
       "      <td>22584645</td>\n",
       "      <td>-0.010832</td>\n",
       "      <td>1.382040</td>\n",
       "      <td>1.372222</td>\n",
       "      <td>-0.010753</td>\n",
       "      <td>-0.010822</td>\n",
       "      <td>1.308468</td>\n",
       "      <td>1.409126</td>\n",
       "      <td>1.369543</td>\n",
       "      <td>1.430043</td>\n",
       "      <td>-0.010819</td>\n",
       "      <td>-0.010298</td>\n",
       "      <td>1.384794</td>\n",
       "      <td>1.299459</td>\n",
       "      <td>-0.010792</td>\n",
       "      <td>1.433670</td>\n",
       "      <td>1.399647</td>\n",
       "      <td>1.401669</td>\n",
       "      <td>1.402684</td>\n",
       "      <td>-0.010530</td>\n",
       "      <td>1.431780</td>\n",
       "      <td>1.360702</td>\n",
       "      <td>1.397005</td>\n",
       "      <td>-0.010683</td>\n",
       "      <td>1.412396</td>\n",
       "      <td>1.347447</td>\n",
       "      <td>1.390638</td>\n",
       "      <td>1.401972</td>\n",
       "      <td>1.278837</td>\n",
       "      <td>1.384331</td>\n",
       "      <td>-0.010694</td>\n",
       "      <td>1.400414</td>\n",
       "      <td>1.428755</td>\n",
       "      <td>1.401862</td>\n",
       "      <td>1.438151</td>\n",
       "      <td>1.280347</td>\n",
       "      <td>1.315399</td>\n",
       "      <td>1.369232</td>\n",
       "      <td>-0.010544</td>\n",
       "      <td>1.378401</td>\n",
       "      <td>-0.010641</td>\n",
       "      <td>-0.010717</td>\n",
       "      <td>1.398584</td>\n",
       "      <td>1.371775</td>\n",
       "      <td>1.436650</td>\n",
       "      <td>1.425716</td>\n",
       "      <td>1.356350</td>\n",
       "      <td>1.404368</td>\n",
       "      <td>1.353413</td>\n",
       "      <td>1.396149</td>\n",
       "      <td>1.415118</td>\n",
       "      <td>1.417322</td>\n",
       "      <td>-0.010632</td>\n",
       "      <td>-0.010649</td>\n",
       "      <td>-0.010687</td>\n",
       "      <td>1.413270</td>\n",
       "      <td>1.406840</td>\n",
       "      <td>-1.054418e-02</td>\n",
       "      <td>1.382941</td>\n",
       "      <td>1.381486</td>\n",
       "      <td>1.401012</td>\n",
       "      <td>1.347619</td>\n",
       "      <td>1.406630</td>\n",
       "      <td>1.343021</td>\n",
       "      <td>1.332622</td>\n",
       "      <td>1.260756</td>\n",
       "      <td>1.395176</td>\n",
       "      <td>1.422775</td>\n",
       "      <td>1.351057</td>\n",
       "      <td>1.405113</td>\n",
       "      <td>1.370170</td>\n",
       "      <td>1.389989</td>\n",
       "      <td>1.417861</td>\n",
       "      <td>1.405920</td>\n",
       "      <td>-0.010772</td>\n",
       "      <td>-0.010824</td>\n",
       "      <td>-0.010511</td>\n",
       "      <td>1.425169</td>\n",
       "      <td>1.370955</td>\n",
       "      <td>1.382901</td>\n",
       "      <td>-0.010469</td>\n",
       "      <td>1.370605</td>\n",
       "      <td>1.432862</td>\n",
       "      <td>1.403038</td>\n",
       "      <td>1.406170</td>\n",
       "      <td>1.418153</td>\n",
       "      <td>1.384516</td>\n",
       "      <td>1.370410</td>\n",
       "      <td>1.356875</td>\n",
       "      <td>1.393057</td>\n",
       "      <td>1.423880</td>\n",
       "      <td>1.422841</td>\n",
       "      <td>-0.010923</td>\n",
       "      <td>1.340130</td>\n",
       "      <td>1.407518</td>\n",
       "      <td>-0.010647</td>\n",
       "      <td>-0.010848</td>\n",
       "      <td>1.415625</td>\n",
       "      <td>1.384928</td>\n",
       "      <td>1.361810</td>\n",
       "      <td>1.408357</td>\n",
       "      <td>1.312808</td>\n",
       "      <td>-0.010927</td>\n",
       "      <td>1.376140</td>\n",
       "      <td>1.386961</td>\n",
       "      <td>1.389611</td>\n",
       "      <td>1.358277</td>\n",
       "      <td>1.394958</td>\n",
       "      <td>-0.010485</td>\n",
       "      <td>-0.010771</td>\n",
       "      <td>1.395968</td>\n",
       "      <td>1.404494</td>\n",
       "      <td>1.389542</td>\n",
       "      <td>1.357006</td>\n",
       "      <td>-0.010673</td>\n",
       "      <td>1.392574</td>\n",
       "      <td>-0.010791</td>\n",
       "      <td>-0.010879</td>\n",
       "      <td>1.347336</td>\n",
       "      <td>-0.010854</td>\n",
       "      <td>-0.010495</td>\n",
       "      <td>1.387943</td>\n",
       "      <td>-0.010606</td>\n",
       "      <td>1.385787</td>\n",
       "      <td>1.418129</td>\n",
       "      <td>-0.010708</td>\n",
       "      <td>1.346678</td>\n",
       "      <td>-0.010884</td>\n",
       "      <td>1.404622</td>\n",
       "      <td>-0.010759</td>\n",
       "      <td>1.410383</td>\n",
       "      <td>-0.010728</td>\n",
       "      <td>1.387479</td>\n",
       "      <td>1.358562</td>\n",
       "      <td>1.410548</td>\n",
       "      <td>1.374404</td>\n",
       "      <td>-0.011005</td>\n",
       "      <td>1.366315</td>\n",
       "      <td>-0.010608</td>\n",
       "      <td>-0.010502</td>\n",
       "      <td>1.369838</td>\n",
       "      <td>1.413518</td>\n",
       "      <td>1.353526</td>\n",
       "      <td>1.316630</td>\n",
       "      <td>-0.010591</td>\n",
       "      <td>1.423649</td>\n",
       "      <td>1.380776</td>\n",
       "      <td>-0.010762</td>\n",
       "      <td>1.433795</td>\n",
       "      <td>-0.010751</td>\n",
       "      <td>1.387890</td>\n",
       "      <td>1.399995</td>\n",
       "      <td>1.399276</td>\n",
       "      <td>1.408812</td>\n",
       "      <td>1.395627</td>\n",
       "      <td>1.420946</td>\n",
       "      <td>1.416293</td>\n",
       "      <td>1.379226</td>\n",
       "      <td>-0.010517</td>\n",
       "      <td>1.423559</td>\n",
       "      <td>-0.010840</td>\n",
       "      <td>1.404303</td>\n",
       "      <td>1.392876</td>\n",
       "      <td>1.427172</td>\n",
       "      <td>1.395902</td>\n",
       "      <td>-0.010727</td>\n",
       "      <td>1.392724</td>\n",
       "      <td>1.410411</td>\n",
       "      <td>-0.010946</td>\n",
       "      <td>1.394731</td>\n",
       "      <td>1.377959</td>\n",
       "      <td>1.417538</td>\n",
       "      <td>1.402054</td>\n",
       "      <td>-0.010755</td>\n",
       "      <td>-0.010799</td>\n",
       "      <td>1.399653</td>\n",
       "      <td>1.408260</td>\n",
       "      <td>1.388709</td>\n",
       "      <td>1.428628</td>\n",
       "      <td>1.243172</td>\n",
       "      <td>-0.010558</td>\n",
       "      <td>-0.010644</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818285</th>\n",
       "      <td>13651601</td>\n",
       "      <td>39999230</td>\n",
       "      <td>22584645</td>\n",
       "      <td>-0.003516</td>\n",
       "      <td>0.522297</td>\n",
       "      <td>0.537427</td>\n",
       "      <td>-0.003571</td>\n",
       "      <td>-0.003489</td>\n",
       "      <td>0.516191</td>\n",
       "      <td>0.530377</td>\n",
       "      <td>0.503185</td>\n",
       "      <td>0.534796</td>\n",
       "      <td>-0.003535</td>\n",
       "      <td>-0.003346</td>\n",
       "      <td>0.529668</td>\n",
       "      <td>0.506450</td>\n",
       "      <td>-0.003678</td>\n",
       "      <td>0.532552</td>\n",
       "      <td>0.519918</td>\n",
       "      <td>0.529654</td>\n",
       "      <td>0.519125</td>\n",
       "      <td>-0.003517</td>\n",
       "      <td>0.531084</td>\n",
       "      <td>0.534269</td>\n",
       "      <td>0.527613</td>\n",
       "      <td>-0.003547</td>\n",
       "      <td>0.532688</td>\n",
       "      <td>0.500653</td>\n",
       "      <td>0.526455</td>\n",
       "      <td>0.531135</td>\n",
       "      <td>0.505078</td>\n",
       "      <td>0.524480</td>\n",
       "      <td>-0.003524</td>\n",
       "      <td>0.525125</td>\n",
       "      <td>0.532700</td>\n",
       "      <td>0.525264</td>\n",
       "      <td>0.535253</td>\n",
       "      <td>0.509034</td>\n",
       "      <td>0.525166</td>\n",
       "      <td>0.525136</td>\n",
       "      <td>-0.003378</td>\n",
       "      <td>0.518966</td>\n",
       "      <td>-0.003481</td>\n",
       "      <td>-0.003442</td>\n",
       "      <td>0.523559</td>\n",
       "      <td>0.506652</td>\n",
       "      <td>0.534933</td>\n",
       "      <td>0.538443</td>\n",
       "      <td>0.517655</td>\n",
       "      <td>0.531009</td>\n",
       "      <td>0.533988</td>\n",
       "      <td>0.532888</td>\n",
       "      <td>0.524719</td>\n",
       "      <td>0.526432</td>\n",
       "      <td>-0.003493</td>\n",
       "      <td>-0.003369</td>\n",
       "      <td>-0.003436</td>\n",
       "      <td>0.527831</td>\n",
       "      <td>0.527329</td>\n",
       "      <td>-3.441146e-03</td>\n",
       "      <td>0.525093</td>\n",
       "      <td>0.517254</td>\n",
       "      <td>0.524896</td>\n",
       "      <td>0.524691</td>\n",
       "      <td>0.533566</td>\n",
       "      <td>0.529931</td>\n",
       "      <td>0.531860</td>\n",
       "      <td>0.495325</td>\n",
       "      <td>0.525488</td>\n",
       "      <td>0.536460</td>\n",
       "      <td>0.517836</td>\n",
       "      <td>0.532676</td>\n",
       "      <td>0.525178</td>\n",
       "      <td>0.528448</td>\n",
       "      <td>0.529317</td>\n",
       "      <td>0.530402</td>\n",
       "      <td>-0.003516</td>\n",
       "      <td>-0.003508</td>\n",
       "      <td>-0.003435</td>\n",
       "      <td>0.531853</td>\n",
       "      <td>0.537065</td>\n",
       "      <td>0.525289</td>\n",
       "      <td>-0.003403</td>\n",
       "      <td>0.530972</td>\n",
       "      <td>0.528852</td>\n",
       "      <td>0.523096</td>\n",
       "      <td>0.519875</td>\n",
       "      <td>0.523234</td>\n",
       "      <td>0.528933</td>\n",
       "      <td>0.523256</td>\n",
       "      <td>0.526756</td>\n",
       "      <td>0.541459</td>\n",
       "      <td>0.532663</td>\n",
       "      <td>0.528343</td>\n",
       "      <td>-0.003747</td>\n",
       "      <td>0.512422</td>\n",
       "      <td>0.529503</td>\n",
       "      <td>-0.003508</td>\n",
       "      <td>-0.003522</td>\n",
       "      <td>0.539287</td>\n",
       "      <td>0.529457</td>\n",
       "      <td>0.515743</td>\n",
       "      <td>0.526503</td>\n",
       "      <td>0.510546</td>\n",
       "      <td>-0.003711</td>\n",
       "      <td>0.538701</td>\n",
       "      <td>0.550489</td>\n",
       "      <td>0.525408</td>\n",
       "      <td>0.531404</td>\n",
       "      <td>0.537914</td>\n",
       "      <td>-0.003333</td>\n",
       "      <td>-0.003509</td>\n",
       "      <td>0.530977</td>\n",
       "      <td>0.528841</td>\n",
       "      <td>0.525733</td>\n",
       "      <td>0.520931</td>\n",
       "      <td>-0.003368</td>\n",
       "      <td>0.536321</td>\n",
       "      <td>-0.003388</td>\n",
       "      <td>-0.003672</td>\n",
       "      <td>0.527359</td>\n",
       "      <td>-0.003668</td>\n",
       "      <td>-0.003224</td>\n",
       "      <td>0.527197</td>\n",
       "      <td>-0.003260</td>\n",
       "      <td>0.524869</td>\n",
       "      <td>0.540974</td>\n",
       "      <td>-0.003599</td>\n",
       "      <td>0.522880</td>\n",
       "      <td>-0.003694</td>\n",
       "      <td>0.534510</td>\n",
       "      <td>-0.003600</td>\n",
       "      <td>0.522167</td>\n",
       "      <td>-0.003566</td>\n",
       "      <td>0.544874</td>\n",
       "      <td>0.530880</td>\n",
       "      <td>0.528539</td>\n",
       "      <td>0.532221</td>\n",
       "      <td>-0.003712</td>\n",
       "      <td>0.533328</td>\n",
       "      <td>-0.003508</td>\n",
       "      <td>-0.003399</td>\n",
       "      <td>0.506364</td>\n",
       "      <td>0.534572</td>\n",
       "      <td>0.529600</td>\n",
       "      <td>0.517158</td>\n",
       "      <td>-0.003378</td>\n",
       "      <td>0.530449</td>\n",
       "      <td>0.528088</td>\n",
       "      <td>-0.003489</td>\n",
       "      <td>0.530805</td>\n",
       "      <td>-0.003508</td>\n",
       "      <td>0.533899</td>\n",
       "      <td>0.527201</td>\n",
       "      <td>0.528181</td>\n",
       "      <td>0.530356</td>\n",
       "      <td>0.523989</td>\n",
       "      <td>0.534736</td>\n",
       "      <td>0.529139</td>\n",
       "      <td>0.513151</td>\n",
       "      <td>-0.003343</td>\n",
       "      <td>0.531859</td>\n",
       "      <td>-0.003687</td>\n",
       "      <td>0.520404</td>\n",
       "      <td>0.532019</td>\n",
       "      <td>0.531966</td>\n",
       "      <td>0.534673</td>\n",
       "      <td>-0.003532</td>\n",
       "      <td>0.523860</td>\n",
       "      <td>0.526491</td>\n",
       "      <td>-0.003599</td>\n",
       "      <td>0.549655</td>\n",
       "      <td>0.533943</td>\n",
       "      <td>0.524333</td>\n",
       "      <td>0.535220</td>\n",
       "      <td>-0.003563</td>\n",
       "      <td>-0.003529</td>\n",
       "      <td>0.540078</td>\n",
       "      <td>0.523454</td>\n",
       "      <td>0.522642</td>\n",
       "      <td>0.527193</td>\n",
       "      <td>0.498276</td>\n",
       "      <td>-0.003413</td>\n",
       "      <td>-0.003431</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>818286 rows × 185 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        subject_id   stay_id   hadm_id         3         4         5  \\\n",
       "0         15332791  30003598  20683754 -0.001014 -0.009162 -0.009464   \n",
       "1         15332791  30003598  20683754 -0.001003 -0.007288 -0.007451   \n",
       "2         15332791  30003598  20683754 -0.000995 -0.002681 -0.002496   \n",
       "3         15332791  30003598  20683754 -0.001012 -0.008751 -0.009039   \n",
       "4         15332791  30003598  20683754 -0.000082  0.648427  0.643528   \n",
       "...            ...       ...       ...       ...       ...       ...   \n",
       "818281    13651601  39999230  22584645 -0.001007 -0.007982 -0.008218   \n",
       "818282    13651601  39999230  22584645 -0.004451  0.628115  0.640276   \n",
       "818283    13651601  39999230  22584645 -0.002580  0.390234  0.407091   \n",
       "818284    13651601  39999230  22584645 -0.010832  1.382040  1.372222   \n",
       "818285    13651601  39999230  22584645 -0.003516  0.522297  0.537427   \n",
       "\n",
       "               6         7         8         9        10        11        12  \\\n",
       "0      -0.001002 -0.000988 -0.009106 -0.009416 -0.009061 -0.009510 -0.000999   \n",
       "1      -0.000990 -0.000978 -0.007154 -0.007484 -0.007209 -0.007576 -0.000989   \n",
       "2      -0.000977 -0.000968 -0.002344 -0.002719 -0.002637 -0.002803 -0.000981   \n",
       "3      -0.001000 -0.000986 -0.008684 -0.009004 -0.008671 -0.009101 -0.000998   \n",
       "4       0.008855  0.011781  0.620801  0.656128  0.631469  0.665730 -0.000060   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "818281 -0.000995 -0.000982 -0.007906 -0.008217 -0.007908 -0.008304 -0.000993   \n",
       "818282 -0.004469 -0.004445  0.613780  0.638984  0.610086  0.644864 -0.004456   \n",
       "818283 -0.002581 -0.002497  0.393172  0.395674  0.372623  0.397213 -0.002574   \n",
       "818284 -0.010753 -0.010822  1.308468  1.409126  1.369543  1.430043 -0.010819   \n",
       "818285 -0.003571 -0.003489  0.516191  0.530377  0.503185  0.534796 -0.003535   \n",
       "\n",
       "              13        14        15        16        17        18        19  \\\n",
       "0      -0.001019 -0.009394 -0.009065 -0.000993 -0.009465 -0.009292 -0.009414   \n",
       "1      -0.001007 -0.007447 -0.007117 -0.000982 -0.007538 -0.007402 -0.007486   \n",
       "2      -0.000997 -0.002664 -0.002325 -0.000974 -0.002783 -0.002726 -0.002720   \n",
       "3      -0.001017 -0.008980 -0.008639 -0.000991 -0.009054 -0.008890 -0.009001   \n",
       "4      -0.000176  0.653565  0.608773 -0.000123  0.663517  0.645223  0.650598   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "818281 -0.001012 -0.008191 -0.007861 -0.000986 -0.008262 -0.008112 -0.008205   \n",
       "818282 -0.004248  0.635045  0.604346 -0.004580  0.643793  0.628418  0.637311   \n",
       "818283 -0.002446  0.397202  0.384369 -0.002708  0.394732  0.385647  0.395470   \n",
       "818284 -0.010298  1.384794  1.299459 -0.010792  1.433670  1.399647  1.401669   \n",
       "818285 -0.003346  0.529668  0.506450 -0.003678  0.532552  0.519918  0.529654   \n",
       "\n",
       "              20        21        22        23        24        25        26  \\\n",
       "0      -0.009063 -0.001027 -0.009478 -0.009288 -0.009373 -0.001029 -0.009483   \n",
       "1      -0.007225 -0.001018 -0.007545 -0.007335 -0.007459 -0.001019 -0.007549   \n",
       "2      -0.002675 -0.001011 -0.002779 -0.002537 -0.002722 -0.001009 -0.002778   \n",
       "3      -0.008683 -0.001025 -0.009061 -0.008864 -0.008975 -0.001028 -0.009078   \n",
       "4       0.648566 -0.000060  0.659954  0.647487  0.651744 -0.000232  0.660096   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "818281 -0.007927 -0.001021 -0.008269 -0.008070 -0.008185 -0.001023 -0.008279   \n",
       "818282  0.627658 -0.004393  0.642199  0.635978  0.634470 -0.004414  0.640854   \n",
       "818283  0.384530 -0.002598  0.394285  0.405601  0.393887 -0.002574  0.397027   \n",
       "818284  1.402684 -0.010530  1.431780  1.360702  1.397005 -0.010683  1.412396   \n",
       "818285  0.519125 -0.003517  0.531084  0.534269  0.527613 -0.003547  0.532688   \n",
       "\n",
       "              27        28        29        30        31        32        33  \\\n",
       "0      -0.008518 -0.009234 -0.009363 -0.009054 -0.009238 -0.001008 -0.009250   \n",
       "1      -0.006784 -0.007350 -0.007437 -0.007102 -0.007348 -0.000998 -0.007353   \n",
       "2      -0.002492 -0.002711 -0.002688 -0.002309 -0.002686 -0.000991 -0.002671   \n",
       "3      -0.008179 -0.008833 -0.008957 -0.008666 -0.008840 -0.001006 -0.008845   \n",
       "4       0.620524  0.651149  0.655505  0.610383  0.653320 -0.000199  0.650320   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "818281 -0.007462 -0.008063 -0.008165 -0.007864 -0.008067 -0.001002 -0.008076   \n",
       "818282  0.605101  0.632492  0.638145  0.600562  0.630259 -0.004429  0.633191   \n",
       "818283  0.372450  0.393519  0.397055  0.385307  0.392029 -0.002556  0.390711   \n",
       "818284  1.347447  1.390638  1.401972  1.278837  1.384331 -0.010694  1.400414   \n",
       "818285  0.500653  0.526455  0.531135  0.505078  0.524480 -0.003524  0.525125   \n",
       "\n",
       "              34        35        36        37        38        39        40  \\\n",
       "0      -0.009453 -0.009230 -0.009493 -0.009371 -0.009301 -0.009320 -0.001029   \n",
       "1      -0.007526 -0.007352 -0.007554 -0.007345 -0.007303 -0.007397 -0.001018   \n",
       "2      -0.002769 -0.002730 -0.002770 -0.002354 -0.002407 -0.002653 -0.001009   \n",
       "3      -0.009037 -0.008822 -0.009082 -0.008937 -0.008857 -0.008915 -0.001028   \n",
       "4       0.663511  0.651852  0.662088  0.616023  0.624541  0.639967 -0.000038   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "818281 -0.008248 -0.008058 -0.008284 -0.008128 -0.008042 -0.008120 -0.001023   \n",
       "818282  0.643026  0.632824  0.646970  0.603738  0.622274  0.629036 -0.004297   \n",
       "818283  0.395694  0.390901  0.397042  0.389058  0.401308  0.393894 -0.002476   \n",
       "818284  1.428755  1.401862  1.438151  1.280347  1.315399  1.369232 -0.010544   \n",
       "818285  0.532700  0.525264  0.535253  0.509034  0.525166  0.525136 -0.003378   \n",
       "\n",
       "              41        42        43        44        45        46        47  \\\n",
       "0      -0.009140 -0.001023 -0.001028 -0.009337 -0.009065 -0.009471 -0.009558   \n",
       "1      -0.007278 -0.001013 -0.001017 -0.007417 -0.007215 -0.007545 -0.007584   \n",
       "2      -0.002703 -0.001005 -0.001008 -0.002692 -0.002650 -0.002789 -0.002709   \n",
       "3      -0.008741 -0.001021 -0.001026 -0.008923 -0.008674 -0.009059 -0.009144   \n",
       "4       0.649639  0.010598  0.017045  0.648658  0.631175  0.667505  0.666819   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "818281 -0.007987 -0.001017 -0.001022 -0.008142 -0.007905 -0.008270 -0.008333   \n",
       "818282  0.624684 -0.004405 -0.004386  0.632126  0.613561  0.646033  0.647766   \n",
       "818283  0.386602 -0.002540 -0.002515  0.388986  0.375801  0.396980  0.402945   \n",
       "818284  1.378401 -0.010641 -0.010717  1.398584  1.371775  1.436650  1.425716   \n",
       "818285  0.518966 -0.003481 -0.003442  0.523559  0.506652  0.534933  0.538443   \n",
       "\n",
       "              48        49        50        51        52        53        54  \\\n",
       "0      -0.009213 -0.009430 -0.009309 -0.009456 -0.009356 -0.009362 -0.001025   \n",
       "1      -0.007325 -0.007496 -0.007334 -0.007505 -0.007454 -0.007461 -0.001013   \n",
       "2      -0.002645 -0.002725 -0.002463 -0.002695 -0.002748 -0.002758 -0.001002   \n",
       "3      -0.008814 -0.009022 -0.008893 -0.009034 -0.008951 -0.008960 -0.001022   \n",
       "4       0.640381  0.657863  0.641844  0.657854  0.653839  0.656496  0.018186   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "818281 -0.008034 -0.008227 -0.008086 -0.008244 -0.008168 -0.008172 -0.001017   \n",
       "818282  0.620910  0.638679  0.634528  0.638768  0.634471  0.636383 -0.004418   \n",
       "818283  0.388552  0.396542  0.406457  0.400064  0.389095  0.390280 -0.002574   \n",
       "818284  1.356350  1.404368  1.353413  1.396149  1.415118  1.417322 -0.010632   \n",
       "818285  0.517655  0.531009  0.533988  0.532888  0.524719  0.526432 -0.003493   \n",
       "\n",
       "              55        56        57        58            59        60  \\\n",
       "0      -0.001021 -0.001035 -0.009370 -0.009389 -1.033548e-03 -0.009285   \n",
       "1      -0.001012 -0.001024 -0.007456 -0.007464 -1.024011e-03 -0.007380   \n",
       "2      -0.001003 -0.001015 -0.002722 -0.002719 -1.015734e-03 -0.002676   \n",
       "3      -0.001020 -0.001033 -0.008966 -0.008977 -1.032175e-03 -0.008871   \n",
       "4       0.023106  0.013818  0.656313  0.657319 -5.540359e-08  0.646976   \n",
       "...          ...       ...       ...       ...           ...       ...   \n",
       "818281 -0.001015 -0.001028 -0.008183 -0.008191 -1.027544e-03 -0.008096   \n",
       "818282 -0.004299 -0.004354  0.636894  0.635798 -4.366832e-03  0.630947   \n",
       "818283 -0.002404 -0.002467  0.391506  0.392246 -2.502233e-03  0.392797   \n",
       "818284 -0.010649 -0.010687  1.413270  1.406840 -1.054418e-02  1.382941   \n",
       "818285 -0.003369 -0.003436  0.527831  0.527329 -3.441146e-03  0.525093   \n",
       "\n",
       "              61        62        63        64        65        66        67  \\\n",
       "0      -0.009166 -0.009313 -0.009333 -0.009467 -0.008837 -0.009275 -0.008142   \n",
       "1      -0.007281 -0.007422 -0.007356 -0.007525 -0.006911 -0.007310 -0.006353   \n",
       "2      -0.002643 -0.002747 -0.002496 -0.002752 -0.002202 -0.002454 -0.001988   \n",
       "3      -0.008751 -0.008913 -0.008921 -0.009047 -0.008431 -0.008862 -0.007720   \n",
       "4       0.640504  0.658676  0.634443  0.652564  0.614382  0.633134  0.542302   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "818281 -0.007981 -0.008130 -0.008107 -0.008256 -0.007652 -0.008049 -0.007008   \n",
       "818282  0.623783  0.633022  0.626138  0.640988  0.630153  0.630044  0.588631   \n",
       "818283  0.384954  0.391134  0.397446  0.398440  0.405233  0.406519  0.379392   \n",
       "818284  1.381486  1.401012  1.347619  1.406630  1.343021  1.332622  1.260756   \n",
       "818285  0.517254  0.524896  0.524691  0.533566  0.529931  0.531860  0.495325   \n",
       "\n",
       "              68        69        70        71        72        73        74  \\\n",
       "0      -0.009303 -0.009602 -0.009177 -0.009367 -0.007936 -0.009302 -0.009367   \n",
       "1      -0.007400 -0.007627 -0.007290 -0.007451 -0.006227 -0.007362 -0.007451   \n",
       "2      -0.002714 -0.002742 -0.002631 -0.002725 -0.001989 -0.002600 -0.002719   \n",
       "3      -0.008888 -0.009185 -0.008781 -0.008965 -0.007570 -0.008865 -0.008960   \n",
       "4       0.648736  0.660888  0.636784  0.655600  0.560615  0.643516  0.655644   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "818281 -0.008109 -0.008374 -0.007998 -0.008177 -0.006865 -0.008080 -0.008173   \n",
       "818282  0.632741  0.645851  0.620243  0.639835  0.629482  0.634154  0.638605   \n",
       "818283  0.391406  0.400474  0.387892  0.398253  0.397103  0.396083  0.393390   \n",
       "818284  1.395176  1.422775  1.351057  1.405113  1.370170  1.389989  1.417861   \n",
       "818285  0.525488  0.536460  0.517836  0.532676  0.525178  0.528448  0.529317   \n",
       "\n",
       "              75        76        77        78        79        80        81  \\\n",
       "0      -0.009460 -0.001046 -0.001006 -0.001023 -0.009440 -0.009383 -0.009410   \n",
       "1      -0.007529 -0.001035 -0.000995 -0.001014 -0.007501 -0.007395 -0.007488   \n",
       "2      -0.002767 -0.001024 -0.000982 -0.001003 -0.002718 -0.002509 -0.002746   \n",
       "3      -0.009050 -0.001044 -0.001004 -0.001022 -0.009029 -0.008952 -0.009006   \n",
       "4       0.658351  0.000456  0.002762 -0.000009  0.653391  0.646850  0.657715   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "818281 -0.008252 -0.001039 -0.000999 -0.001017 -0.008237 -0.008153 -0.008217   \n",
       "818282  0.638037 -0.004445 -0.004457 -0.004352  0.642375  0.639228  0.630375   \n",
       "818283  0.395145 -0.002589 -0.002552 -0.002502  0.395366  0.407419  0.392243   \n",
       "818284  1.405920 -0.010772 -0.010824 -0.010511  1.425169  1.370955  1.382901   \n",
       "818285  0.530402 -0.003516 -0.003508 -0.003435  0.531853  0.537065  0.525289   \n",
       "\n",
       "              82        83        84        85        86        87        88  \\\n",
       "0      -0.001024 -0.009276 -0.009462 -0.009312 -0.009202 -0.009274 -0.009259   \n",
       "1      -0.001014 -0.007338 -0.007538 -0.007412 -0.007328 -0.007379 -0.007363   \n",
       "2      -0.001004 -0.002572 -0.002800 -0.002726 -0.002702 -0.002711 -0.002676   \n",
       "3      -0.001022 -0.008868 -0.009046 -0.008911 -0.008808 -0.008871 -0.008860   \n",
       "4       0.007887  0.646733  0.659195  0.652912  0.643206  0.647057  0.648090   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "818281 -0.001017 -0.008091 -0.008260 -0.008133 -0.008032 -0.008104 -0.008078   \n",
       "818282 -0.004286  0.634462  0.640336  0.631461  0.628850  0.633872  0.634180   \n",
       "818283 -0.002445  0.400946  0.391099  0.388780  0.383642  0.386484  0.396348   \n",
       "818284 -0.010469  1.370605  1.432862  1.403038  1.406170  1.418153  1.384516   \n",
       "818285 -0.003403  0.530972  0.528852  0.523096  0.519875  0.523234  0.528933   \n",
       "\n",
       "              89        90        91        92        93        94        95  \\\n",
       "0      -0.009174 -0.009142 -0.009488 -0.009448 -0.009330 -0.001023 -0.008025   \n",
       "1      -0.007286 -0.007254 -0.007497 -0.007525 -0.007428 -0.001013 -0.006308   \n",
       "2      -0.002625 -0.002596 -0.002608 -0.002771 -0.002737 -0.001004 -0.002016   \n",
       "3      -0.008781 -0.008738 -0.009059 -0.009043 -0.008926 -0.001021 -0.007683   \n",
       "4       0.642169  0.641640  0.653995  0.661781  0.655282  0.014960  0.570485   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "818281 -0.008006 -0.007955 -0.008257 -0.008243 -0.008151 -0.001017 -0.006947   \n",
       "818282  0.627775  0.628867  0.646236  0.642502  0.638432 -0.004673  0.615510   \n",
       "818283  0.392160  0.397522  0.409125  0.395895  0.391692 -0.002781  0.387466   \n",
       "818284  1.370410  1.356875  1.393057  1.423880  1.422841 -0.010923  1.340130   \n",
       "818285  0.523256  0.526756  0.541459  0.532663  0.528343 -0.003747  0.512422   \n",
       "\n",
       "              96        97        98        99       100       101       102  \\\n",
       "0      -0.009363 -0.000988 -0.001019 -0.009590 -0.009362 -0.009161 -0.009398   \n",
       "1      -0.007450 -0.000977 -0.001007 -0.007611 -0.007436 -0.007283 -0.007480   \n",
       "2      -0.002729 -0.000964 -0.000994 -0.002716 -0.002663 -0.002637 -0.002748   \n",
       "3      -0.008953 -0.000986 -0.001016 -0.009170 -0.008954 -0.008762 -0.008996   \n",
       "4       0.652350  0.010563  0.009437  0.660488  0.653095  0.634200  0.653922   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "818281 -0.008172 -0.000981 -0.001011 -0.008366 -0.008172 -0.007999 -0.008208   \n",
       "818282  0.637732 -0.004425 -0.004459  0.647280  0.635196  0.619968  0.635305   \n",
       "818283  0.395038 -0.002588 -0.002540  0.404068  0.397030  0.384124  0.391730   \n",
       "818284  1.407518 -0.010647 -0.010848  1.415625  1.384928  1.361810  1.408357   \n",
       "818285  0.529503 -0.003508 -0.003522  0.539287  0.529457  0.515743  0.526503   \n",
       "\n",
       "             103       104       105       106       107       108       109  \\\n",
       "0      -0.008683 -0.001001 -0.009491 -0.009557 -0.009275 -0.009317 -0.009408   \n",
       "1      -0.006836 -0.000990 -0.007506 -0.007531 -0.007374 -0.007347 -0.007466   \n",
       "2      -0.002240 -0.000983 -0.002604 -0.002538 -0.002688 -0.002493 -0.002668   \n",
       "3      -0.008284 -0.000999 -0.009076 -0.009123 -0.008860 -0.008894 -0.008993   \n",
       "4       0.607131  0.021454  0.654925  0.656218  0.649529  0.639990  0.649495   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "818281 -0.007508 -0.000994 -0.008249 -0.008284 -0.008084 -0.008098 -0.008195   \n",
       "818282  0.609155 -0.004635  0.641852  0.653162  0.631987  0.632678  0.643281   \n",
       "818283  0.387492 -0.002723  0.408402  0.419339  0.391598  0.402969  0.404803   \n",
       "818284  1.312808 -0.010927  1.376140  1.386961  1.389611  1.358277  1.394958   \n",
       "818285  0.510546 -0.003711  0.538701  0.550489  0.525408  0.531404  0.537914   \n",
       "\n",
       "             110       111       112       113       114       115       116  \\\n",
       "0      -0.001010 -0.001008 -0.009300 -0.009451 -0.009306 -0.009206 -0.001004   \n",
       "1      -0.001000 -0.000998 -0.007390 -0.007505 -0.007408 -0.007319 -0.000994   \n",
       "2      -0.000993 -0.000990 -0.002679 -0.002706 -0.002728 -0.002663 -0.000984   \n",
       "3      -0.001009 -0.001006 -0.008895 -0.009038 -0.008906 -0.008816 -0.001002   \n",
       "4      -0.000094  0.006709  0.653425  0.655688  0.652335  0.644923  0.008469   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "818281 -0.001004 -0.001002 -0.008111 -0.008248 -0.008128 -0.008041 -0.000998   \n",
       "818282 -0.004247 -0.004434  0.637444  0.636687  0.632359  0.623401 -0.004305   \n",
       "818283 -0.002415 -0.002518  0.397161  0.393365  0.392074  0.391494 -0.002426   \n",
       "818284 -0.010485 -0.010771  1.395968  1.404494  1.389542  1.357006 -0.010673   \n",
       "818285 -0.003333 -0.003509  0.530977  0.528841  0.525733  0.520931 -0.003368   \n",
       "\n",
       "             117       118       119       120       121       122       123  \\\n",
       "0      -0.009422 -0.001021 -0.001012 -0.009244 -0.001021 -0.001018 -0.009380   \n",
       "1      -0.007464 -0.001010 -0.001001 -0.007288 -0.001009 -0.001007 -0.007459   \n",
       "2      -0.002625 -0.001000 -0.000990 -0.002456 -0.001000 -0.000996 -0.002717   \n",
       "3      -0.009001 -0.001019 -0.001010 -0.008823 -0.001019 -0.001016 -0.008971   \n",
       "4       0.648315  0.010931  0.008140  0.632811  0.004315  0.021207  0.652762   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "818281 -0.008201 -0.001013 -0.001005 -0.008035 -0.001014 -0.001010 -0.008188   \n",
       "818282  0.641467 -0.004361 -0.004600  0.627626 -0.004601 -0.004169  0.633103   \n",
       "818283  0.404081 -0.002412 -0.002726  0.400338 -0.002705 -0.002282  0.394175   \n",
       "818284  1.392574 -0.010791 -0.010879  1.347336 -0.010854 -0.010495  1.387943   \n",
       "818285  0.536321 -0.003388 -0.003672  0.527359 -0.003668 -0.003224  0.527197   \n",
       "\n",
       "             124       125       126       127       128       129       130  \\\n",
       "0      -0.001021 -0.009279 -0.009660 -0.001005 -0.009348 -0.001009 -0.009439   \n",
       "1      -0.001010 -0.007384 -0.007669 -0.000995 -0.007356 -0.000998 -0.007506   \n",
       "2      -0.001000 -0.002700 -0.002761 -0.000989 -0.002479 -0.000991 -0.002720   \n",
       "3      -0.001019 -0.008875 -0.009236 -0.001004 -0.008910 -0.001007 -0.009024   \n",
       "4       0.012574  0.648345  0.665775 -0.000112  0.631719  0.016916  0.658777   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "818281 -0.001013 -0.008101 -0.008413 -0.000999 -0.008102 -0.001002 -0.008231   \n",
       "818282 -0.004218  0.630970  0.648643 -0.004501  0.624420 -0.004588  0.641712   \n",
       "818283 -0.002330  0.392265  0.405943 -0.002630  0.395761 -0.002709  0.399803   \n",
       "818284 -0.010606  1.385787  1.418129 -0.010708  1.346678 -0.010884  1.404622   \n",
       "818285 -0.003260  0.524869  0.540974 -0.003599  0.522880 -0.003694  0.534510   \n",
       "\n",
       "             131       132       133       134       135       136       137  \\\n",
       "0      -0.001048 -0.009281 -0.001029 -0.009491 -0.009398 -0.009379 -0.009317   \n",
       "1      -0.001037 -0.007388 -0.001018 -0.007477 -0.007395 -0.007461 -0.007384   \n",
       "2      -0.001027 -0.002727 -0.001007 -0.002499 -0.002462 -0.002728 -0.002611   \n",
       "3      -0.001046 -0.008880 -0.001027 -0.009074 -0.008964 -0.008973 -0.008895   \n",
       "4       0.009262  0.648784 -0.000121  0.653595  0.641731  0.656637  0.645517   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "818281 -0.001041 -0.008105 -0.001022 -0.008257 -0.008170 -0.008189 -0.008113   \n",
       "818282 -0.004508  0.631868 -0.004497  0.648118  0.632395  0.637571  0.635983   \n",
       "818283 -0.002660  0.386705 -0.002600  0.415050  0.402829  0.393255  0.401552   \n",
       "818284 -0.010759  1.410383 -0.010728  1.387479  1.358562  1.410548  1.374404   \n",
       "818285 -0.003600  0.522167 -0.003566  0.544874  0.530880  0.528539  0.532221   \n",
       "\n",
       "             138       139       140       141       142       143       144  \\\n",
       "0      -0.001008 -0.009379 -0.001011 -0.000994 -0.008322 -0.009528 -0.009209   \n",
       "1      -0.000997 -0.007403 -0.001001 -0.000982 -0.006583 -0.007574 -0.007276   \n",
       "2      -0.000987 -0.002536 -0.000989 -0.000969 -0.002340 -0.002760 -0.002500   \n",
       "3      -0.001006 -0.008952 -0.001010 -0.000991 -0.007931 -0.009114 -0.008804   \n",
       "4       0.025143  0.639684 -0.000106  0.024876  0.595344  0.661314  0.635416   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "818281 -0.001002 -0.008138 -0.001005 -0.000986 -0.007258 -0.008307 -0.008001   \n",
       "818282 -0.004636  0.636064 -0.004412 -0.004309  0.613374  0.642822  0.630843   \n",
       "818283 -0.002709  0.403778 -0.002567 -0.002437  0.375301  0.399507  0.400996   \n",
       "818284 -0.011005  1.366315 -0.010608 -0.010502  1.369838  1.413518  1.353526   \n",
       "818285 -0.003712  0.533328 -0.003508 -0.003399  0.506364  0.534572  0.529600   \n",
       "\n",
       "             145       146       147       148       149       150       151  \\\n",
       "0      -0.009175 -0.001023 -0.009396 -0.009037 -0.001002 -0.009413 -0.001009   \n",
       "1      -0.007202 -0.001013 -0.007479 -0.007162 -0.000990 -0.007485 -0.000998   \n",
       "2      -0.002351 -0.001003 -0.002747 -0.002497 -0.000976 -0.002725 -0.000987   \n",
       "3      -0.008762 -0.001021 -0.008996 -0.008627 -0.001000 -0.009006 -0.001007   \n",
       "4       0.622680  0.002382  0.658893  0.635736  0.006522  0.658221  0.011569   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "818281 -0.007943 -0.001016 -0.008203 -0.007853 -0.000994 -0.008218 -0.001002   \n",
       "818282  0.615015 -0.004310  0.640584  0.633190 -0.004439  0.642474 -0.004415   \n",
       "818283  0.392912 -0.002459  0.393743  0.396658 -0.002547  0.393421 -0.002526   \n",
       "818284  1.316630 -0.010591  1.423649  1.380776 -0.010762  1.433795 -0.010751   \n",
       "818285  0.517158 -0.003378  0.530449  0.528088 -0.003489  0.530805 -0.003508   \n",
       "\n",
       "             152       153       154       155       156       157       158  \\\n",
       "0      -0.009416 -0.009291 -0.009421 -0.009411 -0.009273 -0.009410 -0.009351   \n",
       "1      -0.007470 -0.007389 -0.007494 -0.007490 -0.007375 -0.007482 -0.007433   \n",
       "2      -0.002667 -0.002694 -0.002744 -0.002748 -0.002688 -0.002728 -0.002708   \n",
       "3      -0.009013 -0.008889 -0.009011 -0.009000 -0.008868 -0.008993 -0.008942   \n",
       "4       0.655396  0.650684  0.653964  0.662640  0.643047  0.660101  0.655441   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "818281 -0.008216 -0.008102 -0.008215 -0.008218 -0.008080 -0.008210 -0.008159   \n",
       "818282  0.638843  0.634262  0.635276  0.638636  0.631835  0.644080  0.638551   \n",
       "818283  0.401790  0.393314  0.394298  0.395555  0.390857  0.399255  0.393268   \n",
       "818284  1.387890  1.399995  1.399276  1.408812  1.395627  1.420946  1.416293   \n",
       "818285  0.533899  0.527201  0.528181  0.530356  0.523989  0.534736  0.529139   \n",
       "\n",
       "             159       160       161       162       163       164       165  \\\n",
       "0      -0.009103 -0.001015 -0.009423 -0.001018 -0.009166 -0.009409 -0.009423   \n",
       "1      -0.007242 -0.001005 -0.007500 -0.001007 -0.007307 -0.007475 -0.007504   \n",
       "2      -0.002657 -0.000995 -0.002762 -0.000998 -0.002721 -0.002702 -0.002770   \n",
       "3      -0.008700 -0.001013 -0.009020 -0.001015 -0.008773 -0.008995 -0.009018   \n",
       "4       0.636419 -0.000145  0.659663  0.022437  0.649595  0.653430  0.658319   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "818281 -0.007945 -0.001009 -0.008230 -0.001011 -0.008015 -0.008193 -0.008233   \n",
       "818282  0.619666 -0.004269  0.641941 -0.004624  0.629596  0.637684  0.642160   \n",
       "818283  0.380970 -0.002402  0.395566 -0.002733  0.385286  0.399189  0.395053   \n",
       "818284  1.379226 -0.010517  1.423559 -0.010840  1.404303  1.392876  1.427172   \n",
       "818285  0.513151 -0.003343  0.531859 -0.003687  0.520404  0.532019  0.531966   \n",
       "\n",
       "             166       167       168       169       170       171       172  \\\n",
       "0      -0.009473 -0.001024 -0.009293 -0.009384 -0.001020 -0.009617 -0.009393   \n",
       "1      -0.007513 -0.001012 -0.007372 -0.007466 -0.001009 -0.007597 -0.007446   \n",
       "2      -0.002672 -0.001004 -0.002643 -0.002740 -0.000997 -0.002604 -0.002634   \n",
       "3      -0.009061 -0.001022 -0.008874 -0.008972 -0.001018 -0.009190 -0.008986   \n",
       "4       0.651154  0.019136  0.644634  0.656112  0.020919  0.655382  0.648386   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "818281 -0.008254 -0.001016 -0.008095 -0.008189 -0.001013 -0.008356 -0.008185   \n",
       "818282  0.640476 -0.004441  0.630963  0.635360 -0.004538  0.653489  0.637526   \n",
       "818283  0.401833 -0.002580  0.389914  0.391775 -0.002642  0.417125  0.403281   \n",
       "818284  1.395902 -0.010727  1.392724  1.410411 -0.010946  1.394731  1.377959   \n",
       "818285  0.534673 -0.003532  0.523860  0.526491 -0.003599  0.549655  0.533943   \n",
       "\n",
       "             173       174       175       176       177       178       179  \\\n",
       "0      -0.009280 -0.009501 -0.001018 -0.001024 -0.009511 -0.009249 -0.009229   \n",
       "1      -0.007389 -0.007544 -0.001007 -0.001012 -0.007510 -0.007357 -0.007338   \n",
       "2      -0.002720 -0.002718 -0.000997 -0.001000 -0.002581 -0.002677 -0.002676   \n",
       "3      -0.008881 -0.009087 -0.001016 -0.001022 -0.009090 -0.008847 -0.008831   \n",
       "4       0.650683  0.655027  0.027261  0.000915  0.654234  0.648112  0.646842   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "818281 -0.008105 -0.008275 -0.001011 -0.001016 -0.008271 -0.008072 -0.008049   \n",
       "818282  0.634881  0.641884 -0.004483 -0.004465  0.646213  0.632926  0.629428   \n",
       "818283  0.387903  0.401257 -0.002640 -0.002559  0.408135  0.389162  0.390465   \n",
       "818284  1.417538  1.402054 -0.010755 -0.010799  1.399653  1.408260  1.388709   \n",
       "818285  0.524333  0.535220 -0.003563 -0.003529  0.540078  0.523454  0.522642   \n",
       "\n",
       "             180       181       182       183  184  \n",
       "0      -0.009345 -0.009345 -0.001010 -0.001001    1  \n",
       "1      -0.007445 -0.007325 -0.000999 -0.000990    1  \n",
       "2      -0.002753 -0.002326 -0.000990 -0.000978    1  \n",
       "3      -0.008939 -0.008943 -0.001008 -0.000999    1  \n",
       "4       0.653333  0.607793  0.011688  0.001387    3  \n",
       "...          ...       ...       ...       ...  ...  \n",
       "818281 -0.008160 -0.008088 -0.001004 -0.000994    1  \n",
       "818282  0.638639  0.589754 -0.004335 -0.004362    3  \n",
       "818283  0.390602  0.382147 -0.002467 -0.002478    3  \n",
       "818284  1.428628  1.243172 -0.010558 -0.010644    3  \n",
       "818285  0.527193  0.498276 -0.003413 -0.003431    3  \n",
       "\n",
       "[818286 rows x 185 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = pd.concat([dataset[['subject_id', 'stay_id', 'hadm_id']].reset_index(drop=True), emb_df.reset_index(drop=True)], axis = 1, ignore_index=True)\n",
    "sample = sample.rename(columns = {0: 'subject_id', 1:'stay_id', 2:'hadm_id'})\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41390a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.to_csv('embedding_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
